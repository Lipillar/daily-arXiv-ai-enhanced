{"id": "2509.13338", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE", "68T07, 68T09"], "pdf": "https://arxiv.org/pdf/2509.13338", "abs": "https://arxiv.org/abs/2509.13338", "authors": ["Hassan Gharoun", "Mohammad Sadegh Khorshidi", "Kasra Ranjbarigderi", "Fang Chen", "Amir H. Gandomi"], "title": "Proximity-Based Evidence Retrieval for Uncertainty-Aware Neural Networks", "comment": "15 pages, 4 figures, 3 tables", "summary": "This work proposes an evidence-retrieval mechanism for uncertainty-aware\ndecision-making that replaces a single global cutoff with an\nevidence-conditioned, instance-adaptive criterion. For each test instance,\nproximal exemplars are retrieved in an embedding space; their predictive\ndistributions are fused via Dempster-Shafer theory. The resulting fused belief\nacts as a per-instance thresholding mechanism. Because the supporting evidences\nare explicit, decisions are transparent and auditable. Experiments on\nCIFAR-10/100 with BiT and ViT backbones show higher or comparable\nuncertainty-aware performance with materially fewer confidently incorrect\noutcomes and a sustainable review load compared with applying threshold on\nprediction entropy. Notably, only a few evidences are sufficient to realize\nthese gains; increasing the evidence set yields only modest changes. These\nresults indicate that evidence-conditioned tagging provides a more reliable and\ninterpretable alternative to fixed prediction entropy thresholds for\noperational uncertainty-aware decision-making.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8bc1\u636e\u68c0\u7d22\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u51b3\u7b56\u673a\u5236\uff0c\u7528\u5b9e\u4f8b\u81ea\u9002\u5e94\u7684\u8bc1\u636e\u6761\u4ef6\u9608\u503c\u66ff\u4ee3\u5168\u5c40\u56fa\u5b9a\u9608\u503c\uff0c\u901a\u8fc7Dempster-Shafer\u7406\u8bba\u878d\u5408\u8fd1\u90bb\u6837\u672c\u9884\u6d4b\u5206\u5e03\uff0c\u5b9e\u73b0\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u51b3\u7b56", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u9884\u6d4b\u71b5\u7684\u5168\u5c40\u9608\u503c\u65b9\u6cd5\u5b58\u5728\u7f6e\u4fe1\u9519\u8bef\u9884\u6d4b\u95ee\u9898\uff0c\u9700\u8981\u66f4\u900f\u660e\u3001\u53ef\u5ba1\u8ba1\u4e14\u5b9e\u4f8b\u81ea\u9002\u5e94\u7684\u4e0d\u786e\u5b9a\u6027\u51b3\u7b56\u673a\u5236", "method": "\u4e3a\u6bcf\u4e2a\u6d4b\u8bd5\u5b9e\u4f8b\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u68c0\u7d22\u8fd1\u90bb\u6837\u672c\uff0c\u4f7f\u7528Dempster-Shafer\u7406\u8bba\u878d\u5408\u5176\u9884\u6d4b\u5206\u5e03\uff0c\u751f\u6210\u5b9e\u4f8b\u7279\u5b9a\u7684\u9608\u503c\u6807\u51c6", "result": "\u5728CIFAR-10/100\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u9884\u6d4b\u71b5\u9608\u503c\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u76f8\u5f53\u6216\u66f4\u597d\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6027\u80fd\uff0c\u663e\u8457\u51cf\u5c11\u7f6e\u4fe1\u9519\u8bef\u9884\u6d4b\uff0c\u7ef4\u6301\u53ef\u6301\u7eed\u7684\u5ba1\u6838\u8d1f\u8f7d", "conclusion": "\u8bc1\u636e\u6761\u4ef6\u6807\u8bb0\u4e3a\u64cd\u4f5c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u51b3\u7b56\u63d0\u4f9b\u4e86\u6bd4\u56fa\u5b9a\u9884\u6d4b\u71b5\u9608\u503c\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4ec5\u9700\u5c11\u91cf\u8bc1\u636e\u5373\u53ef\u5b9e\u73b0\u663e\u8457\u6539\u8fdb"}}
{"id": "2509.13353", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13353", "abs": "https://arxiv.org/abs/2509.13353", "authors": ["Muhammad Adnan Shahzad"], "title": "Hybrid Quantum-Classical Model for Image Classification", "comment": null, "summary": "This study presents a systematic comparison between hybrid quantum-classical\nneural networks and purely classical models across three benchmark datasets\n(MNIST, CIFAR100, and STL10) to evaluate their performance, efficiency, and\nrobustness. The hybrid models integrate parameterized quantum circuits with\nclassical deep learning architectures, while the classical counterparts use\nconventional convolutional neural networks (CNNs). Experiments were conducted\nover 50 training epochs for each dataset, with evaluations on validation\naccuracy, test accuracy, training time, computational resource usage, and\nadversarial robustness (tested with $\\epsilon=0.1$ perturbations).Key findings\ndemonstrate that hybrid models consistently outperform classical models in\nfinal accuracy, achieving {99.38\\% (MNIST), 41.69\\% (CIFAR100), and 74.05\\%\n(STL10) validation accuracy, compared to classical benchmarks of 98.21\\%,\n32.25\\%, and 63.76\\%, respectively. Notably, the hybrid advantage scales with\ndataset complexity, showing the most significant gains on CIFAR100 (+9.44\\%)\nand STL10 (+10.29\\%). Hybrid models also train 5--12$\\times$ faster (e.g.,\n21.23s vs. 108.44s per epoch on MNIST) and use 6--32\\% fewer parameters} while\nmaintaining superior generalization to unseen test data.Adversarial robustness\ntests reveal that hybrid models are significantly more resilient on simpler\ndatasets (e.g., 45.27\\% robust accuracy on MNIST vs. 10.80\\% for classical) but\nshow comparable fragility on complex datasets like CIFAR100 ($\\sim$1\\%\nrobustness for both). Resource efficiency analyses indicate that hybrid models\nconsume less memory (4--5GB vs. 5--6GB for classical) and lower CPU utilization\n(9.5\\% vs. 23.2\\% on average).These results suggest that hybrid\nquantum-classical architectures offer compelling advantages in accuracy,\ntraining efficiency, and parameter scalability, particularly for complex vision\ntasks.", "AI": {"tldr": "\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u7eaf\u7ecf\u5178\u6a21\u578b\u5728\u51c6\u786e\u7387\u3001\u8bad\u7ec3\u6548\u7387\u548c\u53c2\u6570\u6548\u7387\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u6570\u636e\u96c6\u4e0a\u4f18\u52bf\u66f4\u660e\u663e\u3002", "motivation": "\u7cfb\u7edf\u6bd4\u8f83\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u4e0e\u7eaf\u7ecf\u5178\u6a21\u578b\u5728\u6027\u80fd\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u5dee\u5f02\uff0c\u63a2\u7d22\u91cf\u5b50\u8ba1\u7b97\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u5728MNIST\u3001CIFAR100\u548cSTL10\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\u4e0e\u7ecf\u5178\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u7ed3\u5408\u7684\u6df7\u5408\u6a21\u578b\uff0c\u4e0e\u4f20\u7edf\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u8bad\u7ec350\u4e2aepoch\uff0c\u8bc4\u4f30\u9a8c\u8bc1\u51c6\u786e\u7387\u3001\u6d4b\u8bd5\u51c6\u786e\u7387\u3001\u8bad\u7ec3\u65f6\u95f4\u3001\u8ba1\u7b97\u8d44\u6e90\u4f7f\u7528\u548c\u5bf9\u6297\u9c81\u68d2\u6027\u3002", "result": "\u6df7\u5408\u6a21\u578b\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u90fd\u4f18\u4e8e\u7ecf\u5178\u6a21\u578b\uff1aMNIST(99.38% vs 98.21%)\u3001CIFAR100(41.69% vs 32.25%)\u3001STL10(74.05% vs 63.76%)\u3002\u8bad\u7ec3\u901f\u5ea6\u5feb5-12\u500d\uff0c\u53c2\u6570\u51cf\u5c116-32%\uff0c\u5185\u5b58\u4f7f\u7528\u66f4\u5c11(4-5GB vs 5-6GB)\uff0cCPU\u5229\u7528\u7387\u66f4\u4f4e(9.5% vs 23.2%)\u3002\u5728\u7b80\u5355\u6570\u636e\u96c6\u4e0a\u5bf9\u6297\u9c81\u68d2\u6027\u663e\u8457\u66f4\u597d(MNIST:45.27% vs 10.80%)\u3002", "conclusion": "\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u67b6\u6784\u5728\u51c6\u786e\u7387\u3001\u8bad\u7ec3\u6548\u7387\u548c\u53c2\u6570\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u7279\u522b\u9002\u7528\u4e8e\u590d\u6742\u7684\u89c6\u89c9\u4efb\u52a1\uff0c\u4e3a\u91cf\u5b50\u8ba1\u7b97\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u8bc1\u636e\u3002"}}
{"id": "2509.13361", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13361", "abs": "https://arxiv.org/abs/2509.13361", "authors": ["Tong Yulin", "Liang Xuechen"], "title": "Research on Expressway Congestion Warning Technology Based on YOLOv11-DIoU and GRU-Attention", "comment": null, "summary": "Expressway traffic congestion severely reduces travel efficiency and hinders\nregional connectivity. Existing \"detection-prediction\" systems have critical\nflaws: low vehicle perception accuracy under occlusion and loss of\nlong-sequence dependencies in congestion forecasting. This study proposes an\nintegrated technical framework to resolve these issues.For traffic flow\nperception, two baseline algorithms were optimized. Traditional YOLOv11 was\nupgraded to YOLOv11-DIoU by replacing GIoU Loss with DIoU Loss, and DeepSort\nwas improved by fusing Mahalanobis (motion) and cosine (appearance) distances.\nExperiments on Chang-Shen Expressway videos showed YOLOv11-DIoU achieved 95.7\\%\nmAP (6.5 percentage points higher than baseline) with 5.3\\% occlusion miss\nrate. DeepSort reached 93.8\\% MOTA (11.3 percentage points higher than SORT)\nwith only 4 ID switches. Using the Greenberg model (for 10-15 vehicles/km\nhigh-density scenarios), speed and density showed a strong negative correlation\n(r=-0.97), conforming to traffic flow theory. For congestion warning, a\nGRU-Attention model was built to capture congestion precursors. Trained 300\nepochs with flow, density, and speed, it achieved 99.7\\% test accuracy (7-9\npercentage points higher than traditional GRU). In 10-minute advance warnings\nfor 30-minute congestion, time error was $\\leq$ 1 minute. Validation with an\nindependent video showed 95\\% warning accuracy, over 90\\% spatial overlap of\ncongestion points, and stable performance in high-flow ($>$5 vehicles/second)\nscenarios.This framework provides quantitative support for expressway\ncongestion control, with promising intelligent transportation applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u96c6\u6210\u6280\u672f\u6846\u67b6\u6765\u89e3\u51b3\u9ad8\u901f\u516c\u8def\u4ea4\u901a\u62e5\u5835\u95ee\u9898\uff0c\u5305\u62ec\u4f18\u5316\u7684\u8f66\u8f86\u611f\u77e5\u7b97\u6cd5YOLOv11-DIoU\u548c\u6539\u8fdb\u7684DeepSort\u8ddf\u8e2a\uff0c\u4ee5\u53ca\u57fa\u4e8eGRU-Attention\u7684\u62e5\u5835\u9884\u8b66\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u611f\u77e5\u7cbe\u5ea6\u548c\u9884\u8b66\u51c6\u786e\u6027\u3002", "motivation": "\u9ad8\u901f\u516c\u8def\u4ea4\u901a\u62e5\u5835\u4e25\u91cd\u964d\u4f4e\u51fa\u884c\u6548\u7387\u5e76\u963b\u788d\u533a\u57df\u8fde\u901a\u6027\u3002\u73b0\u6709\u7684'\u68c0\u6d4b-\u9884\u6d4b'\u7cfb\u7edf\u5b58\u5728\u5173\u952e\u7f3a\u9677\uff1a\u906e\u6321\u6761\u4ef6\u4e0b\u8f66\u8f86\u611f\u77e5\u7cbe\u5ea6\u4f4e\uff0c\u4ee5\u53ca\u62e5\u5835\u9884\u6d4b\u4e2d\u957f\u5e8f\u5217\u4f9d\u8d56\u5173\u7cfb\u7684\u4e22\u5931\u3002", "method": "1) \u8f66\u8f86\u611f\u77e5\uff1a\u4f18\u5316YOLOv11\u4e3aYOLOv11-DIoU\uff08\u7528DIoU Loss\u66ff\u6362GIoU Loss\uff09\uff0c\u6539\u8fdbDeepSort\uff08\u878d\u5408\u9a6c\u6c0f\u8ddd\u79bb\u548c\u4f59\u5f26\u8ddd\u79bb\uff09\uff1b2) \u62e5\u5835\u9884\u8b66\uff1a\u6784\u5efaGRU-Attention\u6a21\u578b\u6355\u6349\u62e5\u5835\u524d\u5146\uff0c\u4f7f\u7528\u6d41\u91cf\u3001\u5bc6\u5ea6\u548c\u901f\u5ea6\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "YOLOv11-DIoU\u8fbe\u523095.7% mAP\uff08\u6bd4\u57fa\u7ebf\u9ad86.5\u4e2a\u767e\u5206\u70b9\uff09\uff0c\u906e\u6321\u6f0f\u68c0\u73875.3%\uff1bDeepSort\u8fbe\u523093.8% MOTA\uff08\u6bd4SORT\u9ad811.3\u4e2a\u767e\u5206\u70b9\uff09\uff1bGRU-Attention\u6a21\u578b\u6d4b\u8bd5\u51c6\u786e\u738799.7%\uff0c10\u5206\u949f\u63d0\u524d\u9884\u8b66\u65f6\u95f4\u8bef\u5dee\u22641\u5206\u949f\uff0c\u72ec\u7acb\u89c6\u9891\u9a8c\u8bc1\u663e\u793a95%\u9884\u8b66\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9ad8\u901f\u516c\u8def\u62e5\u5835\u63a7\u5236\u63d0\u4f9b\u4e86\u91cf\u5316\u652f\u6301\uff0c\u5728\u667a\u80fd\u4ea4\u901a\u5e94\u7528\u4e2d\u5177\u6709\u826f\u597d\u524d\u666f\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u73b0\u6709\u7cfb\u7edf\u7684\u5173\u952e\u7f3a\u9677\u3002"}}
{"id": "2509.13366", "categories": ["cs.CV", "68U99", "J.2"], "pdf": "https://arxiv.org/pdf/2509.13366", "abs": "https://arxiv.org/abs/2509.13366", "authors": ["Tony Rohe", "Martin Margreiter", "Markus Moertl"], "title": "Parking Space Ground Truth Test Automation by Artificial Intelligence Using Convolutional Neural Networks", "comment": "10 pages, 5 figures", "summary": "This research is part of a study of a real-time, cloud-based on-street\nparking service using crowd-sourced in-vehicle fleet data. The service provides\nreal-time information about available parking spots by classifying\ncrowd-sourced detections observed via ultrasonic sensors. The goal of this\nresearch is to optimize the current parking service quality by analyzing the\nautomation of the existing test process for ground truth tests. Therefore,\nmethods from the field of machine learning, especially image pattern\nrecognition, are applied to enrich the database and substitute human\nengineering work in major areas of the analysis process. After an introduction\ninto the related areas of machine learning, this paper explains the methods and\nimplementations made to achieve a high level of automation, applying\nconvolutional neural networks. Finally, predefined metrics present the\nperformance level achieved, showing a time reduction of human resources up to\n99.58 %. The overall improvements are discussed, summarized, and followed by an\noutlook for future development and potential application of the analysis\nautomation tool.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5e94\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u56fe\u50cf\u6a21\u5f0f\u8bc6\u522b\u6280\u672f\uff0c\u81ea\u52a8\u5316\u4e86\u57fa\u4e8e\u4f17\u5305\u8f66\u8f86\u6570\u636e\u7684\u5b9e\u65f6\u8def\u8fb9\u505c\u8f66\u670d\u52a1\u7684\u6d4b\u8bd5\u6d41\u7a0b\uff0c\u5c06\u4eba\u5de5\u8d44\u6e90\u9700\u6c42\u51cf\u5c11\u4e8699.58%\u3002", "motivation": "\u4f18\u5316\u73b0\u6709\u5b9e\u65f6\u8def\u8fb9\u505c\u8f66\u670d\u52a1\u7684\u8d28\u91cf\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u5730\u9762\u5b9e\u51b5\u6d4b\u8bd5\u6d41\u7a0b\u6765\u66ff\u4ee3\u4eba\u5de5\u5de5\u7a0b\u5de5\u4f5c\uff0c\u63d0\u9ad8\u5206\u6790\u6548\u7387\u3002", "method": "\u5e94\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u56fe\u50cf\u6a21\u5f0f\u8bc6\u522b\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u6765\u81ea\u52a8\u5316\u6d4b\u8bd5\u8fc7\u7a0b\u5e76\u4e30\u5bcc\u6570\u636e\u5e93\u3002", "result": "\u5b9e\u73b0\u4e86\u9ad8\u5ea6\u81ea\u52a8\u5316\uff0c\u4eba\u5de5\u8d44\u6e90\u65f6\u95f4\u51cf\u5c11\u4e8699.58%\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u6790\u6548\u7387\u3002", "conclusion": "\u81ea\u52a8\u5316\u5de5\u5177\u5728\u505c\u8f66\u670d\u52a1\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u672a\u6765\u53d1\u5c55\u548c\u6f5c\u5728\u5e94\u7528\u63d0\u4f9b\u4e86\u826f\u597d\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u673a\u5668\u5b66\u4e60\u5728\u5b9e\u65f6\u670d\u52a1\u4f18\u5316\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.13480", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13480", "abs": "https://arxiv.org/abs/2509.13480", "authors": ["Andrea Piergentili", "Beatrice Savoldi", "Matteo Negri", "Luisa Bentivogli"], "title": "Gender-Neutral Rewriting in Italian: Models, Approaches, and Trade-offs", "comment": "Accepted at CLiC-it 2025", "summary": "Gender-neutral rewriting (GNR) aims to reformulate text to eliminate\nunnecessary gender specifications while preserving meaning, a particularly\nchallenging task in grammatical-gender languages like Italian. In this work, we\nconduct the first systematic evaluation of state-of-the-art large language\nmodels (LLMs) for Italian GNR, introducing a two-dimensional framework that\nmeasures both neutrality and semantic fidelity to the input. We compare\nfew-shot prompting across multiple LLMs, fine-tune selected models, and apply\ntargeted cleaning to boost task relevance. Our findings show that open-weight\nLLMs outperform the only existing model dedicated to GNR in Italian, whereas\nour fine-tuned models match or exceed the best open-weight LLM's performance at\na fraction of its size. Finally, we discuss the trade-off between optimizing\nthe training data for neutrality and meaning preservation.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86\u610f\u5927\u5229\u8bed\u6027\u522b\u4e2d\u6027\u6539\u5199\u4efb\u52a1\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86\u8861\u91cf\u4e2d\u7acb\u6027\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u7684\u4e8c\u7ef4\u6846\u67b6\uff0c\u53d1\u73b0\u5f00\u6e90LLMs\u4f18\u4e8e\u73b0\u6709\u4e13\u7528\u6a21\u578b\uff0c\u5fae\u8c03\u540e\u7684\u5c0f\u6a21\u578b\u4e5f\u80fd\u8fbe\u5230\u76f8\u8fd1\u6027\u80fd", "motivation": "\u610f\u5927\u5229\u8bed\u7b49\u8bed\u6cd5\u6027\u522b\u8bed\u8a00\u4e2d\u7684\u6027\u522b\u4e2d\u6027\u6539\u5199\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u6d88\u9664\u4e0d\u5fc5\u8981\u7684\u6027\u522b\u6307\u5b9a\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e0d\u8db3", "method": "\u91c7\u7528\u5c11\u6837\u672c\u63d0\u793a\u6bd4\u8f83\u591a\u4e2aLLMs\uff0c\u5bf9\u9009\u5b9a\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5e94\u7528\u9488\u5bf9\u6027\u6e05\u6d17\u63d0\u5347\u4efb\u52a1\u76f8\u5173\u6027\uff0c\u4f7f\u7528\u4e8c\u7ef4\u6846\u67b6\u8bc4\u4f30\u4e2d\u7acb\u6027\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6", "result": "\u5f00\u6e90\u6743\u91cdLLMs\u4f18\u4e8e\u73b0\u6709\u610f\u5927\u5229\u8bedGNR\u4e13\u7528\u6a21\u578b\uff0c\u5fae\u8c03\u540e\u7684\u5c0f\u6a21\u578b\u4ee5\u66f4\u5c0f\u89c4\u6a21\u8fbe\u5230\u6216\u8d85\u8fc7\u6700\u4f73\u5f00\u6e90LLM\u6027\u80fd", "conclusion": "\u5728\u4f18\u5316\u8bad\u7ec3\u6570\u636e\u65f6\u9700\u8981\u6743\u8861\u4e2d\u7acb\u6027\u548c\u610f\u4e49\u4fdd\u6301\u4e4b\u95f4\u7684\u5e73\u8861\uff0c\u5fae\u8c03\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u5c0f\u6a21\u578b\u5728\u6027\u522b\u4e2d\u6027\u6539\u5199\u4efb\u52a1\u4e0a\u7684\u8868\u73b0"}}
{"id": "2509.13375", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13375", "abs": "https://arxiv.org/abs/2509.13375", "authors": ["Yuxiao Lee", "Xiaofeng Cao", "Wei Ye", "Jiangchao Yao", "Jingkuan Song", "Heng Tao Shen"], "title": "An Empirical Analysis of VLM-based OOD Detection: Mechanisms, Advantages, and Sensitivity", "comment": null, "summary": "Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable\nzero-shot out-of-distribution (OOD) detection capabilities, vital for reliable\nAI systems. Despite this promising capability, a comprehensive understanding of\n(1) why they work so effectively, (2) what advantages do they have over\nsingle-modal methods, and (3) how is their behavioral robustness -- remains\nnotably incomplete within the research community. This paper presents a\nsystematic empirical analysis of VLM-based OOD detection using in-distribution\n(ID) and OOD prompts. (1) Mechanisms: We systematically characterize and\nformalize key operational properties within the VLM embedding space that\nfacilitate zero-shot OOD detection. (2) Advantages: We empirically quantify the\nsuperiority of these models over established single-modal approaches,\nattributing this distinct advantage to the VLM's capacity to leverage rich\nsemantic novelty. (3) Sensitivity: We uncovers a significant and previously\nunder-explored asymmetry in their robustness profile: while exhibiting\nresilience to common image noise, these VLM-based methods are highly sensitive\nto prompt phrasing. Our findings contribute a more structured understanding of\nthe strengths and critical vulnerabilities inherent in VLM-based OOD detection,\noffering crucial, empirically-grounded guidance for developing more robust and\nreliable future designs.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u7684\u96f6\u6837\u672c\u5206\u5e03\u5916\u68c0\u6d4b\u673a\u5236\uff0c\u63ed\u793a\u4e86VLM\u76f8\u6bd4\u5355\u6a21\u6001\u65b9\u6cd5\u7684\u4f18\u52bf\u6e90\u4e8e\u8bed\u4e49\u65b0\u9896\u6027\u5229\u7528\u80fd\u529b\uff0c\u5e76\u53d1\u73b0\u5176\u5bf9\u63d0\u793a\u8bcd\u8868\u8ff0\u9ad8\u5ea6\u654f\u611f\u7684\u5173\u952e\u8106\u5f31\u6027", "motivation": "\u5c3d\u7ba1CLIP\u7b49\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u5206\u5e03\u5916\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7814\u7a76\u754c\u5bf9\u5176\u5de5\u4f5c\u673a\u5236\u3001\u76f8\u5bf9\u4e8e\u5355\u6a21\u6001\u65b9\u6cd5\u7684\u4f18\u52bf\u4ee5\u53ca\u884c\u4e3a\u9c81\u68d2\u6027\u7f3a\u4e4f\u7cfb\u7edf\u7406\u89e3", "method": "\u901a\u8fc7\u7cfb\u7edf\u5b9e\u8bc1\u5206\u6790\uff0c\u4f7f\u7528\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u63d0\u793a\u8bcd\uff0c\u4ece\u4e09\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u7814\u7a76\uff1a(1)\u673a\u5236\u5206\u6790-\u5f62\u5f0f\u5316VLM\u5d4c\u5165\u7a7a\u95f4\u7684\u5173\u952e\u64cd\u4f5c\u7279\u6027\uff1b(2)\u4f18\u52bf\u91cf\u5316-\u5b9e\u8bc1\u6bd4\u8f83VLM\u4e0e\u5355\u6a21\u6001\u65b9\u6cd5\uff1b(3)\u654f\u611f\u6027\u6d4b\u8bd5-\u8bc4\u4f30\u5bf9\u56fe\u50cf\u566a\u58f0\u548c\u63d0\u793a\u8bcd\u8868\u8ff0\u7684\u9c81\u68d2\u6027", "result": "\u53d1\u73b0VLM\u7684\u96f6\u6837\u672cOOD\u68c0\u6d4b\u4f18\u52bf\u6e90\u4e8e\u5176\u5229\u7528\u4e30\u5bcc\u8bed\u4e49\u65b0\u9896\u6027\u7684\u80fd\u529b\uff1b\u63ed\u793a\u4e86\u4e00\u4e2a\u91cd\u8981\u7684\u4e0d\u5bf9\u79f0\u9c81\u68d2\u6027\u7279\u5f81\uff1a\u5bf9\u5e38\u89c1\u56fe\u50cf\u566a\u58f0\u5177\u6709\u5f39\u6027\uff0c\u4f46\u5bf9\u63d0\u793a\u8bcd\u8868\u8ff0\u9ad8\u5ea6\u654f\u611f", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5bf9VLM\u57fa\u4e8eOOD\u68c0\u6d4b\u4f18\u52bf\u548c\u5173\u952e\u8106\u5f31\u6027\u7684\u7ed3\u6784\u5316\u7406\u89e3\uff0c\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u53ef\u9760\u7684\u672a\u6765\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u6307\u5bfc"}}
{"id": "2509.13539", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13539", "abs": "https://arxiv.org/abs/2509.13539", "authors": ["Alisa Kanganis", "Katherine A. Keith"], "title": "Op-Fed: Opinion, Stance, and Monetary Policy Annotations on FOMC Transcripts Using Active Learning", "comment": null, "summary": "The U.S. Federal Open Market Committee (FOMC) regularly discusses and sets\nmonetary policy, affecting the borrowing and spending decisions of millions of\npeople. In this work, we release Op-Fed, a dataset of 1044 human-annotated\nsentences and their contexts from FOMC transcripts. We faced two major\ntechnical challenges in dataset creation: imbalanced classes -- we estimate\nfewer than 8% of sentences express a non-neutral stance towards monetary policy\n-- and inter-sentence dependence -- 65% of instances require context beyond the\nsentence-level. To address these challenges, we developed a five-stage\nhierarchical schema to isolate aspects of opinion, monetary policy, and stance\ntowards monetary policy as well as the level of context needed. Second, we\nselected instances to annotate using active learning, roughly doubling the\nnumber of positive instances across all schema aspects. Using Op-Fed, we found\na top-performing, closed-weight LLM achieves 0.80 zero-shot accuracy in opinion\nclassification but only 0.61 zero-shot accuracy classifying stance towards\nmonetary policy -- below our human baseline of 0.89. We expect Op-Fed to be\nuseful for future model training, confidence calibration, and as a seed dataset\nfor future annotation efforts.", "AI": {"tldr": "Op-Fed\u662f\u4e00\u4e2a\u5305\u542b1044\u4e2aFOMC\u4f1a\u8bae\u8bb0\u5f55\u4eba\u5de5\u6807\u6ce8\u53e5\u5b50\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8d27\u5e01\u653f\u7b56\u7acb\u573a\u5206\u6790\uff0c\u89e3\u51b3\u4e86\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u53e5\u5b50\u95f4\u4f9d\u8d56\u7684\u6280\u672f\u6311\u6218\u3002", "motivation": "FOMC\u7684\u8d27\u5e01\u653f\u7b56\u51b3\u7b56\u5f71\u54cd\u6570\u767e\u4e07\u4eba\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u8868\u8fbe\u975e\u4e2d\u6027\u8d27\u5e01\u653f\u7b56\u7acb\u573a\u7684\u53e5\u5b50\u4e0d\u8db38%\uff0c\u4e1465%\u7684\u5b9e\u4f8b\u9700\u8981\u53e5\u5b50\u7ea7\u4ee5\u4e0a\u7684\u4e0a\u4e0b\u6587\uff0c\u9700\u8981\u4e13\u95e8\u7684\u6570\u636e\u96c6\u6765\u652f\u6301\u76f8\u5173\u7814\u7a76\u3002", "method": "\u5f00\u53d1\u4e86\u4e94\u9636\u6bb5\u5206\u5c42\u6807\u6ce8\u6846\u67b6\u6765\u5206\u79bb\u610f\u89c1\u3001\u8d27\u5e01\u653f\u7b56\u548c\u8d27\u5e01\u653f\u7b56\u7acb\u573a\u7b49\u7ef4\u5ea6\uff1b\u4f7f\u7528\u4e3b\u52a8\u5b66\u4e60\u9009\u62e9\u6807\u6ce8\u5b9e\u4f8b\uff0c\u4f7f\u6240\u6709\u6a21\u5f0f\u65b9\u9762\u7684\u6b63\u4f8b\u6570\u91cf\u5927\u81f4\u7ffb\u500d\u3002", "result": "\u6027\u80fd\u6700\u4f73\u7684\u95ed\u6e90LLM\u5728\u610f\u89c1\u5206\u7c7b\u4e0a\u8fbe\u52300.80\u7684\u96f6\u6837\u672c\u51c6\u786e\u7387\uff0c\u4f46\u5728\u8d27\u5e01\u653f\u7b56\u7acb\u573a\u5206\u7c7b\u4e0a\u53ea\u67090.61\uff0c\u4f4e\u4e8e\u4eba\u7c7b\u57fa\u7ebf0.89\u3002", "conclusion": "Op-Fed\u6570\u636e\u96c6\u53ef\u7528\u4e8e\u672a\u6765\u6a21\u578b\u8bad\u7ec3\u3001\u7f6e\u4fe1\u5ea6\u6821\u51c6\uff0c\u5e76\u4f5c\u4e3a\u672a\u6765\u6807\u6ce8\u5de5\u4f5c\u7684\u79cd\u5b50\u6570\u636e\u96c6\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u8d27\u5e01\u653f\u7b56\u7acb\u573a\u5206\u6790\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2509.13385", "categories": ["cs.CV", "cs.DM", "cs.LG", "51K05 (primary) 57-08, 53Z50, 55U10 (secondary)", "G.2.2"], "pdf": "https://arxiv.org/pdf/2509.13385", "abs": "https://arxiv.org/abs/2509.13385", "authors": ["Charlotte Beylier", "Parvaneh Joharinad", "J\u00fcrgen Jost", "Nahid Torbati"], "title": "Curvature as a tool for evaluating dimensionality reduction and estimating intrinsic dimension", "comment": "31 pages, 14 figures", "summary": "Utilizing recently developed abstract notions of sectional curvature, we\nintroduce a method for constructing a curvature-based geometric profile of\ndiscrete metric spaces. The curvature concept that we use here captures the\nmetric relations between triples of points and other points. More\nsignificantly, based on this curvature profile, we introduce a quantitative\nmeasure to evaluate the effectiveness of data representations, such as those\nproduced by dimensionality reduction techniques. Furthermore, Our experiments\ndemonstrate that this curvature-based analysis can be employed to estimate the\nintrinsic dimensionality of datasets. We use this to explore the large-scale\ngeometry of empirical networks and to evaluate the effectiveness of\ndimensionality reduction techniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u622a\u9762\u66f2\u7387\u7684\u51e0\u4f55\u5206\u6790\u65b9\u6cd5\uff0c\u7528\u4e8e\u6784\u5efa\u79bb\u6563\u5ea6\u91cf\u7a7a\u95f4\u7684\u66f2\u7387\u5256\u9762\uff0c\u5e76\u4ee5\u6b64\u8bc4\u4f30\u6570\u636e\u8868\u793a\u6548\u679c\u548c\u4f30\u8ba1\u6570\u636e\u96c6\u7684\u5185\u5728\u7ef4\u5ea6\u3002", "motivation": "\u5229\u7528\u65b0\u53d1\u5c55\u7684\u622a\u9762\u66f2\u7387\u62bd\u8c61\u6982\u5ff5\uff0c\u65e8\u5728\u4e3a\u79bb\u6563\u5ea6\u91cf\u7a7a\u95f4\u5efa\u7acb\u51e0\u4f55\u7279\u5f81\u63cf\u8ff0\uff0c\u7279\u522b\u662f\u4e3a\u4e86\u8bc4\u4f30\u964d\u7ef4\u6280\u672f\u7b49\u6570\u636e\u8868\u793a\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "method": "\u57fa\u4e8e\u6355\u83b7\u70b9\u4e09\u5143\u7ec4\u4e0e\u5176\u4ed6\u70b9\u4e4b\u95f4\u5ea6\u91cf\u5173\u7cfb\u7684\u66f2\u7387\u6982\u5ff5\uff0c\u6784\u5efa\u66f2\u7387\u5256\u9762\u5206\u6790\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u53ef\u7528\u4e8e\u4f30\u8ba1\u6570\u636e\u96c6\u7684\u5185\u5728\u7ef4\u5ea6\uff0c\u63a2\u7d22\u7ecf\u9a8c\u7f51\u7edc\u7684\u5927\u89c4\u6a21\u51e0\u4f55\u7ed3\u6784\uff0c\u5e76\u8bc4\u4f30\u964d\u7ef4\u6280\u672f\u7684\u6548\u679c\u3002", "conclusion": "\u66f2\u7387\u57fa\u5206\u6790\u4e3a\u79bb\u6563\u5ea6\u91cf\u7a7a\u95f4\u7684\u51e0\u4f55\u7279\u5f81\u63cf\u8ff0\u548c\u6570\u636e\u8868\u793a\u6548\u679c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b9a\u91cf\u6d4b\u91cf\u5de5\u5177\u3002"}}
{"id": "2509.13569", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13569", "abs": "https://arxiv.org/abs/2509.13569", "authors": ["John Mendon\u00e7a", "Lining Zhang", "Rahul Mallidi", "Alon Lavie", "Isabel Trancoso", "Luis Fernando D'Haro", "Jo\u00e3o Sedoc"], "title": "Overview of Dialog System Evaluation Track: Dimensionality, Language, Culture and Safety at DSTC 12", "comment": "DSTC12 Track 1 Overview Paper. https://chateval.org/dstc12", "summary": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for robust dialogue system evaluation, yet comprehensive assessment\nremains challenging. Traditional metrics often prove insufficient, and safety\nconsiderations are frequently narrowly defined or culturally biased. The DSTC12\nTrack 1, \"Dialog System Evaluation: Dimensionality, Language, Culture and\nSafety,\" is part of the ongoing effort to address these critical gaps. The\ntrack comprised two subtasks: (1) Dialogue-level, Multi-dimensional Automatic\nEvaluation Metrics, and (2) Multilingual and Multicultural Safety Detection.\nFor Task 1, focused on 10 dialogue dimensions, a Llama-3-8B baseline achieved\nthe highest average Spearman's correlation (0.1681), indicating substantial\nroom for improvement. In Task 2, while participating teams significantly\noutperformed a Llama-Guard-3-1B baseline on the multilingual safety subset (top\nROC-AUC 0.9648), the baseline proved superior on the cultural subset (0.5126\nROC-AUC), highlighting critical needs in culturally-aware safety. This paper\ndescribes the datasets and baselines provided to participants, as well as\nsubmission evaluation results for each of the two proposed subtasks.", "AI": {"tldr": "DSTC12 Track 1\u9488\u5bf9\u5bf9\u8bdd\u7cfb\u7edf\u8bc4\u4f30\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u591a\u7ef4\u5ea6\u81ea\u52a8\u8bc4\u4f30\u548c\u591a\u8bed\u8a00\u6587\u5316\u5b89\u5168\u68c0\u6d4b\uff0c\u63d0\u51fa\u4e86\u57fa\u51c6\u6d4b\u8bd5\u548c\u57fa\u7ebf\u6a21\u578b\uff0c\u7ed3\u679c\u663e\u793a\u73b0\u6709\u65b9\u6cd5\u5728\u6587\u5316\u5b89\u5168\u65b9\u9762\u4ecd\u6709\u5f88\u5927\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u51f8\u663e\u4e86\u5bf9\u8bdd\u7cfb\u7edf\u8bc4\u4f30\u7684\u4e0d\u8db3\uff0c\u4f20\u7edf\u6307\u6807\u4e0d\u591f\u5168\u9762\uff0c\u5b89\u5168\u8003\u8651\u5b58\u5728\u6587\u5316\u504f\u89c1\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u5b50\u4efb\u52a1\u8fdb\u884c\u8bc4\u4f30\uff1a(1)\u5bf9\u8bdd\u7ea7\u591a\u7ef4\u5ea6\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff0810\u4e2a\u7ef4\u5ea6\uff09\uff0c(2)\u591a\u8bed\u8a00\u548c\u591a\u6587\u5316\u5b89\u5168\u68c0\u6d4b\uff0c\u4f7f\u7528Llama-3-8B\u548cLlama-Guard-3-1B\u4f5c\u4e3a\u57fa\u7ebf\u6a21\u578b\u3002", "result": "\u4efb\u52a11\u4e2dLlama-3-8B\u57fa\u7ebf\u83b7\u5f97\u6700\u9ad8\u5e73\u5747Spearman\u76f8\u5173\u7cfb\u65700.1681\uff0c\u663e\u793a\u4ecd\u6709\u5f88\u5927\u6539\u8fdb\u7a7a\u95f4\uff1b\u4efb\u52a12\u4e2d\u53c2\u8d5b\u56e2\u961f\u5728\u591a\u8bed\u8a00\u5b89\u5168\u5b50\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff08\u6700\u9ad8ROC-AUC 0.9648\uff09\uff0c\u4f46\u57fa\u7ebf\u5728\u6587\u5316\u5b50\u96c6\u4e0a\u66f4\u4f18\uff080.5126 ROC-AUC\uff09\u3002", "conclusion": "\u5bf9\u8bdd\u7cfb\u7edf\u8bc4\u4f30\u9700\u8981\u66f4\u5168\u9762\u7684\u7ef4\u5ea6\u8003\u91cf\uff0c\u7279\u522b\u662f\u5728\u6587\u5316\u610f\u8bc6\u5b89\u5168\u65b9\u9762\u5b58\u5728\u663e\u8457\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5177\u6587\u5316\u654f\u611f\u6027\u7684\u5b89\u5168\u68c0\u6d4b\u65b9\u6cd5\u3002"}}
{"id": "2509.13388", "categories": ["cs.CV", "cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.13388", "abs": "https://arxiv.org/abs/2509.13388", "authors": ["Yadvendra Gurjar", "Ruoni Wan", "Ehsan Farahbakhsh", "Rohitash Chandra"], "title": "Landcover classification and change detection using remote sensing and machine learning: a case study of Western Fiji", "comment": null, "summary": "As a developing country, Fiji is facing rapid urbanisation, which is visible\nin the massive development projects that include housing, roads, and civil\nworks. In this study, we present machine learning and remote sensing frameworks\nto compare land use and land cover change from 2013 to 2024 in Nadi, Fiji. The\nultimate goal of this study is to provide technical support in land cover/land\nuse modelling and change detection. We used Landsat-8 satellite image for the\nstudy region and created our training dataset with labels for supervised\nmachine learning. We used Google Earth Engine and unsupervised machine learning\nvia k-means clustering to generate the land cover map. We used convolutional\nneural networks to classify the selected regions' land cover types. We present\na visualisation of change detection, highlighting urban area changes over time\nto monitor changes in the map.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u548c\u9065\u611f\u6280\u672f\u5206\u6790\u6590\u6d4e\u7eb3\u8fea\u5730\u533a2013-2024\u5e74\u7684\u571f\u5730\u5229\u7528\u53d8\u5316\uff0c\u901a\u8fc7Google Earth Engine\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u571f\u5730\u8986\u76d6\u5206\u7c7b\u548c\u53d8\u5316\u68c0\u6d4b\u3002", "motivation": "\u6590\u6d4e\u4f5c\u4e3a\u53d1\u5c55\u4e2d\u56fd\u5bb6\u6b63\u9762\u4e34\u5feb\u901f\u57ce\u5e02\u5316\uff0c\u9700\u8981\u6280\u672f\u652f\u6301\u6765\u76d1\u6d4b\u571f\u5730\u5229\u7528\u53d8\u5316\uff0c\u4e3a\u57ce\u5e02\u89c4\u5212\u548c\u53d1\u5c55\u9879\u76ee\u63d0\u4f9b\u6570\u636e\u652f\u6491\u3002", "method": "\u4f7f\u7528Landsat-8\u536b\u661f\u5f71\u50cf\uff0c\u7ed3\u5408Google Earth Engine\u5e73\u53f0\uff0c\u91c7\u7528k-means\u805a\u7c7b\u65e0\u76d1\u7763\u5b66\u4e60\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6709\u76d1\u7763\u5b66\u4e60\u8fdb\u884c\u571f\u5730\u8986\u76d6\u5206\u7c7b\u3002", "result": "\u751f\u6210\u4e86\u571f\u5730\u8986\u76d6\u53d8\u5316\u53ef\u89c6\u5316\u56fe\uff0c\u7a81\u51fa\u4e86\u57ce\u5e02\u533a\u57df\u968f\u65f6\u95f4\u7684\u53d8\u5316\uff0c\u6210\u529f\u76d1\u6d4b\u4e86\u571f\u5730\u5229\u7528\u53d8\u5316\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u571f\u5730\u5229\u7528\u5efa\u6a21\u548c\u53d8\u5316\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6280\u672f\u652f\u6491\uff0c\u6709\u52a9\u4e8e\u76d1\u6d4b\u57ce\u5e02\u5316\u8fdb\u7a0b\u4e2d\u7684\u571f\u5730\u5229\u7528\u53d8\u5316\u3002"}}
{"id": "2509.13624", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13624", "abs": "https://arxiv.org/abs/2509.13624", "authors": ["Shambhavi Krishna", "Atharva Naik", "Chaitali Agarwal", "Sudharshan Govindan", "Taesung Lee", "Haw-Shiuan Chang"], "title": "Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning", "comment": "Camera-ready version. Accepted to appear in the proceedings of the\n  14th Joint Conference on Lexical and Computational Semantics (*SEM 2025)", "summary": "Large language models are increasingly deployed across diverse applications.\nThis often includes tasks LLMs have not encountered during training. This\nimplies that enumerating and obtaining the high-quality training data for all\ntasks is infeasible. Thus, we often need to rely on transfer learning using\ndatasets with different characteristics, and anticipate out-of-distribution\nrequests. Motivated by this practical need, we propose an analysis framework,\nbuilding a transfer learning matrix and dimensionality reduction, to dissect\nthese cross-task interactions. We train and analyze 10 models to identify\nlatent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic)\nand discover the side effects of the transfer learning. Our findings reveal\nthat performance improvements often defy explanations based on surface-level\ndataset similarity or source data quality. Instead, hidden statistical factors\nof the source dataset, such as class distribution and generation length\nproclivities, alongside specific linguistic features, are actually more\ninfluential. This work offers insights into the complex dynamics of transfer\nlearning, paving the way for more predictable and effective LLM adaptation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u6790\u6846\u67b6\u6765\u7814\u7a76LLM\u8de8\u4efb\u52a1\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u6f5c\u5728\u80fd\u529b\u548c\u526f\u4f5c\u7528\uff0c\u53d1\u73b0\u6027\u80fd\u63d0\u5347\u4e3b\u8981\u53d7\u9690\u85cf\u7edf\u8ba1\u56e0\u7d20\u800c\u975e\u8868\u9762\u6570\u636e\u96c6\u76f8\u4f3c\u6027\u5f71\u54cd", "motivation": "\u7531\u4e8eLLM\u90e8\u7f72\u5230\u5404\u79cd\u65b0\u4efb\u52a1\u65f6\u65e0\u6cd5\u83b7\u5f97\u6240\u6709\u4efb\u52a1\u7684\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u9700\u8981\u4f9d\u8d56\u4e0d\u540c\u7279\u5f81\u7684\u8fc1\u79fb\u5b66\u4e60\u6765\u5904\u7406\u5206\u5e03\u5916\u8bf7\u6c42", "method": "\u6784\u5efa\u8fc1\u79fb\u5b66\u4e60\u77e9\u9635\u548c\u964d\u7ef4\u5206\u6790\u6846\u67b6\uff0c\u8bad\u7ec310\u4e2a\u6a21\u578b\u6765\u8bc6\u522b\u6f5c\u5728\u80fd\u529b\uff08\u63a8\u7406\u3001\u60c5\u611f\u5206\u7c7b\u3001\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u3001\u7b97\u672f\u7b49\uff09\u5e76\u5206\u6790\u8de8\u4efb\u52a1\u4ea4\u4e92", "result": "\u53d1\u73b0\u6027\u80fd\u6539\u8fdb\u5f80\u5f80\u65e0\u6cd5\u7528\u8868\u9762\u6570\u636e\u96c6\u76f8\u4f3c\u6027\u6216\u6e90\u6570\u636e\u8d28\u91cf\u6765\u89e3\u91ca\uff0c\u800c\u662f\u53d7\u6e90\u6570\u636e\u96c6\u7684\u9690\u85cf\u7edf\u8ba1\u56e0\u7d20\uff08\u7c7b\u522b\u5206\u5e03\u3001\u751f\u6210\u957f\u5ea6\u503e\u5411\uff09\u548c\u7279\u5b9a\u8bed\u8a00\u7279\u5f81\u5f71\u54cd\u66f4\u5927", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63ed\u793a\u4e86\u8fc1\u79fb\u5b66\u4e60\u7684\u590d\u6742\u52a8\u6001\uff0c\u4e3a\u66f4\u53ef\u9884\u6d4b\u548c\u6709\u6548\u7684LLM\u9002\u5e94\u94fa\u5e73\u4e86\u9053\u8def"}}
{"id": "2509.13396", "categories": ["cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.13396", "abs": "https://arxiv.org/abs/2509.13396", "authors": ["Xinan Wang", "Di Shi", "Fengyu Wang"], "title": "Real-Time Detection and Tracking of Foreign Object Intrusions in Power Systems via Feature-Based Edge Intelligence", "comment": "12 page Journal paper, accepted by IEEE Open Access Journal of Power\n  and Energy", "summary": "This paper presents a novel three-stage framework for real-time foreign\nobject intrusion (FOI) detection and tracking in power transmission systems.\nThe framework integrates: (1) a YOLOv7 segmentation model for fast and robust\nobject localization, (2) a ConvNeXt-based feature extractor trained with\ntriplet loss to generate discriminative embeddings, and (3) a feature-assisted\nIoU tracker that ensures resilient multi-object tracking under occlusion and\nmotion. To enable scalable field deployment, the pipeline is optimized for\ndeployment on low-cost edge hardware using mixed-precision inference. The\nsystem supports incremental updates by adding embeddings from previously unseen\nobjects into a reference database without requiring model retraining. Extensive\nexperiments on real-world surveillance and drone video datasets demonstrate the\nframework's high accuracy and robustness across diverse FOI scenarios. In\naddition, hardware benchmarks on NVIDIA Jetson devices confirm the framework's\npracticality and scalability for real-world edge applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7535\u529b\u4f20\u8f93\u7cfb\u7edf\u5b9e\u65f6\u5f02\u7269\u5165\u4fb5\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u7684\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u7ed3\u5408YOLOv7\u5206\u5272\u3001ConvNeXt\u7279\u5f81\u63d0\u53d6\u548c\u7279\u5f81\u8f85\u52a9IoU\u8ddf\u8e2a\uff0c\u652f\u6301\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u548c\u589e\u91cf\u66f4\u65b0\u3002", "motivation": "\u7535\u529b\u4f20\u8f93\u7cfb\u7edf\u4e2d\u5f02\u7269\u5165\u4fb5\u68c0\u6d4b\u5bf9\u7535\u7f51\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u5b9e\u65f6\u3001\u51c6\u786e\u4e14\u80fd\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fd0\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u8981\u5904\u7406\u906e\u6321\u3001\u8fd0\u52a8\u7b49\u590d\u6742\u573a\u666f\u3002", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1) YOLOv7\u5206\u5272\u6a21\u578b\u8fdb\u884c\u5feb\u901f\u76ee\u6807\u5b9a\u4f4d\uff1b2) ConvNeXt\u7279\u5f81\u63d0\u53d6\u5668\u914d\u5408\u4e09\u5143\u7ec4\u635f\u5931\u751f\u6210\u5224\u522b\u6027\u5d4c\u5165\uff1b3) \u7279\u5f81\u8f85\u52a9IoU\u8ddf\u8e2a\u5668\u5904\u7406\u906e\u6321\u548c\u591a\u76ee\u6807\u8ddf\u8e2a\u3002\u91c7\u7528\u6df7\u5408\u7cbe\u5ea6\u63a8\u7406\u4f18\u5316\u8fb9\u7f18\u90e8\u7f72\u3002", "result": "\u5728\u771f\u5b9e\u76d1\u63a7\u548c\u65e0\u4eba\u673a\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u5728\u5404\u79cd\u5f02\u7269\u5165\u4fb5\u573a\u666f\u4e0b\u5177\u6709\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002NVIDIA Jetson\u8bbe\u5907\u4e0a\u7684\u786c\u4ef6\u57fa\u51c6\u6d4b\u8bd5\u8bc1\u5b9e\u4e86\u5176\u5728\u5b9e\u9645\u8fb9\u7f18\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7535\u529b\u4f20\u8f93\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5b9e\u65f6\u5f02\u7269\u5165\u4fb5\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u8fb9\u7f18\u90e8\u7f72\u548c\u589e\u91cf\u5b66\u4e60\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.13664", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13664", "abs": "https://arxiv.org/abs/2509.13664", "authors": ["Zhuoxuan Zhang", "Jinhao Duan", "Edward Kim", "Kaidi Xu"], "title": "Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs", "comment": "To be appeared in EMNLP 2025 (main)", "summary": "Ambiguity is pervasive in real-world questions, yet large language models\n(LLMs) often respond with confident answers rather than seeking clarification.\nIn this work, we show that question ambiguity is linearly encoded in the\ninternal representations of LLMs and can be both detected and controlled at the\nneuron level. During the model's pre-filling stage, we identify that a small\nnumber of neurons, as few as one, encode question ambiguity information. Probes\ntrained on these Ambiguity-Encoding Neurons (AENs) achieve strong performance\non ambiguity detection and generalize across datasets, outperforming\nprompting-based and representation-based baselines. Layerwise analysis reveals\nthat AENs emerge from shallow layers, suggesting early encoding of ambiguity\nsignals in the model's processing pipeline. Finally, we show that through\nmanipulating AENs, we can control LLM's behavior from direct answering to\nabstention. Our findings reveal that LLMs form compact internal representations\nof question ambiguity, enabling interpretable and controllable behavior.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u5728\u6d45\u5c42\u795e\u7ecf\u5143\u4e2d\u7ebf\u6027\u7f16\u7801\u95ee\u9898\u6b67\u4e49\u4fe1\u606f\uff0c\u4ec5\u9700\u5c11\u91cf\u795e\u7ecf\u5143\u5373\u53ef\u68c0\u6d4b\u548c\u63a7\u5236\u6b67\u4e49\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u4e8e\u63d0\u793a\u548c\u8868\u793a\u7684\u65b9\u6cd5", "motivation": "\u73b0\u5b9e\u95ee\u9898\u666e\u904d\u5b58\u5728\u6b67\u4e49\u6027\uff0c\u4f46LLM\u5f80\u5f80\u7ed9\u51fa\u81ea\u4fe1\u56de\u7b54\u800c\u975e\u5bfb\u6c42\u6f84\u6e05\uff0c\u9700\u8981\u7406\u89e3LLM\u5982\u4f55\u5185\u90e8\u8868\u793a\u6b67\u4e49\u4fe1\u606f", "method": "\u5728\u6a21\u578b\u9884\u586b\u5145\u9636\u6bb5\u8bc6\u522b\u6b67\u4e49\u7f16\u7801\u795e\u7ecf\u5143(AENs)\uff0c\u8bad\u7ec3\u63a2\u9488\u8fdb\u884c\u6b67\u4e49\u68c0\u6d4b\uff0c\u901a\u8fc7\u64cd\u7eb5AENs\u63a7\u5236\u6a21\u578b\u884c\u4e3a", "result": "AENs\u63a2\u9488\u5728\u6b67\u4e49\u68c0\u6d4b\u4e0a\u8868\u73b0\u4f18\u5f02\u4e14\u5177\u6709\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\uff0c\u6d45\u5c42\u795e\u7ecf\u5143\u7f16\u7801\u6b67\u4e49\u4fe1\u606f\uff0c\u53ef\u901a\u8fc7\u795e\u7ecf\u5143\u64cd\u7eb5\u5b9e\u73b0\u4ece\u76f4\u63a5\u56de\u7b54\u5230\u5f03\u6743\u7684\u884c\u4e3a\u63a7\u5236", "conclusion": "LLM\u5f62\u6210\u7d27\u51d1\u7684\u5185\u90e8\u6b67\u4e49\u8868\u793a\uff0c\u652f\u6301\u53ef\u89e3\u91ca\u548c\u53ef\u63a7\u7684\u884c\u4e3a\uff0c\u4e3a\u6539\u5584LLM\u5bf9\u6b67\u4e49\u95ee\u9898\u7684\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84"}}
{"id": "2509.13399", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13399", "abs": "https://arxiv.org/abs/2509.13399", "authors": ["Tianyu Chen", "Yasi Zhang", "Zhi Zhang", "Peiyu Yu", "Shu Wang", "Zhendong Wang", "Kevin Lin", "Xiaofei Wang", "Zhengyuan Yang", "Linjie Li", "Chung-Ching Lin", "Jianwen Xie", "Oscar Leong", "Lijuan Wang", "Ying Nian Wu", "Mingyuan Zhou"], "title": "EdiVal-Agent: An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing", "comment": "Tianyu Chen and Yasi Zhang contributed equally; Oscar Leong, Lijuan\n  Wang, Ying Nian Wu, and Mingyuan Zhou advised equally", "summary": "Instruction-based image editing has advanced rapidly, yet reliable and\ninterpretable evaluation remains a bottleneck. Current protocols either (i)\ndepend on paired reference images -- resulting in limited coverage and\ninheriting biases from prior generative models -- or (ii) rely solely on\nzero-shot vision-language models (VLMs), whose prompt-based assessments of\ninstruction following, content consistency, and visual quality are often\nimprecise.\n  To address this, we introduce EdiVal-Agent, an automated, scalable, and\nfine-grained evaluation framework for multi-turn instruction-based editing from\nan object-centric perspective, supported by a suite of expert tools. Given an\nimage, EdiVal-Agent first decomposes it into semantically meaningful objects,\nthen synthesizes diverse, context-aware editing instructions. For evaluation,\nit integrates VLMs with open-vocabulary object detectors to assess instruction\nfollowing, uses semantic-level feature extractors to evaluate content\nconsistency, and leverages human preference models to judge visual quality. We\nshow that combining VLMs with object detectors yields stronger agreement with\nhuman judgments in instruction-following evaluation compared to using VLMs\nalone and CLIP-based metrics. Furthermore, the pipeline's modular design allows\nfuture tools to be seamlessly integrated, enhancing evaluation accuracy over\ntime.\n  Instantiating this pipeline, we build EdiVal-Bench, a multi-turn editing\nbenchmark covering 9 instruction types and 11 state-of-the-art editing models\nspanning autoregressive (AR) (including Nano Banana, GPT-Image-1),\nflow-matching, and diffusion paradigms. We demonstrate that EdiVal-Agent can be\nused to identify existing failure modes, thereby informing the development of\nthe next generation of editing models. Project page:\nhttps://tianyucodings.github.io/EdiVAL-page/.", "AI": {"tldr": "\u63d0\u51fa\u4e86EdiVal-Agent\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u8f6e\u6307\u4ee4\u56fe\u50cf\u7f16\u8f91\u7684\u7ec6\u7c92\u5ea6\u8bc4\u4f30\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u7269\u4f53\u68c0\u6d4b\u5668\u63d0\u5347\u8bc4\u4f30\u51c6\u786e\u6027\uff0c\u5e76\u6784\u5efa\u4e86EdiVal-Bench\u57fa\u51c6\u6d4b\u8bd5", "motivation": "\u5f53\u524d\u6307\u4ee4\u56fe\u50cf\u7f16\u8f91\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u8981\u4e48\u4f9d\u8d56\u914d\u5bf9\u7684\u53c2\u8003\u56fe\u50cf\u5bfc\u81f4\u8986\u76d6\u8303\u56f4\u6709\u9650\uff0c\u8981\u4e48\u4ec5\u4f7f\u7528\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bfc\u81f4\u8bc4\u4f30\u4e0d\u7cbe\u786e", "method": "\u5f00\u53d1\u4e86EdiVal-Agent\u6846\u67b6\uff0c\u9996\u5148\u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u8bed\u4e49\u5bf9\u8c61\uff0c\u7136\u540e\u5408\u6210\u591a\u6837\u5316\u7684\u7f16\u8f91\u6307\u4ee4\uff0c\u7ed3\u5408VLMs\u3001\u5f00\u653e\u8bcd\u6c47\u7269\u4f53\u68c0\u6d4b\u5668\u3001\u8bed\u4e49\u7279\u5f81\u63d0\u53d6\u5668\u548c\u4eba\u7c7b\u504f\u597d\u6a21\u578b\u8fdb\u884c\u591a\u7ef4\u5ea6\u8bc4\u4f30", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408VLMs\u548c\u7269\u4f53\u68c0\u6d4b\u5668\u5728\u6307\u4ee4\u9075\u5faa\u8bc4\u4f30\u4e2d\u6bd4\u5355\u72ec\u4f7f\u7528VLMs\u548cCLIP\u6307\u6807\u66f4\u80fd\u4e0e\u4eba\u7c7b\u5224\u65ad\u4e00\u81f4\uff0c\u6a21\u5757\u5316\u8bbe\u8ba1\u652f\u6301\u672a\u6765\u5de5\u5177\u96c6\u6210", "conclusion": "EdiVal-Agent\u80fd\u591f\u8bc6\u522b\u73b0\u6709\u7f16\u8f91\u6a21\u578b\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u7f16\u8f91\u6a21\u578b\u7684\u5f00\u53d1\u63d0\u4f9b\u4fe1\u606f\uff0c\u6846\u67b6\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u6301\u7eed\u6539\u8fdb\u80fd\u529b"}}
{"id": "2509.13672", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13672", "abs": "https://arxiv.org/abs/2509.13672", "authors": ["Shang Qin", "Jingheng Ye", "Yinghui Li", "Hai-Tao Zheng", "Qi Li", "Jinxiao Shan", "Zhixing Li", "Hong-Gee Kim"], "title": "CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction", "comment": null, "summary": "The growing demand for automated writing assistance in diverse academic\ndomains highlights the need for robust Chinese Grammatical Error Correction\n(CGEC) systems that can adapt across disciplines. However, existing CGEC\nresearch largely lacks dedicated benchmarks for multi-disciplinary academic\nwriting, overlooking continual learning (CL) as a promising solution to handle\ndomain-specific linguistic variation and prevent catastrophic forgetting. To\nfill this crucial gap, we introduce CL$^2$GEC, the first Continual Learning\nbenchmark for Chinese Literature Grammatical Error Correction, designed to\nevaluate adaptive CGEC across multiple academic fields. Our benchmark includes\n10,000 human-annotated sentences spanning 10 disciplines, each exhibiting\ndistinct linguistic styles and error patterns. CL$^2$GEC focuses on evaluating\ngrammatical error correction in a continual learning setting, simulating\nsequential exposure to diverse academic disciplines to reflect real-world\neditorial dynamics. We evaluate large language models under sequential tuning,\nparameter-efficient adaptation, and four representative CL algorithms, using\nboth standard GEC metrics and continual learning metrics adapted to task-level\nvariation. Experimental results reveal that regularization-based methods\nmitigate forgetting more effectively than replay-based or naive sequential\napproaches. Our benchmark provides a rigorous foundation for future research in\nadaptive grammatical error correction across diverse academic domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u4e2d\u6587\u6587\u732e\u8bed\u6cd5\u7ea0\u9519\u6301\u7eed\u5b66\u4e60\u57fa\u51c6CL\u00b2GEC\uff0c\u5305\u542b10\u4e2a\u5b66\u79d1\u768410,000\u6761\u6807\u6ce8\u53e5\u5b50\uff0c\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u5b66\u79d1\u8bed\u6cd5\u7ea0\u9519\u4e2d\u7684\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u4e2d\u6587\u8bed\u6cd5\u7ea0\u9519\u7814\u7a76\u7f3a\u4e4f\u591a\u5b66\u79d1\u5b66\u672f\u5199\u4f5c\u7684\u4e13\u7528\u57fa\u51c6\uff0c\u5ffd\u89c6\u4e86\u6301\u7eed\u5b66\u4e60\u5728\u5904\u7406\u9886\u57df\u7279\u5f02\u6027\u8bed\u8a00\u53d8\u5f02\u548c\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u6784\u5efa\u5305\u542b10\u4e2a\u5b66\u79d110,000\u53e5\u6807\u6ce8\u6570\u636e\u7684CL\u00b2GEC\u57fa\u51c6\uff0c\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u987a\u5e8f\u8c03\u4f18\u3001\u53c2\u6570\u9ad8\u6548\u9002\u5e94\u548c\u56db\u79cd\u4ee3\u8868\u6027\u6301\u7eed\u5b66\u4e60\u7b97\u6cd5\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u6b63\u5219\u5316\u7684\u65b9\u6cd5\u6bd4\u57fa\u4e8e\u56de\u653e\u6216\u7b80\u5355\u987a\u5e8f\u65b9\u6cd5\u66f4\u80fd\u6709\u6548\u7f13\u89e3\u9057\u5fd8\u95ee\u9898\u3002", "conclusion": "\u8be5\u57fa\u51c6\u4e3a\u8de8\u5b66\u79d1\u5b66\u672f\u9886\u57df\u7684\u81ea\u9002\u5e94\u8bed\u6cd5\u7ea0\u9519\u7814\u7a76\u63d0\u4f9b\u4e86\u4e25\u8c28\u7684\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u4e2d\u6587\u8bed\u6cd5\u7ea0\u9519\u7cfb\u7edf\u5728\u591a\u5b66\u79d1\u73af\u5883\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u53d1\u5c55\u3002"}}
{"id": "2509.13414", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13414", "abs": "https://arxiv.org/abs/2509.13414", "authors": ["Nikhil Keetha", "Norman M\u00fcller", "Johannes Sch\u00f6nberger", "Lorenzo Porzi", "Yuchen Zhang", "Tobias Fischer", "Arno Knapitsch", "Duncan Zauss", "Ethan Weber", "Nelson Antunes", "Jonathon Luiten", "Manuel Lopez-Antequera", "Samuel Rota Bul\u00f2", "Christian Richardt", "Deva Ramanan", "Sebastian Scherer", "Peter Kontschieder"], "title": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "comment": "Project Page: https://map-anything.github.io/", "summary": "We introduce MapAnything, a unified transformer-based feed-forward model that\ningests one or more images along with optional geometric inputs such as camera\nintrinsics, poses, depth, or partial reconstructions, and then directly\nregresses the metric 3D scene geometry and cameras. MapAnything leverages a\nfactored representation of multi-view scene geometry, i.e., a collection of\ndepth maps, local ray maps, camera poses, and a metric scale factor that\neffectively upgrades local reconstructions into a globally consistent metric\nframe. Standardizing the supervision and training across diverse datasets,\nalong with flexible input augmentation, enables MapAnything to address a broad\nrange of 3D vision tasks in a single feed-forward pass, including uncalibrated\nstructure-from-motion, calibrated multi-view stereo, monocular depth\nestimation, camera localization, depth completion, and more. We provide\nextensive experimental analyses and model ablations demonstrating that\nMapAnything outperforms or matches specialist feed-forward models while\noffering more efficient joint training behavior, thus paving the way toward a\nuniversal 3D reconstruction backbone.", "AI": {"tldr": "MapAnything\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u4e8etransformer\u7684\u524d\u9988\u6a21\u578b\uff0c\u80fd\u591f\u5904\u7406\u5355\u5f20\u6216\u591a\u5f20\u56fe\u50cf\u4ee5\u53ca\u53ef\u9009\u51e0\u4f55\u8f93\u5165\uff0c\u76f4\u63a5\u56de\u5f52\u5ea6\u91cf3D\u573a\u666f\u51e0\u4f55\u548c\u76f8\u673a\u53c2\u6570\uff0c\u5728\u591a\u79cd3D\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf3D\u91cd\u5efa\u65b9\u6cd5\u9700\u8981\u4e13\u95e8\u6a21\u578b\u5904\u7406\u4e0d\u540c\u4efb\u52a1\u7684\u95ee\u9898\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u4e2a\u7edf\u4e00\u7684\u6a21\u578b\u80fd\u591f\u5904\u7406\u591a\u79cd3D\u89c6\u89c9\u4efb\u52a1\uff0c\u5305\u62ec\u672a\u6807\u5b9a\u8fd0\u52a8\u6062\u590d\u7ed3\u6784\u3001\u591a\u89c6\u56fe\u7acb\u4f53\u89c6\u89c9\u3001\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7b49\u3002", "method": "\u91c7\u7528\u57fa\u4e8etransformer\u7684\u524d\u9988\u67b6\u6784\uff0c\u8f93\u5165\u56fe\u50cf\u548c\u53ef\u9009\u51e0\u4f55\u4fe1\u606f\uff0c\u4f7f\u7528\u5206\u89e3\u7684\u591a\u89c6\u56fe\u573a\u666f\u51e0\u4f55\u8868\u793a\uff08\u6df1\u5ea6\u56fe\u3001\u5c40\u90e8\u5c04\u7ebf\u56fe\u3001\u76f8\u673a\u4f4d\u59ff\u548c\u5ea6\u91cf\u5c3a\u5ea6\u56e0\u5b50\uff09\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u76d1\u7763\u548c\u7075\u6d3b\u8f93\u5165\u589e\u5f3a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMapAnything\u5728\u591a\u4e2a3D\u89c6\u89c9\u4efb\u52a1\u4e2d\u4f18\u4e8e\u6216\u5339\u914d\u4e13\u95e8\u7684\u524d\u9988\u6a21\u578b\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u8054\u5408\u8bad\u7ec3\u6027\u80fd\u3002", "conclusion": "MapAnything\u4e3a\u6784\u5efa\u901a\u75283D\u91cd\u5efa\u9aa8\u5e72\u7f51\u7edc\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5c55\u793a\u4e86\u5355\u4e00\u6a21\u578b\u5904\u7406\u591a\u6837\u53163D\u89c6\u89c9\u4efb\u52a1\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.13677", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.13677", "abs": "https://arxiv.org/abs/2509.13677", "authors": ["Xinxu Zhou", "Jiaqi Bai", "Zhenqi Sun", "Fanxiang Zeng", "Yue Liu"], "title": "AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation", "comment": null, "summary": "Although significant progress has been made in many tasks within the field of\nNatural Language Processing (NLP), Controlled Text Generation (CTG) continues\nto face numerous challenges, particularly in achieving fine-grained conditional\ncontrol over generation. Additionally, in real scenario and online\napplications, cost considerations, scalability, domain knowledge learning and\nmore precise control are required, presenting more challenge for CTG. This\npaper introduces a novel and scalable framework, AgentCTG, which aims to\nenhance precise and complex control over the text generation by simulating the\ncontrol and regulation mechanisms in multi-agent workflows. We explore various\ncollaboration methods among different agents and introduce an auto-prompt\nmodule to further enhance the generation effectiveness. AgentCTG achieves\nstate-of-the-art results on multiple public datasets. To validate its\neffectiveness in practical applications, we propose a new challenging\nCharacter-Driven Rewriting task, which aims to convert the original text into\nnew text that conform to specific character profiles and simultaneously\npreserve the domain knowledge. When applied to online navigation with\nrole-playing, our approach significantly enhances the driving experience\nthrough improved content delivery. By optimizing the generation of contextually\nrelevant text, we enable a more immersive interaction within online\ncommunities, fostering greater personalization and user engagement.", "AI": {"tldr": "AgentCTG\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4e2d\u7684\u63a7\u5236\u548c\u8c03\u8282\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6587\u672c\u751f\u6210\u7684\u7cbe\u786e\u590d\u6742\u63a7\u5236\uff0c\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u53d7\u63a7\u6587\u672c\u751f\u6210\u9762\u4e34\u7ec6\u7c92\u5ea6\u6761\u4ef6\u63a7\u5236\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u8981\u8003\u8651\u6210\u672c\u3001\u53ef\u6269\u5c55\u6027\u3001\u9886\u57df\u77e5\u8bc6\u5b66\u4e60\u548c\u66f4\u7cbe\u786e\u63a7\u5236\u7b49\u591a\u91cd\u56e0\u7d20\u3002", "method": "\u63d0\u51faAgentCTG\u6846\u67b6\uff0c\u6a21\u62df\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4e2d\u7684\u63a7\u5236\u548c\u8c03\u8282\u673a\u5236\uff0c\u63a2\u7d22\u4e0d\u540c\u667a\u80fd\u4f53\u95f4\u7684\u534f\u4f5c\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u81ea\u52a8\u63d0\u793a\u6a21\u5757\u6765\u589e\u5f3a\u751f\u6210\u6548\u679c\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5728\u89d2\u8272\u9a71\u52a8\u91cd\u5199\u4efb\u52a1\u4e2d\u80fd\u591f\u5c06\u539f\u59cb\u6587\u672c\u8f6c\u6362\u4e3a\u7b26\u5408\u7279\u5b9a\u89d2\u8272\u914d\u7f6e\u6587\u4ef6\u7684\u65b0\u6587\u672c\uff0c\u540c\u65f6\u4fdd\u7559\u9886\u57df\u77e5\u8bc6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5728\u7ebf\u5bfc\u822a\u548c\u89d2\u8272\u626e\u6f14\u5e94\u7528\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u9a7e\u9a76\u4f53\u9a8c\uff0c\u901a\u8fc7\u4f18\u5316\u4e0a\u4e0b\u6587\u76f8\u5173\u6587\u672c\u7684\u751f\u6210\uff0c\u5b9e\u73b0\u4e86\u66f4\u6c89\u6d78\u5f0f\u7684\u5728\u7ebf\u793e\u533a\u4e92\u52a8\uff0c\u4fc3\u8fdb\u4e86\u4e2a\u6027\u5316\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u3002"}}
{"id": "2509.13474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13474", "abs": "https://arxiv.org/abs/2509.13474", "authors": ["Yujia Lin", "Nicholas Evans"], "title": "Semantic-Enhanced Cross-Modal Place Recognition for Robust Robot Localization", "comment": null, "summary": "Ensuring accurate localization of robots in environments without GPS\ncapability is a challenging task. Visual Place Recognition (VPR) techniques can\npotentially achieve this goal, but existing RGB-based methods are sensitive to\nchanges in illumination, weather, and other seasonal changes. Existing\ncross-modal localization methods leverage the geometric properties of RGB\nimages and 3D LiDAR maps to reduce the sensitivity issues highlighted above.\nCurrently, state-of-the-art methods struggle in complex scenes, fine-grained or\nhigh-resolution matching, and situations where changes can occur in viewpoint.\nIn this work, we introduce a framework we call Semantic-Enhanced Cross-Modal\nPlace Recognition (SCM-PR) that combines high-level semantics utilizing RGB\nimages for robust localization in LiDAR maps. Our proposed method introduces: a\nVMamba backbone for feature extraction of RGB images; a Semantic-Aware Feature\nFusion (SAFF) module for using both place descriptors and segmentation masks;\nLiDAR descriptors that incorporate both semantics and geometry; and a\ncross-modal semantic attention mechanism in NetVLAD to improve matching.\nIncorporating the semantic information also was instrumental in designing a\nMulti-View Semantic-Geometric Matching and a Semantic Consistency Loss, both in\na contrastive learning framework. Our experimental work on the KITTI and\nKITTI-360 datasets show that SCM-PR achieves state-of-the-art performance\ncompared to other cross-modal place recognition methods.", "AI": {"tldr": "\u63d0\u51faSCM-PR\u6846\u67b6\uff0c\u7ed3\u5408RGB\u56fe\u50cf\u7684\u8bed\u4e49\u4fe1\u606f\u5b9e\u73b0\u9c81\u68d2\u7684\u6fc0\u5149\u96f7\u8fbe\u5730\u56fe\u5b9a\u4f4d\uff0c\u5728KITTI\u6570\u636e\u96c6\u4e0a\u8fbe\u5230state-of-the-art\u6027\u80fd", "motivation": "\u89e3\u51b3\u65e0GPS\u73af\u5883\u4e0b\u673a\u5668\u4eba\u5b9a\u4f4d\u95ee\u9898\uff0c\u73b0\u6709RGB\u65b9\u6cd5\u5bf9\u5149\u7167\u3001\u5929\u6c14\u548c\u5b63\u8282\u53d8\u5316\u654f\u611f\uff0c\u800c\u8de8\u6a21\u6001\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u3001\u7ec6\u7c92\u5ea6\u5339\u914d\u548c\u89c6\u89d2\u53d8\u5316\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73", "method": "\u4f7f\u7528VMamba\u9aa8\u5e72\u7f51\u7edc\u63d0\u53d6RGB\u7279\u5f81\uff0c\u63d0\u51fa\u8bed\u4e49\u611f\u77e5\u7279\u5f81\u878d\u5408\u6a21\u5757(SAFF)\uff0c\u7ed3\u5408\u4f4d\u7f6e\u63cf\u8ff0\u7b26\u548c\u5206\u5272\u63a9\u7801\uff0c\u8bbe\u8ba1\u5305\u542b\u8bed\u4e49\u548c\u51e0\u4f55\u7684\u6fc0\u5149\u96f7\u8fbe\u63cf\u8ff0\u7b26\uff0c\u5728NetVLAD\u4e2d\u5f15\u5165\u8de8\u6a21\u6001\u8bed\u4e49\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u8bbe\u8ba1\u591a\u89c6\u89d2\u8bed\u4e49\u51e0\u4f55\u5339\u914d\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u635f\u5931", "result": "\u5728KITTI\u548cKITTI-360\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u5176\u4ed6\u8de8\u6a21\u6001\u4f4d\u7f6e\u8bc6\u522b\u65b9\u6cd5\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "\u901a\u8fc7\u878d\u5408\u9ad8\u7ea7\u8bed\u4e49\u4fe1\u606f\uff0cSCM-PR\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u8de8\u6a21\u6001\u4f4d\u7f6e\u8bc6\u522b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u73af\u5883\u548c\u89c6\u89d2\u53d8\u5316\u60c5\u51b5\u4e0b"}}
{"id": "2509.13683", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13683", "abs": "https://arxiv.org/abs/2509.13683", "authors": ["Suyuchen Wang", "Jinlin Wang", "Xinyu Wang", "Shiqi Li", "Xiangru Tang", "Sirui Hong", "Xiao-Wen Chang", "Chenglin Wu", "Bang Liu"], "title": "Improving Context Fidelity via Native Retrieval-Augmented Reasoning", "comment": "Accepted as a main conference paper at EMNLP 2025", "summary": "Large language models (LLMs) often struggle with context fidelity, producing\ninconsistent answers when responding to questions based on provided\ninformation. Existing approaches either rely on expensive supervised\nfine-tuning to generate evidence post-answer or train models to perform web\nsearches without necessarily improving utilization of the given context. We\npropose CARE, a novel native retrieval-augmented reasoning framework that\nteaches LLMs to explicitly integrate in-context evidence within their reasoning\nprocess with the model's own retrieval capabilities. Our method requires\nlimited labeled evidence data while significantly enhancing both retrieval\naccuracy and answer generation performance through strategically retrieved\nin-context tokens in the reasoning chain. Extensive experiments on multiple\nreal-world and counterfactual QA benchmarks demonstrate that our approach\nsubstantially outperforms supervised fine-tuning, traditional\nretrieval-augmented generation methods, and external retrieval solutions. This\nwork represents a fundamental advancement in making LLMs more accurate,\nreliable, and efficient for knowledge-intensive tasks.", "AI": {"tldr": "CARE\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u539f\u751f\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u6559\u5bfcLLMs\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u663e\u5f0f\u6574\u5408\u4e0a\u4e0b\u6587\u8bc1\u636e\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u7d22\u51c6\u786e\u6027\u548c\u7b54\u6848\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u4fdd\u771f\u5ea6\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5f53\u57fa\u4e8e\u63d0\u4f9b\u7684\u4fe1\u606f\u56de\u7b54\u95ee\u9898\u65f6\u4f1a\u4ea7\u751f\u4e0d\u4e00\u81f4\u7684\u7b54\u6848\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6602\u8d35\u7684\u76d1\u7763\u5fae\u8c03\u6765\u751f\u6210\u7b54\u6848\u540e\u8bc1\u636e\uff0c\u8981\u4e48\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u7f51\u7edc\u641c\u7d22\u4f46\u672a\u5fc5\u6539\u5584\u7ed9\u5b9a\u4e0a\u4e0b\u6587\u7684\u5229\u7528\u3002", "method": "\u63d0\u51faCARE\u6846\u67b6\uff0c\u6559\u5bfcLLMs\u5229\u7528\u81ea\u8eab\u68c0\u7d22\u80fd\u529b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u663e\u5f0f\u6574\u5408\u4e0a\u4e0b\u6587\u8bc1\u636e\uff0c\u53ea\u9700\u6709\u9650\u7684\u6807\u8bb0\u8bc1\u636e\u6570\u636e\uff0c\u901a\u8fc7\u7b56\u7565\u6027\u68c0\u7d22\u7684\u4e0a\u4e0b\u6587\u6807\u8bb0\u6765\u589e\u5f3a\u63a8\u7406\u94fe\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u548c\u53cd\u4e8b\u5b9eQA\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\u3001\u4f20\u7edf\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u548c\u5916\u90e8\u68c0\u7d22\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4ee3\u8868\u4e86\u5728\u4f7fLLMs\u66f4\u51c6\u786e\u3001\u53ef\u9760\u548c\u9ad8\u6548\u5730\u5904\u7406\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u65b9\u9762\u7684\u6839\u672c\u6027\u8fdb\u6b65\u3002"}}
{"id": "2509.13482", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13482", "abs": "https://arxiv.org/abs/2509.13482", "authors": ["Hao Xu", "Xiaolin Wu", "Xi Zhang"], "title": "Improving 3D Gaussian Splatting Compression by Scene-Adaptive Lattice Vector Quantization", "comment": "Code available at https://github.com/hxu160/SALVQ", "summary": "3D Gaussian Splatting (3DGS) is rapidly gaining popularity for its\nphotorealistic rendering quality and real-time performance, but it generates\nmassive amounts of data. Hence compressing 3DGS data is necessary for the cost\neffectiveness of 3DGS models. Recently, several anchor-based neural compression\nmethods have been proposed, achieving good 3DGS compression performance.\nHowever, they all rely on uniform scalar quantization (USQ) due to its\nsimplicity. A tantalizing question is whether more sophisticated quantizers can\nimprove the current 3DGS compression methods with very little extra overhead\nand minimal change to the system. The answer is yes by replacing USQ with\nlattice vector quantization (LVQ). To better capture scene-specific\ncharacteristics, we optimize the lattice basis for each scene, improving LVQ's\nadaptability and R-D efficiency. This scene-adaptive LVQ (SALVQ) strikes a\nbalance between the R-D efficiency of vector quantization and the low\ncomplexity of USQ. SALVQ can be seamlessly integrated into existing 3DGS\ncompression architectures, enhancing their R-D performance with minimal\nmodifications and computational overhead. Moreover, by scaling the lattice\nbasis vectors, SALVQ can dynamically adjust lattice density, enabling a single\nmodel to accommodate multiple bit rate targets. This flexibility eliminates the\nneed to train separate models for different compression levels, significantly\nreducing training time and memory consumption.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u573a\u666f\u81ea\u9002\u5e94\u683c\u70b9\u5411\u91cf\u91cf\u5316(SALVQ)\u76843D\u9ad8\u65af\u6cfc\u6e85\u538b\u7f29\u65b9\u6cd5\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u5747\u5300\u6807\u91cf\u91cf\u5316\uff0c\u5728\u4fdd\u6301\u4f4e\u590d\u6742\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u7387\u5931\u771f\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u76843DGS\u538b\u7f29\u65b9\u6cd5\u90fd\u4f9d\u8d56\u7b80\u5355\u7684\u5747\u5300\u6807\u91cf\u91cf\u5316(USQ)\uff0c\u4f46\u66f4\u5148\u8fdb\u7684\u91cf\u5316\u5668\u53ef\u80fd\u4ee5\u6781\u5c0f\u7684\u989d\u5916\u5f00\u9500\u663e\u8457\u63d0\u5347\u538b\u7f29\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u683c\u70b9\u5411\u91cf\u91cf\u5316(LVQ)\u66ff\u4ee3USQ\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u573a\u666f\u4f18\u5316\u683c\u70b9\u57fa\u5411\u91cf\uff0c\u5b9e\u73b0\u573a\u666f\u81ea\u9002\u5e94\u7684LVQ(SALVQ)\u3002\u901a\u8fc7\u7f29\u653e\u683c\u70b9\u57fa\u5411\u91cf\u53ef\u4ee5\u52a8\u6001\u8c03\u6574\u91cf\u5316\u5bc6\u5ea6\u3002", "result": "SALVQ\u5728\u7387\u5931\u771f\u6548\u7387\u4e0a\u4f18\u4e8eUSQ\uff0c\u80fd\u591f\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u67093DGS\u538b\u7f29\u67b6\u6784\u4e2d\uff0c\u4ec5\u9700\u6700\u5c0f\u4fee\u6539\u548c\u8ba1\u7b97\u5f00\u9500\u3002\u5355\u4e2a\u6a21\u578b\u53ef\u652f\u6301\u591a\u79cd\u6bd4\u7279\u7387\u76ee\u6807\u3002", "conclusion": "SALVQ\u5728\u5411\u91cf\u91cf\u5316\u7684\u7387\u5931\u771f\u6548\u7387\u548c\u5747\u5300\u6807\u91cf\u91cf\u5316\u7684\u4f4e\u590d\u6742\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4e0d\u540c\u538b\u7f29\u7ea7\u522b\u6240\u9700\u7684\u8bad\u7ec3\u65f6\u95f4\u548c\u5185\u5b58\u6d88\u8017\u3002"}}
{"id": "2509.13695", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13695", "abs": "https://arxiv.org/abs/2509.13695", "authors": ["Yosuke Mikami", "Daiki Matsuoka", "Hitomi Yanaka"], "title": "Can Large Language Models Robustly Perform Natural Language Inference for Japanese Comparatives?", "comment": "To appear in Proceedings of the 16th International Conference on\n  Computational Semantics (IWCS 2025)", "summary": "Large Language Models (LLMs) perform remarkably well in Natural Language\nInference (NLI). However, NLI involving numerical and logical expressions\nremains challenging. Comparatives are a key linguistic phenomenon related to\nsuch inference, but the robustness of LLMs in handling them, especially in\nlanguages that are not dominant in the models' training data, such as Japanese,\nhas not been sufficiently explored. To address this gap, we construct a\nJapanese NLI dataset that focuses on comparatives and evaluate various LLMs in\nzero-shot and few-shot settings. Our results show that the performance of the\nmodels is sensitive to the prompt formats in the zero-shot setting and\ninfluenced by the gold labels in the few-shot examples. The LLMs also struggle\nto handle linguistic phenomena unique to Japanese. Furthermore, we observe that\nprompts containing logical semantic representations help the models predict the\ncorrect labels for inference problems that they struggle to solve even with\nfew-shot examples.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86\u65e5\u8bed\u6bd4\u8f83\u7ea7\u63a8\u7406\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86LLM\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u5bf9\u63d0\u793a\u683c\u5f0f\u654f\u611f\u4e14\u96be\u4ee5\u5904\u7406\u65e5\u8bed\u7279\u6709\u7684\u8bed\u8a00\u73b0\u8c61\uff0c\u4f46\u903b\u8f91\u8bed\u4e49\u8868\u793a\u80fd\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u6d89\u53ca\u6570\u503c\u548c\u903b\u8f91\u8868\u8fbe\u5f0f\u7684\u63a8\u7406\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\u3002\u6bd4\u8f83\u7ea7\u662f\u4e0e\u6b64\u7c7b\u63a8\u7406\u76f8\u5173\u7684\u5173\u952e\u8bed\u8a00\u73b0\u8c61\uff0c\u4f46LLM\u5728\u5904\u7406\u6bd4\u8f83\u7ea7\u65b9\u9762\u7684\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u4e0d\u5360\u4e3b\u5bfc\u5730\u4f4d\u7684\u8bed\u8a00\uff08\u5982\u65e5\u8bed\uff09\u4e2d\uff0c\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u6bd4\u8f83\u7ea7\u7684\u65e5\u8bedNLI\u6570\u636e\u96c6\uff0c\u5e76\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\u4e86\u5404\u79cdLLM\u3002\u5206\u6790\u4e86\u4e0d\u540c\u63d0\u793a\u683c\u5f0f\u7684\u5f71\u54cd\u4ee5\u53ca\u5c11\u6837\u672c\u793a\u4f8b\u4e2d\u9ec4\u91d1\u6807\u7b7e\u7684\u5f71\u54cd\u3002", "result": "\u6a21\u578b\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u5bf9\u63d0\u793a\u683c\u5f0f\u654f\u611f\uff0c\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u53d7\u5230\u9ec4\u91d1\u6807\u7b7e\u7684\u5f71\u54cd\u3002LLM\u96be\u4ee5\u5904\u7406\u65e5\u8bed\u7279\u6709\u7684\u8bed\u8a00\u73b0\u8c61\u3002\u5305\u542b\u903b\u8f91\u8bed\u4e49\u8868\u793a\u7684\u63d0\u793a\u6709\u52a9\u4e8e\u6a21\u578b\u9884\u6d4b\u90a3\u4e9b\u5373\u4f7f\u5728\u5c11\u6837\u672c\u793a\u4f8b\u4e2d\u4e5f\u96be\u4ee5\u89e3\u51b3\u7684\u63a8\u7406\u95ee\u9898\u7684\u6b63\u786e\u6807\u7b7e\u3002", "conclusion": "LLM\u5728\u5904\u7406\u65e5\u8bed\u6bd4\u8f83\u7ea7\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u8bed\u8a00\u7279\u6709\u73b0\u8c61\u65f6\u3002\u903b\u8f91\u8bed\u4e49\u8868\u793a\u53ef\u4ee5\u4f5c\u4e3a\u6709\u6548\u7684\u63d0\u793a\u7b56\u7565\u6765\u63d0\u5347\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\uff0c\u8fd9\u4e3a\u6539\u8fdb\u591a\u8bed\u8a00NLI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2509.13484", "categories": ["cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.13484", "abs": "https://arxiv.org/abs/2509.13484", "authors": ["Liu Liu", "Alexandra Kudaeva", "Marco Cipriano", "Fatimeh Al Ghannam", "Freya Tan", "Gerard de Melo", "Andres Sevtsuk"], "title": "MINGLE: VLMs for Semantically Complex Region Detection in Urban Scenes", "comment": "13 pages, 4 figures, under review at AAAI 2026", "summary": "Understanding group-level social interactions in public spaces is crucial for\nurban planning, informing the design of socially vibrant and inclusive\nenvironments. Detecting such interactions from images involves interpreting\nsubtle visual cues such as relations, proximity, and co-movement - semantically\ncomplex signals that go beyond traditional object detection. To address this\nchallenge, we introduce a social group region detection task, which requires\ninferring and spatially grounding visual regions defined by abstract\ninterpersonal relations. We propose MINGLE (Modeling INterpersonal Group-Level\nEngagement), a modular three-stage pipeline that integrates: (1) off-the-shelf\nhuman detection and depth estimation, (2) VLM-based reasoning to classify\npairwise social affiliation, and (3) a lightweight spatial aggregation\nalgorithm to localize socially connected groups. To support this task and\nencourage future research, we present a new dataset of 100K urban street-view\nimages annotated with bounding boxes and labels for both individuals and\nsocially interacting groups. The annotations combine human-created labels and\noutputs from the MINGLE pipeline, ensuring semantic richness and broad coverage\nof real-world scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86MINGLE\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u7ba1\u9053\u68c0\u6d4b\u56fe\u50cf\u4e2d\u7684\u793e\u4ea4\u7fa4\u4f53\u533a\u57df\uff0c\u5305\u542b\u4eba\u4f53\u68c0\u6d4b\u3001VLM\u793e\u4ea4\u5173\u7cfb\u5206\u7c7b\u548c\u7a7a\u95f4\u805a\u5408\u7b97\u6cd5\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b10\u4e07\u5f20\u8857\u666f\u56fe\u50cf\u7684\u65b0\u6570\u636e\u96c6\u3002", "motivation": "\u7406\u89e3\u516c\u5171\u573a\u6240\u7684\u7fa4\u4f53\u793e\u4ea4\u4e92\u52a8\u5bf9\u57ce\u5e02\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u4ece\u56fe\u50cf\u4e2d\u89e3\u8bfb\u5173\u7cfb\u3001\u63a5\u8fd1\u5ea6\u548c\u5171\u540c\u8fd0\u52a8\u7b49\u590d\u6742\u89c6\u89c9\u7ebf\u7d22\uff0c\u8fd9\u8d85\u51fa\u4e86\u4f20\u7edf\u76ee\u6807\u68c0\u6d4b\u7684\u8303\u56f4\u3002", "method": "MINGLE\u4e09\u9636\u6bb5\u7ba1\u9053\uff1a1)\u73b0\u6210\u7684\u4eba\u4f53\u68c0\u6d4b\u548c\u6df1\u5ea6\u4f30\u8ba1\uff1b2)\u57fa\u4e8eVLM\u7684\u6210\u5bf9\u793e\u4ea4\u5173\u7cfb\u5206\u7c7b\uff1b3)\u8f7b\u91cf\u7ea7\u7a7a\u95f4\u805a\u5408\u7b97\u6cd5\u5b9a\u4f4d\u793e\u4ea4\u8fde\u63a5\u7fa4\u4f53", "result": "\u521b\u5efa\u4e86\u5305\u542b10\u4e07\u5f20\u57ce\u5e02\u8857\u666f\u56fe\u50cf\u7684\u65b0\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u4e86\u4e2a\u4eba\u548c\u793e\u4ea4\u4e92\u52a8\u7fa4\u4f53\u7684\u8fb9\u754c\u6846\u548c\u6807\u7b7e\uff0c\u7ed3\u5408\u4eba\u5de5\u6807\u6ce8\u548cMINGLE\u8f93\u51fa\u786e\u4fdd\u8bed\u4e49\u4e30\u5bcc\u6027\u548c\u73b0\u5b9e\u573a\u666f\u8986\u76d6", "conclusion": "\u63d0\u51fa\u4e86\u793e\u4ea4\u7fa4\u4f53\u533a\u57df\u68c0\u6d4b\u65b0\u4efb\u52a1\u548cMINGLE\u65b9\u6cd5\uff0c\u4e3a\u7406\u89e3\u7fa4\u4f53\u793e\u4ea4\u4e92\u52a8\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u53d1\u5e03\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\u63a8\u52a8\u672a\u6765\u7814\u7a76"}}
{"id": "2509.13696", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13696", "abs": "https://arxiv.org/abs/2509.13696", "authors": ["Iyadh Ben Cheikh Larbi", "Ajay Madhavan Ravichandran", "Aljoscha Burchardt", "Roland Roller"], "title": "Integrating Text and Time-Series into (Large) Language Models to Predict Medical Outcomes", "comment": "Presented and published at BioCreative IX", "summary": "Large language models (LLMs) excel at text generation, but their ability to\nhandle clinical classification tasks involving structured data, such as time\nseries, remains underexplored. In this work, we adapt instruction-tuned LLMs\nusing DSPy-based prompt optimization to process clinical notes and structured\nEHR inputs jointly. Our results show that this approach achieves performance on\npar with specialized multimodal systems while requiring less complexity and\noffering greater adaptability across tasks.", "AI": {"tldr": "\u4f7f\u7528DSPy\u63d0\u793a\u4f18\u5316\u6280\u672f\u8ba9\u6307\u4ee4\u8c03\u4f18\u7684\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u540c\u65f6\u5904\u7406\u4e34\u5e8a\u6587\u672c\u548c\u7ed3\u6784\u5316\u7535\u5b50\u75c5\u5386\u6570\u636e\uff0c\u5728\u4e34\u5e8a\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e0e\u4e13\u7528\u591a\u6a21\u6001\u7cfb\u7edf\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4f46\u66f4\u7b80\u5355\u7075\u6d3b", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u5305\u542b\u7ed3\u6784\u5316\u6570\u636e\uff08\u5982\u65f6\u95f4\u5e8f\u5217\uff09\u7684\u4e34\u5e8a\u5206\u7c7b\u4efb\u52a1\u65b9\u9762\u4ecd\u6709\u5f85\u63a2\u7d22", "method": "\u91c7\u7528\u57fa\u4e8eDSPy\u7684\u63d0\u793a\u4f18\u5316\u6280\u672f\u6765\u9002\u914d\u6307\u4ee4\u8c03\u4f18\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u8054\u5408\u5904\u7406\u4e34\u5e8a\u7b14\u8bb0\u548c\u7ed3\u6784\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u8f93\u5165", "result": "\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4e0e\u4e13\u7528\u591a\u6a21\u6001\u7cfb\u7edf\u76f8\u5f53\uff0c\u540c\u65f6\u9700\u8981\u66f4\u5c11\u7684\u590d\u6742\u6027\uff0c\u5e76\u5728\u4e0d\u540c\u4efb\u52a1\u95f4\u5177\u6709\u66f4\u597d\u7684\u9002\u5e94\u6027", "conclusion": "\u901a\u8fc7\u63d0\u793a\u4f18\u5316\u6280\u672f\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u5904\u7406\u4e34\u5e8a\u591a\u6a21\u6001\u6570\u636e\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u4f9b\u66f4\u7b80\u5355\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.13496", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13496", "abs": "https://arxiv.org/abs/2509.13496", "authors": ["Rajatsubhra Chakraborty", "Xujun Che", "Depeng Xu", "Cori Faklaris", "Xi Niu", "Shuhan Yuan"], "title": "BiasMap: Leveraging Cross-Attentions to Discover and Mitigate Hidden Social Biases in Text-to-Image Generation", "comment": null, "summary": "Bias discovery is critical for black-box generative models, especiall\ntext-to-image (TTI) models. Existing works predominantly focus on output-level\ndemographic distributions, which do not necessarily guarantee concept\nrepresentations to be disentangled post-mitigation. We propose BiasMap, a\nmodel-agnostic framework for uncovering latent concept-level representational\nbiases in stable diffusion models. BiasMap leverages cross-attention\nattribution maps to reveal structural entanglements between demographics (e.g.,\ngender, race) and semantics (e.g., professions), going deeper into\nrepresentational bias during the image generation. Using attribution maps of\nthese concepts, we quantify the spatial demographics-semantics concept\nentanglement via Intersection over Union (IoU), offering a lens into bias that\nremains hidden in existing fairness discovery approaches. In addition, we\nfurther utilize BiasMap for bias mitigation through energy-guided diffusion\nsampling that directly modifies latent noise space and minimizes the expected\nSoftIoU during the denoising process. Our findings show that existing fairness\ninterventions may reduce the output distributional gap but often fail to\ndisentangle concept-level coupling, whereas our mitigation method can mitigate\nconcept entanglement in image generation while complementing distributional\nbias mitigation.", "AI": {"tldr": "BiasMap\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u53d1\u73b0\u7a33\u5b9a\u6269\u6563\u6a21\u578b\u4e2d\u7684\u6f5c\u5728\u6982\u5ff5\u7ea7\u8868\u5f81\u504f\u89c1\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u5f52\u56e0\u56fe\u63ed\u793a\u4eba\u53e3\u7edf\u8ba1\u5b66\u4e0e\u8bed\u4e49\u6982\u5ff5\u7684\u7ed3\u6784\u6027\u7ea0\u7f20\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u80fd\u91cf\u5f15\u5bfc\u6269\u6563\u91c7\u6837\u7684\u504f\u89c1\u7f13\u89e3\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u504f\u89c1\u53d1\u73b0\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8f93\u51fa\u5c42\u9762\u7684\u4eba\u53e3\u7edf\u8ba1\u5206\u5e03\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u504f\u89c1\u7f13\u89e3\u540e\u6982\u5ff5\u8868\u5f81\u7684\u89e3\u8026\u3002\u9700\u8981\u66f4\u6df1\u5c42\u6b21\u5730\u63a2\u7d22\u751f\u6210\u6a21\u578b\u4e2d\u7684\u8868\u5f81\u504f\u89c1\u3002", "method": "\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u5f52\u56e0\u56fe\u5206\u6790\u4eba\u53e3\u7edf\u8ba1\u5b66\uff08\u6027\u522b\u3001\u79cd\u65cf\uff09\u548c\u8bed\u4e49\uff08\u804c\u4e1a\uff09\u6982\u5ff5\u7684\u7a7a\u95f4\u7ea0\u7f20\uff0c\u901a\u8fc7IoU\u91cf\u5316\u6982\u5ff5\u8026\u5408\u7a0b\u5ea6\uff0c\u5e76\u91c7\u7528\u80fd\u91cf\u5f15\u5bfc\u6269\u6563\u91c7\u6837\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u6700\u5c0f\u5316SoftIoU\u6765\u7f13\u89e3\u504f\u89c1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u516c\u5e73\u6027\u5e72\u9884\u53ef\u80fd\u51cf\u5c11\u8f93\u51fa\u5206\u5e03\u5dee\u8ddd\uff0c\u4f46\u5f80\u5f80\u65e0\u6cd5\u89e3\u8026\u6982\u5ff5\u7ea7\u8026\u5408\uff0c\u800cBiasMap\u65b9\u6cd5\u80fd\u591f\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u7f13\u89e3\u6982\u5ff5\u7ea0\u7f20\uff0c\u540c\u65f6\u8865\u5145\u5206\u5e03\u504f\u89c1\u7f13\u89e3\u3002", "conclusion": "BiasMap\u63d0\u4f9b\u4e86\u53d1\u73b0\u548c\u7f13\u89e3\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4e2d\u6f5c\u5728\u6982\u5ff5\u7ea7\u8868\u5f81\u504f\u89c1\u7684\u65b0\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u68c0\u6d4b\u5230\u7684\u9690\u85cf\u504f\u89c1\uff0c\u5e76\u901a\u8fc7\u76f4\u63a5\u4fee\u6539\u6f5c\u5728\u566a\u58f0\u7a7a\u95f4\u5b9e\u73b0\u6709\u6548\u7684\u504f\u89c1\u7f13\u89e3\u3002"}}
{"id": "2509.13702", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13702", "abs": "https://arxiv.org/abs/2509.13702", "authors": ["Xiao Zheng"], "title": "DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models", "comment": null, "summary": "Large Language Model (LLM) hallucination is a significant barrier to their\nreliable deployment. Current methods like Retrieval-Augmented Generation (RAG)\nare often reactive. We introduce **Dynamic Self-reinforcing Calibration for\nHallucination Suppression (DSCC-HS)**, a novel, proactive framework that\nintervenes during autoregressive decoding. Inspired by dual-process cognitive\ntheory, DSCC-HS uses a compact proxy model, trained in adversarial roles as a\nFactual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP). During\ninference, these proxies dynamically steer a large target model by injecting a\nreal-time steering vector, which is the difference between FAP and HDP logits,\nat each decoding step. This plug-and-play approach requires no modification to\nthe target model. Our experiments on TruthfulQA and BioGEN show DSCC-HS\nachieves state-of-the-art performance. On TruthfulQA, it reached a 99.2%\nFactual Consistency Rate (FCR). On the long-form BioGEN benchmark, it attained\nthe highest FActScore of 46.50. These results validate DSCC-HS as a principled\nand efficient solution for enhancing LLM factuality.", "AI": {"tldr": "DSCC-HS\u662f\u4e00\u79cd\u65b0\u9896\u7684\u4e3b\u52a8\u5f0f\u5e7b\u89c9\u6291\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u7684\u7d27\u51d1\u4ee3\u7406\u6a21\u578b\u5728\u81ea\u56de\u5f52\u89e3\u7801\u8fc7\u7a0b\u4e2d\u5b9e\u65f6\u5f15\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u65e0\u9700\u4fee\u6539\u76ee\u6807\u6a21\u578b\u5373\u53ef\u663e\u8457\u63d0\u5347\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982RAG\u901a\u5e38\u662f\u53cd\u5e94\u5f0f\u7684\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u4e3b\u52a8\u5e72\u9884\u673a\u5236\u6765\u6291\u5236LLM\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u9ad8\u90e8\u7f72\u53ef\u9760\u6027\u3002", "method": "\u57fa\u4e8e\u53cc\u8fc7\u7a0b\u8ba4\u77e5\u7406\u8bba\uff0c\u8bad\u7ec3\u4e24\u4e2a\u5bf9\u6297\u89d2\u8272\u4ee3\u7406\u6a21\u578b\uff08\u4e8b\u5b9e\u5bf9\u9f50\u4ee3\u7406FAP\u548c\u5e7b\u89c9\u68c0\u6d4b\u4ee3\u7406HDP\uff09\uff0c\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u8ba1\u7b97\u4e24\u8005logits\u5dee\u5f02\u751f\u6210\u5b9e\u65f6\u5f15\u5bfc\u5411\u91cf\uff0c\u5728\u6bcf\u4e00\u6b65\u89e3\u7801\u8fc7\u7a0b\u4e2d\u52a8\u6001\u5f15\u5bfc\u76ee\u6807\u6a21\u578b\u3002", "result": "\u5728TruthfulQA\u4e0a\u8fbe\u523099.2%\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u7387\uff0c\u5728BioGEN\u957f\u6587\u672c\u57fa\u51c6\u4e0a\u83b7\u5f97\u6700\u9ad8FActScore 46.50\uff0c\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "DSCC-HS\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5373\u63d2\u5373\u7528\u7684\u65b9\u5f0f\u6709\u6548\u589e\u5f3aLLM\u7684\u4e8b\u5b9e\u6027\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.13504", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13504", "abs": "https://arxiv.org/abs/2509.13504", "authors": ["Uriel Garcilazo-Cruz", "Joseph O. Okeme", "Rodrigo A. Vargas--Hern\u00e1ndez"], "title": "LivePyxel: Accelerating image annotations with a Python-integrated webcam live streaming", "comment": "8 pages, 10 figures, SM, 5 pages, 4 figures", "summary": "The lack of flexible annotation tools has hindered the deployment of AI\nmodels in some scientific areas. Most existing image annotation software\nrequires users to upload a precollected dataset, which limits support for\non-demand pipelines and introduces unnecessary steps to acquire images. This\nconstraint is particularly problematic in laboratory environments, where\nreal-time data acquisition from instruments such as microscopes is increasingly\ncommon. In this work, we introduce \\texttt{LivePixel}, a Python-based graphical\nuser interface that integrates with imaging systems, such as webcams,\nmicroscopes, and others, to enable real-time image annotation. LivePyxel is\ndesigned to be easy to use through a simple interface that allows users to\nprecisely delimit areas for annotation using tools commonly found in commercial\ngraphics editing software. Of particular interest is the availability of\nB\\'ezier splines and binary masks, and the software's capacity to work with\nnon-destructive layers that enable high-performance editing. LivePyxel also\nintegrates a wide compatibility across video devices, and it's optimized for\nobject detection operations via the use of OpenCV in combination with\nhigh-performance libraries designed to handle matrix and linear algebra\noperations via Numpy effectively. LivePyxel facilitates seamless data\ncollection and labeling, accelerating the development of AI models in\nexperimental workflows. LivePyxel freely available at\nhttps://github.com/UGarCil/LivePyxel", "AI": {"tldr": "LivePixel\u662f\u4e00\u4e2a\u57fa\u4e8ePython\u7684\u5b9e\u65f6\u56fe\u50cf\u6807\u6ce8\u5de5\u5177\uff0c\u53ef\u76f4\u63a5\u8fde\u63a5\u6210\u50cf\u8bbe\u5907\u8fdb\u884c\u5b9e\u65f6\u6807\u6ce8\uff0c\u652f\u6301\u8d1d\u585e\u5c14\u66f2\u7ebf\u548c\u4e8c\u8fdb\u5236\u63a9\u7801\u7b49\u4e13\u4e1a\u6807\u6ce8\u529f\u80fd", "motivation": "\u73b0\u6709\u56fe\u50cf\u6807\u6ce8\u5de5\u5177\u9700\u8981\u5148\u4e0a\u4f20\u9884\u6536\u96c6\u6570\u636e\u96c6\uff0c\u65e0\u6cd5\u652f\u6301\u5b9e\u65f6\u6570\u636e\u91c7\u96c6\u548c\u6309\u9700\u6807\u6ce8\uff0c\u8fd9\u5728\u5b9e\u9a8c\u5ba4\u5b9e\u65f6\u4eea\u5668\u6570\u636e\u91c7\u96c6\u573a\u666f\u4e2d\u5c24\u4e3a\u4e0d\u4fbf", "method": "\u5f00\u53d1Python\u56fe\u5f62\u754c\u9762\u5de5\u5177\uff0c\u96c6\u6210OpenCV\u548cNumpy\u9ad8\u6027\u80fd\u5e93\uff0c\u652f\u6301\u591a\u79cd\u89c6\u9891\u8bbe\u5907\u8fde\u63a5\uff0c\u63d0\u4f9b\u7c7b\u4f3c\u5546\u4e1a\u56fe\u5f62\u8f6f\u4ef6\u7684\u6807\u6ce8\u5de5\u5177\u548c\u975e\u7834\u574f\u6027\u56fe\u5c42\u7f16\u8f91", "result": "\u5b9e\u73b0\u4e86\u5b9e\u65f6\u56fe\u50cf\u6807\u6ce8\u529f\u80fd\uff0c\u652f\u6301\u8d1d\u585e\u5c14\u66f2\u7ebf\u3001\u4e8c\u8fdb\u5236\u63a9\u7801\u7b49\u4e13\u4e1a\u6807\u6ce8\u65b9\u5f0f\uff0c\u53ef\u4e0e\u663e\u5fae\u955c\u3001\u7f51\u7edc\u6444\u50cf\u5934\u7b49\u6210\u50cf\u8bbe\u5907\u76f4\u63a5\u96c6\u6210", "conclusion": "LivePixel\u89e3\u51b3\u4e86\u79d1\u5b66\u9886\u57dfAI\u6a21\u578b\u90e8\u7f72\u4e2d\u7684\u6807\u6ce8\u5de5\u5177\u9650\u5236\uff0c\u52a0\u901f\u4e86\u5b9e\u9a8c\u5de5\u4f5c\u6d41\u7a0b\u4e2dAI\u6a21\u578b\u7684\u5f00\u53d1"}}
{"id": "2509.13706", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13706", "abs": "https://arxiv.org/abs/2509.13706", "authors": ["Peter Beidler", "Mark Nguyen", "Kevin Lybarger", "Ola Holmberg", "Eric Ford", "John Kang"], "title": "Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models", "comment": null, "summary": "PURPOSE: Incident reports are an important tool for safety and quality\nimprovement in healthcare, but manual review is time-consuming and requires\nsubject matter expertise. Here we present a natural language processing (NLP)\nscreening tool to detect high-severity incident reports in radiation oncology\nacross two institutions.\n  METHODS AND MATERIALS: We used two text datasets to train and evaluate our\nNLP models: 7,094 reports from our institution (Inst.), and 571 from IAEA\nSAFRON (SF), all of which had severity scores labeled by clinical content\nexperts. We trained and evaluated two types of models: baseline support vector\nmachines (SVM) and BlueBERT which is a large language model pretrained on\nPubMed abstracts and hospitalized patient data. We assessed for\ngeneralizability of our model in two ways. First, we evaluated models trained\nusing Inst.-train on SF-test. Second, we trained a BlueBERT_TRANSFER model that\nwas first fine-tuned on Inst.-train then on SF-train before testing on SF-test\nset. To further analyze model performance, we also examined a subset of 59\nreports from our Inst. dataset, which were manually edited for clarity.\n  RESULTS Classification performance on the Inst. test achieved AUROC 0.82\nusing SVM and 0.81 using BlueBERT. Without cross-institution transfer learning,\nperformance on the SF test was limited to an AUROC of 0.42 using SVM and 0.56\nusing BlueBERT. BlueBERT_TRANSFER, which was fine-tuned on both datasets,\nimproved the performance on SF test to AUROC 0.78. Performance of SVM, and\nBlueBERT_TRANSFER models on the manually curated Inst. reports (AUROC 0.85 and\n0.74) was similar to human performance (AUROC 0.81).\n  CONCLUSION: In summary, we successfully developed cross-institution NLP\nmodels on incident report text from radiation oncology centers. These models\nwere able to detect high-severity reports similarly to humans on a curated\ndataset.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2aNLP\u7b5b\u67e5\u5de5\u5177\uff0c\u7528\u4e8e\u81ea\u52a8\u68c0\u6d4b\u653e\u5c04\u80bf\u7624\u5b66\u4e2d\u7684\u9ad8\u4e25\u91cd\u6027\u4e8b\u4ef6\u62a5\u544a\uff0c\u901a\u8fc7\u8de8\u673a\u6784\u8fc1\u79fb\u5b66\u4e60\u5b9e\u73b0\u4e86\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u76f8\u5f53\u7684\u68c0\u6d4b\u6027\u80fd", "motivation": "\u533b\u7597\u4e8b\u4ef6\u62a5\u544a\u7684\u624b\u52a8\u5ba1\u67e5\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u6765\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u8d28\u91cf\u6539\u8fdb\u7684\u6548\u7387", "method": "\u4f7f\u7528\u652f\u6301\u5411\u91cf\u673a(SVM)\u548cBlueBERT\u6a21\u578b\uff0c\u5728\u4e24\u4e2a\u673a\u6784\u76847,094\u4efd\u62a5\u544a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u91c7\u7528\u8de8\u673a\u6784\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565", "result": "\u5728\u673a\u6784\u6d4b\u8bd5\u96c6\u4e0aAUROC\u8fbe\u52300.82(SVM)\u548c0.81(BlueBERT)\uff0c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u5c06\u8de8\u673a\u6784\u6027\u80fd\u4ece0.56\u63d0\u5347\u52300.78\uff0c\u5728\u4eba\u5de5\u7f16\u8f91\u6570\u636e\u96c6\u4e0a\u4e0e\u4eba\u7c7b\u6027\u80fd\u76f8\u5f53(AUROC 0.81)", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86\u8de8\u673a\u6784NLP\u6a21\u578b\uff0c\u80fd\u591f\u50cf\u4eba\u7c7b\u4e13\u5bb6\u4e00\u6837\u68c0\u6d4b\u9ad8\u4e25\u91cd\u6027\u4e8b\u4ef6\u62a5\u544a\uff0c\u4e3a\u533b\u7597\u5b89\u5168\u8d28\u91cf\u6539\u8fdb\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u52a8\u5316\u5de5\u5177"}}
{"id": "2509.13506", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13506", "abs": "https://arxiv.org/abs/2509.13506", "authors": ["Xingzi Xu", "Qi Li", "Shuwen Qiu", "Julien Han", "Karim Bouyarmane"], "title": "DEFT-VTON: Efficient Virtual Try-On with Consistent Generalised H-Transform", "comment": "Published in 2025 CVPR Workshop", "summary": "Diffusion models enable high-quality virtual try-on (VTO) with their\nestablished image synthesis abilities. Despite the extensive end-to-end\ntraining of large pre-trained models involved in current VTO methods,\nreal-world applications often prioritize limited training and inference,\nserving, and deployment budgets for VTO. To solve this obstacle, we apply\nDoob's h-transform efficient fine-tuning (DEFT) for adapting large pre-trained\nunconditional models for downstream image-conditioned VTO abilities. DEFT\nfreezes the pre-trained model's parameters and trains a small h-transform\nnetwork to learn a conditional h-transform. The h-transform network allows\ntraining only 1.42 percent of the frozen parameters, compared to a baseline of\n5.52 percent in traditional parameter-efficient fine-tuning (PEFT).\n  To further improve DEFT's performance and decrease existing models' inference\ntime, we additionally propose an adaptive consistency loss. Consistency\ntraining distills slow but high-performing diffusion models into a fast one\nwhile retaining performance by enforcing consistencies along the inference\npath. Inspired by constrained optimization, instead of distillation, we combine\nthe consistency loss and the denoising score matching loss in a data-adaptive\nmanner for fine-tuning existing VTO models at a low cost. Empirical results\nshow the proposed DEFT-VTON method achieves state-of-the-art performance on VTO\ntasks, with as few as 15 denoising steps, while maintaining competitive\nresults.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDEFT-VTON\u65b9\u6cd5\uff0c\u901a\u8fc7Doob's h-transform\u9ad8\u6548\u5fae\u8c03\u6280\u672f\uff0c\u4ec5\u8bad\u7ec31.42%\u7684\u53c2\u6570\u5b9e\u73b0\u865a\u62df\u8bd5\u7a7f\uff0c\u5e76\u7ed3\u5408\u81ea\u9002\u5e94\u4e00\u81f4\u6027\u635f\u5931\u5c06\u63a8\u7406\u6b65\u9aa4\u51cf\u5c11\u523015\u6b65\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2d\u865a\u62df\u8bd5\u7a7f(VTO)\u9700\u8981\u6709\u9650\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u9884\u7b97\uff0c\u800c\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u4f7f\u7528DEFT\u51bb\u7ed3\u9884\u8bad\u7ec3\u6a21\u578b\u53c2\u6570\uff0c\u8bad\u7ec3\u5c0f\u578bh-transform\u7f51\u7edc\u5b66\u4e60\u6761\u4ef6\u53d8\u6362\uff1b\u63d0\u51fa\u81ea\u9002\u5e94\u4e00\u81f4\u6027\u635f\u5931\uff0c\u5c06\u4e00\u81f4\u6027\u635f\u5931\u548c\u53bb\u566a\u5f97\u5206\u5339\u914d\u635f\u5931\u4ee5\u6570\u636e\u81ea\u9002\u5e94\u65b9\u5f0f\u7ed3\u5408\u8fdb\u884c\u5fae\u8c03\u3002", "result": "DEFT-VTON\u5728VTO\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4ec5\u970015\u4e2a\u53bb\u566a\u6b65\u9aa4\uff0c\u53c2\u6570\u91cf\u4ec5\u9700\u8bad\u7ec31.42%\uff08\u76f8\u6bd4\u4f20\u7edfPEFT\u76845.52%\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4ee5\u6781\u4f4e\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u865a\u62df\u8bd5\u7a7f\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13723", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13723", "abs": "https://arxiv.org/abs/2509.13723", "authors": ["Yaxin Gao", "Yao Lu", "Zongfei Zhang", "Jiaqi Nie", "Shanqing Yu", "Qi Xuan"], "title": "DSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success in many natural\nlanguage processing (NLP) tasks. To achieve more accurate output, the prompts\nused to drive LLMs have become increasingly longer, which incurs higher\ncomputational costs. To address this prompt inflation problem, prompt\ncompression has been proposed. However, most existing methods require training\na small auxiliary model for compression, incurring a significant amount of\nadditional computation. To avoid this, we propose a two-stage, training-free\napproach, called Dual-Stage Progressive Compression (DSPC). In the\ncoarse-grained stage, semantic-related sentence filtering removes sentences\nwith low semantic value based on TF-IDF. In the fine-grained stage, token\nimportance is assessed using attention contribution, cross-model loss\ndifference, and positional importance, enabling the pruning of low-utility\ntokens while preserving semantics. We validate DSPC on LLaMA-3.1-8B-Instruct\nand GPT-3.5-Turbo under a constrained token budget and observe consistent\nimprovements. For instance, in the FewShot task of the Longbench dataset, DSPC\nachieves a performance of 49.17 by using only 3x fewer tokens, outperforming\nthe best state-of-the-art baseline LongLLMLingua by 7.76.", "AI": {"tldr": "\u63d0\u51faDSPC\u53cc\u9636\u6bb5\u65e0\u8bad\u7ec3\u63d0\u793a\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u53e5\u5b50\u8fc7\u6ee4\u548ctoken\u526a\u679d\u5728\u51cf\u5c113\u500dtoken\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd7.76\u5206", "motivation": "\u89e3\u51b3LLM\u63d0\u793a\u8bcd\u81a8\u80c0\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u589e\u52a0\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u8bad\u7ec3\u8f85\u52a9\u6a21\u578b\u5e26\u6765\u989d\u5916\u8ba1\u7b97\u5f00\u9500", "method": "\u4e24\u9636\u6bb5\u65e0\u8bad\u7ec3\u538b\u7f29\uff1a\u7c97\u7c92\u5ea6\u9636\u6bb5\u57fa\u4e8eTF-IDF\u8bed\u4e49\u8fc7\u6ee4\u4f4e\u4ef7\u503c\u53e5\u5b50\uff1b\u7ec6\u7c92\u5ea6\u9636\u6bb5\u901a\u8fc7\u6ce8\u610f\u529b\u8d21\u732e\u3001\u8de8\u6a21\u578b\u635f\u5931\u5dee\u5f02\u548c\u4f4d\u7f6e\u91cd\u8981\u6027\u8bc4\u4f30token\u91cd\u8981\u6027\u8fdb\u884c\u526a\u679d", "result": "\u5728LLaMA-3.1-8B-Instruct\u548cGPT-3.5-Turbo\u4e0a\u9a8c\u8bc1\uff0c\u5728Longbench\u6570\u636e\u96c6\u7684FewShot\u4efb\u52a1\u4e2d\u4ec5\u75281/3 token\u8fbe\u523049.17\u6027\u80fd\uff0c\u6bd4\u6700\u4f73\u57fa\u7ebfLongLLMLingua\u63d0\u53477.76", "conclusion": "DSPC\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u63d0\u793a\u538b\u7f29\u95ee\u9898\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u6a21\u578b\u6027\u80fd"}}
{"id": "2509.13507", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13507", "abs": "https://arxiv.org/abs/2509.13507", "authors": ["Artem Savkin", "Thomas Lapotre", "Kevin Strauss", "Uzair Akbar", "Federico Tombari"], "title": "Adversarial Appearance Learning in Augmented Cityscapes for Pedestrian Recognition in Autonomous Driving", "comment": null, "summary": "In the autonomous driving area synthetic data is crucial for cover specific\ntraffic scenarios which autonomous vehicle must handle. This data commonly\nintroduces domain gap between synthetic and real domains. In this paper we\ndeploy data augmentation to generate custom traffic scenarios with VRUs in\norder to improve pedestrian recognition. We provide a pipeline for augmentation\nof the Cityscapes dataset with virtual pedestrians. In order to improve\naugmentation realism of the pipeline we reveal a novel generative network\narchitecture for adversarial learning of the data-set lighting conditions. We\nalso evaluate our approach on the tasks of semantic and instance segmentation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u589e\u5f3a\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u5b66\u4e60\u5149\u7167\u6761\u4ef6\uff0c\u5728Cityscapes\u6570\u636e\u96c6\u4e2d\u6dfb\u52a0\u865a\u62df\u884c\u4eba\u6765\u6539\u5584\u884c\u4eba\u8bc6\u522b\u6027\u80fd", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u9700\u8981\u5408\u6210\u6570\u636e\u6765\u8986\u76d6\u7279\u5b9a\u4ea4\u901a\u573a\u666f\uff0c\u4f46\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u5b58\u5728\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u884c\u4eba\u8bc6\u522b\u80fd\u529b", "method": "\u5f00\u53d1\u6570\u636e\u589e\u5f3a\u6d41\u6c34\u7ebf\uff0c\u5728Cityscapes\u6570\u636e\u96c6\u4e2d\u6dfb\u52a0\u865a\u62df\u884c\u4eba\uff1b\u63d0\u51fa\u65b0\u9896\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u67b6\u6784\u6765\u5b66\u4e60\u6570\u636e\u96c6\u7684\u5149\u7167\u6761\u4ef6\uff0c\u63d0\u9ad8\u589e\u5f3a\u7684\u771f\u5b9e\u6027", "result": "\u5728\u8bed\u4e49\u5206\u5272\u548c\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u4e0a\u5bf9\u65b9\u6cd5\u8fdb\u884c\u4e86\u8bc4\u4f30", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u751f\u6210\u81ea\u5b9a\u4e49\u4ea4\u901a\u573a\u666f\u7684\u5408\u6210\u6570\u636e\uff0c\u6539\u5584\u884c\u4eba\u8bc6\u522b\u6027\u80fd\uff0c\u51cf\u5c11\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u7684\u57df\u5dee\u8ddd"}}
{"id": "2509.13734", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13734", "abs": "https://arxiv.org/abs/2509.13734", "authors": ["Yosuke Mikami", "Daiki Matsuoka", "Hitomi Yanaka"], "title": "Implementing a Logical Inference System for Japanese Comparatives", "comment": "In Proceedings of the 5th Workshop on Natural Logic Meets Machine\n  Learning (NALOMA)", "summary": "Natural Language Inference (NLI) involving comparatives is challenging\nbecause it requires understanding quantities and comparative relations\nexpressed by sentences. While some approaches leverage Large Language Models\n(LLMs), we focus on logic-based approaches grounded in compositional semantics,\nwhich are promising for robust handling of numerical and logical expressions.\nPrevious studies along these lines have proposed logical inference systems for\nEnglish comparatives. However, it has been pointed out that there are several\nmorphological and semantic differences between Japanese and English\ncomparatives. These differences make it difficult to apply such systems\ndirectly to Japanese comparatives. To address this gap, this study proposes\nccg-jcomp, a logical inference system for Japanese comparatives based on\ncompositional semantics. We evaluate the proposed system on a Japanese NLI\ndataset containing comparative expressions. We demonstrate the effectiveness of\nour system by comparing its accuracy with that of existing LLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86ccg-jcomp\u7cfb\u7edf\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u7ec4\u5408\u8bed\u4e49\u7684\u65e5\u8bed\u6bd4\u8f83\u53e5\u903b\u8f91\u63a8\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u89e3\u51b3\u65e5\u8bed\u6bd4\u8f83\u53e5\u5728\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u65e5\u8bed\u548c\u82f1\u8bed\u6bd4\u8f83\u53e5\u5728\u5f62\u6001\u548c\u8bed\u4e49\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4f7f\u5f97\u73b0\u6709\u7684\u82f1\u8bed\u6bd4\u8f83\u53e5\u903b\u8f91\u63a8\u7406\u7cfb\u7edf\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u65e5\u8bed\u3002\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u65e5\u8bed\u6bd4\u8f83\u53e5\u7279\u70b9\u5f00\u53d1\u65b0\u7684\u903b\u8f91\u63a8\u7406\u7cfb\u7edf\u3002", "method": "\u57fa\u4e8e\u7ec4\u5408\u8bed\u4e49\u6784\u5efa\u903b\u8f91\u63a8\u7406\u7cfb\u7edfccg-jcomp\uff0c\u4e13\u95e8\u5904\u7406\u65e5\u8bed\u6bd4\u8f83\u53e5\u7684\u6570\u503c\u548c\u903b\u8f91\u8868\u8fbe\u5f0f\uff0c\u5e76\u5728\u5305\u542b\u6bd4\u8f83\u8868\u8fbe\u7684\u65e5\u8bedNLI\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u4e0e\u4f20\u7edf\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u7684\u51c6\u786e\u6027\u6bd4\u8f83\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "conclusion": "ccg-jcomp\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5904\u7406\u65e5\u8bed\u6bd4\u8f83\u53e5\u7684\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\uff0c\u4e3a\u65e5\u8bed\u6bd4\u8f83\u53e5\u7684\u903b\u8f91\u63a8\u7406\u63d0\u4f9b\u4e86\u4e13\u95e8\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13508", "categories": ["cs.CV", "I.4.3; I.4.6"], "pdf": "https://arxiv.org/pdf/2509.13508", "abs": "https://arxiv.org/abs/2509.13508", "authors": ["Maksim Penkin", "Andrey Krylov"], "title": "FunKAN: Functional Kolmogorov-Arnold Network for Medical Image Enhancement and Segmentation", "comment": "9 pages, 5 figures, submitted to the Fortieth AAAI Conference on\n  Artificial Intelligence (AAAI-26)", "summary": "Medical image enhancement and segmentation are critical yet challenging tasks\nin modern clinical practice, constrained by artifacts and complex anatomical\nvariations. Traditional deep learning approaches often rely on complex\narchitectures with limited interpretability. While Kolmogorov-Arnold networks\noffer interpretable solutions, their reliance on flattened feature\nrepresentations fundamentally disrupts the intrinsic spatial structure of\nimaging data. To address this issue we propose a Functional Kolmogorov-Arnold\nNetwork (FunKAN) -- a novel interpretable neural framework, designed\nspecifically for image processing, that formally generalizes the\nKolmogorov-Arnold representation theorem onto functional spaces and learns\ninner functions using Fourier decomposition over the basis Hermite functions.\nWe explore FunKAN on several medical image processing tasks, including Gibbs\nringing suppression in magnetic resonance images, benchmarking on IXI dataset.\nWe also propose U-FunKAN as state-of-the-art binary medical segmentation model\nwith benchmarks on three medical datasets: BUSI (ultrasound images), GlaS\n(histological structures) and CVC-ClinicDB (colonoscopy videos), detecting\nbreast cancer, glands and polyps, respectively. Experiments on those diverse\ndatasets demonstrate that our approach outperforms other KAN-based backbones in\nboth medical image enhancement (PSNR, TV) and segmentation (IoU, F1). Our work\nbridges the gap between theoretical function approximation and medical image\nanalysis, offering a robust, interpretable solution for clinical applications.", "AI": {"tldr": "\u63d0\u51faFunKAN\u7f51\u7edc\uff0c\u5c06Kolmogorov-Arnold\u5b9a\u7406\u63a8\u5e7f\u5230\u51fd\u6570\u7a7a\u95f4\uff0c\u4f7f\u7528\u5085\u91cc\u53f6\u5206\u89e3\u548cHermite\u57fa\u51fd\u6570\uff0c\u5728\u533b\u5b66\u56fe\u50cf\u589e\u5f3a\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u67b6\u6784\u590d\u6742\u4e14\u53ef\u89e3\u91ca\u6027\u6709\u9650\uff0c\u800c\u73b0\u6709KAN\u7f51\u7edc\u4f1a\u7834\u574f\u56fe\u50cf\u7684\u7a7a\u95f4\u7ed3\u6784\u7279\u5f81\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u53c8\u80fd\u5904\u7406\u56fe\u50cf\u7a7a\u95f4\u7ed3\u6784\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51faFunctional Kolmogorov-Arnold Network (FunKAN)\uff0c\u5728\u51fd\u6570\u7a7a\u95f4\u4e0a\u63a8\u5e7fKolmogorov-Arnold\u8868\u793a\u5b9a\u7406\uff0c\u4f7f\u7528\u5085\u91cc\u53f6\u5206\u89e3\u548cHermite\u57fa\u51fd\u6570\u5b66\u4e60\u5185\u90e8\u51fd\u6570", "result": "\u5728IXI\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0MRI\u5409\u5e03\u65af\u4f2a\u5f71\u6291\u5236\uff0c\u5728BUSI\u3001GlaS\u3001CVC-ClinicDB\u4e09\u4e2a\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u5206\u5272\u4efb\u52a1\uff0c\u5728PSNR\u3001TV\u3001IoU\u3001F1\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u5176\u4ed6KAN\u57fa\u7840\u6a21\u578b", "conclusion": "FunKAN\u67b6\u8d77\u4e86\u7406\u8bba\u51fd\u6570\u903c\u8fd1\u4e0e\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u4e3a\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.13775", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13775", "abs": "https://arxiv.org/abs/2509.13775", "authors": ["Vani Kanjirangat", "Ljiljana Dolamic", "Fabio Rinaldi"], "title": "Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications", "comment": "4 main pages, 4 additional, 5 figures", "summary": "This paper discusses our exploration of different data-efficient and\nparameter-efficient approaches to Arabic Dialect Identification (ADI). In\nparticular, we investigate various soft-prompting strategies, including\nprefix-tuning, prompt-tuning, P-tuning, and P-tuning V2, as well as LoRA\nreparameterizations. For the data-efficient strategy, we analyze hard prompting\nwith zero-shot and few-shot inferences to analyze the dialect identification\ncapabilities of Large Language Models (LLMs). For the parameter-efficient PEFT\napproaches, we conducted our experiments using Arabic-specific encoder models\non several major datasets. We also analyzed the n-shot inferences on\nopen-source decoder-only models, a general multilingual model (Phi-3.5), and an\nArabic-specific one(SILMA). We observed that the LLMs generally struggle to\ndifferentiate the dialectal nuances in the few-shot or zero-shot setups. The\nsoft-prompted encoder variants perform better, while the LoRA-based fine-tuned\nmodels perform best, even surpassing full fine-tuning.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u8bc6\u522b(ADI)\u7684\u6570\u636e\u9ad8\u6548\u548c\u53c2\u6570\u9ad8\u6548\u65b9\u6cd5\uff0c\u5305\u62ec\u5404\u79cd\u8f6f\u63d0\u793a\u7b56\u7565\u548cLoRA\u91cd\u53c2\u6570\u5316\uff0c\u53d1\u73b0LLM\u5728\u5c11\u6837\u672c/\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u96be\u4ee5\u533a\u5206\u65b9\u8a00\u7ec6\u5fae\u5dee\u522b\uff0c\u800cLoRA\u5fae\u8c03\u6a21\u578b\u8868\u73b0\u6700\u4f73", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u6570\u636e\u9ad8\u6548\u548c\u53c2\u6570\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u8bc6\u522b\u95ee\u9898\uff0c\u63a2\u7d22\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u548c\u5fae\u8c03\u65b9\u6cd5\u7684\u6548\u679c", "method": "\u4f7f\u7528\u8f6f\u63d0\u793a\u7b56\u7565(prefix-tuning\u3001prompt-tuning\u3001P-tuning\u3001P-tuning V2)\u548cLoRA\u91cd\u53c2\u6570\u5316\uff0c\u5728\u963f\u62c9\u4f2f\u8bed\u4e13\u7528\u7f16\u7801\u5668\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u540c\u65f6\u5206\u6790\u5f00\u6e90\u89e3\u7801\u5668\u6a21\u578b\u3001\u591a\u8bed\u8a00\u6a21\u578b\u548c\u963f\u62c9\u4f2f\u8bed\u4e13\u7528\u6a21\u578b\u7684n-shot\u63a8\u7406", "result": "LLM\u5728\u5c11\u6837\u672c\u6216\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u96be\u4ee5\u533a\u5206\u65b9\u8a00\u7ec6\u5fae\u5dee\u522b\uff0c\u8f6f\u63d0\u793a\u7f16\u7801\u5668\u53d8\u4f53\u8868\u73b0\u66f4\u597d\uff0c\u800c\u57fa\u4e8eLoRA\u7684\u5fae\u8c03\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86\u5b8c\u5168\u5fae\u8c03", "conclusion": "LoRA\u5fae\u8c03\u662f\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u8bc6\u522b\u7684\u6700\u6709\u6548\u65b9\u6cd5\uff0c\u800cLLM\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u7684\u65b9\u8a00\u8bc6\u522b\u80fd\u529b\u6709\u9650"}}
{"id": "2509.13515", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13515", "abs": "https://arxiv.org/abs/2509.13515", "authors": ["Jiangbei Yue", "Shuonan Yang", "Tailin Chen", "Jianbo Jiao", "Zeyu Fu"], "title": "Multimodal Hate Detection Using Dual-Stream Graph Neural Networks", "comment": null, "summary": "Hateful videos present serious risks to online safety and real-world\nwell-being, necessitating effective detection methods. Although multimodal\nclassification approaches integrating information from several modalities\noutperform unimodal ones, they typically neglect that even minimal hateful\ncontent defines a video's category. Specifically, they generally treat all\ncontent uniformly, instead of emphasizing the hateful components. Additionally,\nexisting multimodal methods cannot systematically capture structured\ninformation in videos, limiting the effectiveness of multimodal fusion. To\naddress these limitations, we propose a novel multimodal dual-stream graph\nneural network model. It constructs an instance graph by separating the given\nvideo into several instances to extract instance-level features. Then, a\ncomplementary weight graph assigns importance weights to these features,\nhighlighting hateful instances. Importance weights and instance features are\ncombined to generate video labels. Our model employs a graph-based framework to\nsystematically model structured relationships within and across modalities.\nExtensive experiments on public datasets show that our model is\nstate-of-the-art in hateful video classification and has strong explainability.\nCode is available:\nhttps://github.com/Multimodal-Intelligence-Lab-MIL/MultiHateGNN.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u53cc\u6d41\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u901a\u8fc7\u6784\u5efa\u5b9e\u4f8b\u56fe\u548c\u4e92\u8865\u6743\u91cd\u56fe\u6765\u7a81\u51fa\u4ec7\u6068\u5185\u5bb9\uff0c\u5728\u4ec7\u6068\u89c6\u9891\u5206\u7c7b\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u65b9\u6cd5\u901a\u5e38\u5ffd\u89c6\u4ec7\u6068\u5185\u5bb9\u7684\u51b3\u5b9a\u6027\u4f5c\u7528\uff0c\u5bf9\u6240\u6709\u5185\u5bb9\u4e00\u89c6\u540c\u4ec1\u800c\u975e\u5f3a\u8c03\u4ec7\u6068\u6210\u5206\uff0c\u4e14\u65e0\u6cd5\u7cfb\u7edf\u6355\u6349\u89c6\u9891\u4e2d\u7684\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u591a\u6a21\u6001\u878d\u5408\u6548\u679c", "method": "\u6784\u5efa\u5b9e\u4f8b\u56fe\u5c06\u89c6\u9891\u5206\u5272\u4e3a\u591a\u4e2a\u5b9e\u4f8b\u63d0\u53d6\u7279\u5f81\uff0c\u7136\u540e\u901a\u8fc7\u4e92\u8865\u6743\u91cd\u56fe\u4e3a\u8fd9\u4e9b\u7279\u5f81\u5206\u914d\u91cd\u8981\u6027\u6743\u91cd\u4ee5\u7a81\u51fa\u4ec7\u6068\u5b9e\u4f8b\uff0c\u6700\u540e\u7ed3\u5408\u6743\u91cd\u548c\u7279\u5f81\u751f\u6210\u89c6\u9891\u6807\u7b7e", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u4ec7\u6068\u89c6\u9891\u5206\u7c7b\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u5177\u6709\u5f88\u5f3a\u7684\u53ef\u89e3\u91ca\u6027", "conclusion": "\u63d0\u51fa\u7684\u591a\u6a21\u6001\u53cc\u6d41\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u5f3a\u8c03\u4ec7\u6068\u5185\u5bb9\u548c\u7cfb\u7edf\u5efa\u6a21\u7ed3\u6784\u5316\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ec7\u6068\u89c6\u9891\u68c0\u6d4b\u6027\u80fd"}}
{"id": "2509.13790", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13790", "abs": "https://arxiv.org/abs/2509.13790", "authors": ["Yangning Li", "Tingwei Lu", "Yinghui Li", "Yankai Chen", "Wei-Chieh Huang", "Wenhao Jiang", "Hui Wang", "Hai-Tao Zheng", "Philip S. Yu"], "title": "Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning", "comment": "EMNLP 2025 Findings", "summary": "Efficient instruction tuning aims to enhance the ultimate performance of\nlarge language models (LLMs) trained on a given instruction dataset. Curriculum\nlearning as a typical data organization strategy has shown preliminary\neffectiveness in instruction tuning. However, current curriculum tuning methods\nsuffer from the curriculum rigidity, since they rely solely on static heuristic\ndifficulty metrics. These methods fail to adapt to the evolving capabilities of\nmodels during training, resulting in a fixed and potentially sub-optimal\nlearning trajectory. To address the issue, Competence-Aware Multi-Perspective\ncUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS\noffers several advantages: (1) Dynamic selection for sub-curriculum. (2)\nCompetency-aware adjustment to the curriculum schedule. (3) Multiple\ndifficulty-based scheduling. Extensive experiments prove the superior\nperformance of CAMPUS, compared to other state-of-the-art baselines for\nefficient instruction tuning.", "AI": {"tldr": "CAMPUS\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u8bfe\u7a0b\u9009\u62e9\u548c\u80fd\u529b\u611f\u77e5\u8c03\u6574\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8bfe\u7a0b\u5b66\u4e60\u4e2d\u9759\u6001\u96be\u5ea6\u5ea6\u91cf\u7684\u521a\u6027\u9650\u5236\uff0c\u5728\u6307\u4ee4\u8c03\u4f18\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u8bfe\u7a0b\u8c03\u4f18\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u542f\u53d1\u5f0f\u96be\u5ea6\u5ea6\u91cf\uff0c\u65e0\u6cd5\u9002\u5e94\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u80fd\u529b\u6f14\u53d8\uff0c\u5bfc\u81f4\u56fa\u5b9a\u7684\u3001\u53ef\u80fd\u6b21\u4f18\u7684\u5b66\u4e60\u8f68\u8ff9\u3002", "method": "\u63d0\u51faCAMPUS\u6846\u67b6\uff0c\u5305\u542b\u52a8\u6001\u5b50\u8bfe\u7a0b\u9009\u62e9\u3001\u80fd\u529b\u611f\u77e5\u7684\u8bfe\u7a0b\u8fdb\u5ea6\u8c03\u6574\u548c\u57fa\u4e8e\u591a\u79cd\u96be\u5ea6\u7684\u8c03\u5ea6\u7b56\u7565\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eCAMPUS\u5728\u9ad8\u6548\u6307\u4ee4\u8c03\u4f18\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CAMPUS\u901a\u8fc7\u52a8\u6001\u548c\u9002\u5e94\u6027\u5f3a\u7684\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6307\u4ee4\u8c03\u4f18\u4e2d\u7684\u6700\u7ec8\u6027\u80fd\u3002"}}
{"id": "2509.13525", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13525", "abs": "https://arxiv.org/abs/2509.13525", "authors": ["Romain Hardy", "Tyler Berzin", "Pranav Rajpurkar"], "title": "ColonCrafter: A Depth Estimation Model for Colonoscopy Videos Using Diffusion Priors", "comment": "12 pages, 8 figures", "summary": "Three-dimensional (3D) scene understanding in colonoscopy presents\nsignificant challenges that necessitate automated methods for accurate depth\nestimation. However, existing depth estimation models for endoscopy struggle\nwith temporal consistency across video sequences, limiting their applicability\nfor 3D reconstruction. We present ColonCrafter, a diffusion-based depth\nestimation model that generates temporally consistent depth maps from monocular\ncolonoscopy videos. Our approach learns robust geometric priors from synthetic\ncolonoscopy sequences to generate temporally consistent depth maps. We also\nintroduce a style transfer technique that preserves geometric structure while\nadapting real clinical videos to match our synthetic training domain.\nColonCrafter achieves state-of-the-art zero-shot performance on the C3VD\ndataset, outperforming both general-purpose and endoscopy-specific approaches.\nAlthough full trajectory 3D reconstruction remains a challenge, we demonstrate\nclinically relevant applications of ColonCrafter, including 3D point cloud\ngeneration and surface coverage assessment.", "AI": {"tldr": "ColonCrafter\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u7ed3\u80a0\u955c\u89c6\u9891\uff0c\u80fd\u591f\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u7684\u6df1\u5ea6\u56fe\uff0c\u5728C3VD\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002", "motivation": "\u7ed3\u80a0\u955c\u4e2d\u76843D\u573a\u666f\u7406\u89e3\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u73b0\u6709\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u5728\u89c6\u9891\u5e8f\u5217\u4e2d\u7f3a\u4e4f\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u9650\u5236\u4e86\u5176\u57283D\u91cd\u5efa\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u6269\u6563\u7684\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\uff0c\u4ece\u5408\u6210\u7ed3\u80a0\u955c\u5e8f\u5217\u5b66\u4e60\u51e0\u4f55\u5148\u9a8c\u6765\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u7684\u6df1\u5ea6\u56fe\uff0c\u5e76\u5f15\u5165\u98ce\u683c\u8fc1\u79fb\u6280\u672f\u5c06\u771f\u5b9e\u4e34\u5e8a\u89c6\u9891\u9002\u914d\u5230\u5408\u6210\u8bad\u7ec3\u57df\u3002", "result": "\u5728C3VD\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u4f18\u4e8e\u901a\u7528\u548c\u7ed3\u80a0\u955c\u4e13\u7528\u65b9\u6cd5\uff0c\u5c55\u793a\u4e863D\u70b9\u4e91\u751f\u6210\u548c\u8868\u9762\u8986\u76d6\u8bc4\u4f30\u7b49\u4e34\u5e8a\u5e94\u7528\u3002", "conclusion": "\u867d\u7136\u5b8c\u6574\u7684\u8f68\u8ff93D\u91cd\u5efa\u4ecd\u5177\u6311\u6218\u6027\uff0c\u4f46ColonCrafter\u5728\u4e34\u5e8a\u76f8\u5173\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u7ed3\u80a0\u955c3D\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13803", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13803", "abs": "https://arxiv.org/abs/2509.13803", "authors": ["Laura Garc\u00eda-Sardi\u00f1a", "Hermenegildo Fabregat", "Daniel Deniz", "Rabih Zbib"], "title": "Measuring Gender Bias in Job Title Matching for Grammatical Gender Languages", "comment": null, "summary": "This work sets the ground for studying how explicit grammatical gender\nassignment in job titles can affect the results of automatic job ranking\nsystems. We propose the usage of metrics for ranking comparison controlling for\ngender to evaluate gender bias in job title ranking systems, in particular RBO\n(Rank-Biased Overlap). We generate and share test sets for a job title matching\ntask in four grammatical gender languages, including occupations in masculine\nand feminine form and annotated by gender and matching relevance. We use the\nnew test sets and the proposed methodology to evaluate the gender bias of\nseveral out-of-the-box multilingual models to set as baselines, showing that\nall of them exhibit varying degrees of gender bias.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u8bc4\u4f30\u81ea\u52a8\u804c\u4f4d\u6392\u540d\u7cfb\u7edf\u4e2d\u6027\u522b\u504f\u89c1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u521b\u5efa\u5305\u542b\u9634\u9633\u6027\u804c\u4f4d\u540d\u79f0\u7684\u6d4b\u8bd5\u96c6\uff0c\u4f7f\u7528RBO\u6307\u6807\u6765\u8861\u91cf\u591a\u8bed\u8a00\u6a21\u578b\u5728\u804c\u4f4d\u5339\u914d\u4efb\u52a1\u4e2d\u7684\u6027\u522b\u504f\u89c1\u7a0b\u5ea6\u3002", "motivation": "\u7814\u7a76\u663e\u5f0f\u8bed\u6cd5\u6027\u522b\u5728\u804c\u4f4d\u540d\u79f0\u4e2d\u7684\u5206\u914d\u5982\u4f55\u5f71\u54cd\u81ea\u52a8\u804c\u4f4d\u6392\u540d\u7cfb\u7edf\u7684\u7ed3\u679c\uff0c\u65e8\u5728\u8bc4\u4f30\u548c\u91cf\u5316\u73b0\u6709\u6a21\u578b\u5728\u6027\u522b\u504f\u89c1\u65b9\u9762\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4f7f\u7528RBO\u6307\u6807\u8fdb\u884c\u6392\u540d\u6bd4\u8f83\uff0c\u521b\u5efa\u4e86\u56db\u79cd\u8bed\u6cd5\u6027\u522b\u8bed\u8a00\u7684\u6d4b\u8bd5\u96c6\uff08\u5305\u542b\u9633\u6027\u548c\u9634\u6027\u5f62\u5f0f\u7684\u804c\u4f4d\u540d\u79f0\uff09\uff0c\u5e76\u7528\u8fd9\u4e9b\u6d4b\u8bd5\u96c6\u8bc4\u4f30\u591a\u4e2a\u73b0\u6210\u7684\u591a\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u6240\u6709\u6d4b\u8bd5\u7684\u591a\u8bed\u8a00\u6a21\u578b\u90fd\u8868\u73b0\u51fa\u4e0d\u540c\u7a0b\u5ea6\u7684\u6027\u522b\u504f\u89c1\uff0c\u8bc1\u660e\u4e86\u73b0\u6709\u7cfb\u7edf\u5728\u6027\u522b\u516c\u5e73\u6027\u65b9\u9762\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8bc4\u4f30\u804c\u4f4d\u6392\u540d\u7cfb\u7edf\u7684\u6027\u522b\u504f\u89c1\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u57fa\u7840\u548c\u6d4b\u8bd5\u6570\u636e\u96c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u591a\u8bed\u8a00\u6a21\u578b\u5728\u6027\u522b\u516c\u5e73\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u516c\u5e73\u7684AI\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.13536", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13536", "abs": "https://arxiv.org/abs/2509.13536", "authors": ["Yinlong Bai", "Hongxin Zhang", "Sheng Zhong", "Junkai Niu", "Hai Li", "Yijia He", "Yi Zhou"], "title": "MemGS: Memory-Efficient Gaussian Splatting for Real-Time SLAM", "comment": null, "summary": "Recent advancements in 3D Gaussian Splatting (3DGS) have made a significant\nimpact on rendering and reconstruction techniques. Current research\npredominantly focuses on improving rendering performance and reconstruction\nquality using high-performance desktop GPUs, largely overlooking applications\nfor embedded platforms like micro air vehicles (MAVs). These devices, with\ntheir limited computational resources and memory, often face a trade-off\nbetween system performance and reconstruction quality. In this paper, we\nimprove existing methods in terms of GPU memory usage while enhancing rendering\nquality. Specifically, to address redundant 3D Gaussian primitives in SLAM, we\npropose merging them in voxel space based on geometric similarity. This reduces\nGPU memory usage without impacting system runtime performance. Furthermore,\nrendering quality is improved by initializing 3D Gaussian primitives via\nPatch-Grid (PG) point sampling, enabling more accurate modeling of the entire\nscene. Quantitative and qualitative evaluations on publicly available datasets\ndemonstrate the effectiveness of our improvements.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5d4c\u5165\u5f0f\u5e73\u53f0\u76843D\u9ad8\u65af\u6cfc\u6e85\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f53\u7d20\u7a7a\u95f4\u51e0\u4f55\u76f8\u4f3c\u6027\u5408\u5e76\u5197\u4f59\u9ad8\u65af\u57fa\u5143\u6765\u51cf\u5c11GPU\u5185\u5b58\u4f7f\u7528\uff0c\u540c\u65f6\u4f7f\u7528Patch-Grid\u70b9\u91c7\u6837\u63d0\u9ad8\u6e32\u67d3\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d3D\u9ad8\u65af\u6cfc\u6e85\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u9ad8\u6027\u80fd\u684c\u9762GPU\uff0c\u5ffd\u7565\u4e86\u5d4c\u5165\u5f0f\u5e73\u53f0\uff08\u5982\u5fae\u578b\u98de\u884c\u5668\uff09\u7684\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\u3002\u8fd9\u4e9b\u8bbe\u5907\u9700\u8981\u5728\u7cfb\u7edf\u6027\u80fd\u548c\u91cd\u5efa\u8d28\u91cf\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002", "method": "1. \u5728\u4f53\u7d20\u7a7a\u95f4\u4e2d\u57fa\u4e8e\u51e0\u4f55\u76f8\u4f3c\u6027\u5408\u5e76SLAM\u4e2d\u7684\u5197\u4f593D\u9ad8\u65af\u57fa\u5143\uff0c\u51cf\u5c11GPU\u5185\u5b58\u4f7f\u7528\uff1b2. \u4f7f\u7528Patch-Grid\u70b9\u91c7\u6837\u521d\u59cb\u53163D\u9ad8\u65af\u57fa\u5143\uff0c\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u573a\u666f\u5efa\u6a21\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11GPU\u5185\u5b58\u4f7f\u7528\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u6e32\u67d3\u8d28\u91cf\uff0c\u4e14\u4e0d\u5f71\u54cd\u7cfb\u7edf\u8fd0\u884c\u65f6\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u5d4c\u5165\u5f0f\u5e73\u53f0\u57283D\u9ad8\u65af\u6cfc\u6e85\u5e94\u7528\u4e2d\u9762\u4e34\u7684\u5185\u5b58\u9650\u5236\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u76843D\u91cd\u5efa\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13813", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13813", "abs": "https://arxiv.org/abs/2509.13813", "authors": ["Edward Phillips", "Sean Wu", "Soheila Molaei", "Danielle Belgrave", "Anshul Thakur", "David Clifton"], "title": "Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs", "comment": null, "summary": "Large language models demonstrate impressive results across diverse tasks but\nare still known to hallucinate, generating linguistically plausible but\nincorrect answers to questions. Uncertainty quantification has been proposed as\na strategy for hallucination detection, but no existing black-box approach\nprovides estimates for both global and local uncertainty. The former attributes\nuncertainty to a batch of responses, while the latter attributes uncertainty to\nindividual responses. Current local methods typically rely on white-box access\nto internal model states, whilst black-box methods only provide global\nuncertainty estimates. We introduce a geometric framework to address this,\nbased on archetypal analysis of batches of responses sampled with only\nblack-box model access. At the global level, we propose Geometric Volume, which\nmeasures the convex hull volume of archetypes derived from response embeddings.\nAt the local level, we propose Geometric Suspicion, which ranks responses by\nreliability and enables hallucination reduction through preferential response\nselection. Unlike prior dispersion methods which yield only a single global\nscore, our approach provides semantic boundary points which have utility for\nattributing reliability to individual responses. Experiments show that our\nframework performs comparably to or better than prior methods on short form\nquestion-answering datasets, and achieves superior results on medical datasets\nwhere hallucinations carry particularly critical risks. We also provide\ntheoretical justification by proving a link between convex hull volume and\nentropy.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u51e0\u4f55\u6846\u67b6\u7684\u9ed1\u76d2\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u4f9b\u5168\u5c40\u548c\u5c40\u90e8\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u7528\u4e8e\u68c0\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898", "motivation": "\u73b0\u6709\u9ed1\u76d2\u65b9\u6cd5\u53ea\u80fd\u63d0\u4f9b\u5168\u5c40\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u800c\u5c40\u90e8\u65b9\u6cd5\u9700\u8981\u767d\u76d2\u8bbf\u95ee\u6a21\u578b\u5185\u90e8\u72b6\u6001\uff0c\u9700\u8981\u4e00\u79cd\u4ec5\u901a\u8fc7\u9ed1\u76d2\u8bbf\u95ee\u5c31\u80fd\u540c\u65f6\u63d0\u4f9b\u5168\u5c40\u548c\u5c40\u90e8\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u65b9\u6cd5", "method": "\u57fa\u4e8e\u539f\u578b\u5206\u6790\u7684\u51e0\u4f55\u6846\u67b6\uff0c\u4f7f\u7528\u54cd\u5e94\u5d4c\u5165\u7684\u51f8\u5305\u4f53\u79ef\u6d4b\u91cf\u5168\u5c40\u4e0d\u786e\u5b9a\u6027\uff08Geometric Volume\uff09\uff0c\u901a\u8fc7\u53ef\u9760\u6027\u6392\u540d\u63d0\u4f9b\u5c40\u90e8\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff08Geometric Suspicion\uff09", "result": "\u5728\u77ed\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u76f8\u5f53\u6216\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u533b\u7597\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u66f4\u4f18\u7ed3\u679c\uff0c\u7279\u522b\u662f\u5728\u5e7b\u89c9\u98ce\u9669\u8f83\u9ad8\u7684\u573a\u666f", "conclusion": "\u8be5\u51e0\u4f55\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\u7684\u5168\u5c40\u548c\u5c40\u90e8\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u95ee\u9898\uff0c\u4e3a\u5e7b\u89c9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u5efa\u7acb\u4e86\u51f8\u5305\u4f53\u79ef\u4e0e\u71b5\u4e4b\u95f4\u7684\u8054\u7cfb"}}
{"id": "2509.13577", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13577", "abs": "https://arxiv.org/abs/2509.13577", "authors": ["Tongfei Guo", "Lili Su"], "title": "Dynamic Aware: Adaptive Multi-Mode Out-of-Distribution Detection for Trajectory Prediction in Autonomous Vehicles", "comment": "8 pages, 7 figures", "summary": "Trajectory prediction is central to the safe and seamless operation of\nautonomous vehicles (AVs). In deployment, however, prediction models inevitably\nface distribution shifts between training data and real-world conditions, where\nrare or underrepresented traffic scenarios induce out-of-distribution (OOD)\ncases. While most prior OOD detection research in AVs has concentrated on\ncomputer vision tasks such as object detection and segmentation,\ntrajectory-level OOD detection remains largely underexplored. A recent study\nformulated this problem as a quickest change detection (QCD) task, providing\nformal guarantees on the trade-off between detection delay and false alarms\n[1]. Building on this foundation, we propose a new framework that introduces\nadaptive mechanisms to achieve robust detection in complex driving\nenvironments. Empirical analysis across multiple real-world datasets reveals\nthat prediction errors -- even on in-distribution samples -- exhibit\nmode-dependent distributions that evolve over time with dataset-specific\ndynamics. By explicitly modeling these error modes, our method achieves\nsubstantial improvements in both detection delay and false alarm rates.\nComprehensive experiments on established trajectory prediction benchmarks show\nthat our framework significantly outperforms prior UQ- and vision-based OOD\napproaches in both accuracy and computational efficiency, offering a practical\npath toward reliable, driving-aware autonomy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u7684\u81ea\u9002\u5e94\u5feb\u901f\u53d8\u5316\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u9884\u6d4b\u8bef\u5dee\u7684\u6a21\u5f0f\u4f9d\u8d56\u6027\u5206\u5e03\uff0c\u5728\u590d\u6742\u9a7e\u9a76\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u90e8\u7f72\u8fc7\u7a0b\u4e2d\u9762\u4e34\u8bad\u7ec3\u6570\u636e\u4e0e\u73b0\u5b9e\u6761\u4ef6\u4e4b\u95f4\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u7f55\u89c1\u6216\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u4ea4\u901a\u573a\u666f\u4e2d\u4f1a\u4ea7\u751f\u5206\u5e03\u5916(OOD)\u60c5\u51b5\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u7684OOD\u68c0\u6d4b\uff0c\u800c\u8f68\u8ff9\u7ea7\u522b\u7684OOD\u68c0\u6d4b\u7814\u7a76\u76f8\u5bf9\u4e0d\u8db3\u3002", "method": "\u57fa\u4e8e\u5feb\u901f\u53d8\u5316\u68c0\u6d4b(QCD)\u4efb\u52a1\u6846\u67b6\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u673a\u5236\u6765\u5efa\u6a21\u9884\u6d4b\u8bef\u5dee\u7684\u6a21\u5f0f\u4f9d\u8d56\u6027\u5206\u5e03\uff0c\u8fd9\u4e9b\u5206\u5e03\u4f1a\u968f\u65f6\u95f4\u6f14\u53d8\u5e76\u5177\u6709\u6570\u636e\u96c6\u7279\u5b9a\u7684\u52a8\u6001\u7279\u6027\u3002\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u8fd9\u4e9b\u8bef\u5dee\u6a21\u5f0f\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u68c0\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u5ef6\u8fdf\u548c\u8bef\u62a5\u7387\u65b9\u9762\u90fd\u6709\u663e\u8457\u6539\u5584\uff0c\u5728\u5df2\u5efa\u7acb\u7684\u8f68\u8ff9\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u660e\u663e\u4f18\u4e8e\u5148\u524d\u7684UQ\u548c\u57fa\u4e8e\u89c6\u89c9\u7684OOD\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u90fd\u6709\u4f18\u52bf\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5b9e\u73b0\u53ef\u9760\u3001\u9a7e\u9a76\u611f\u77e5\u7684\u81ea\u4e3b\u6027\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u8def\u5f84\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5efa\u6a21\u8bef\u5dee\u5206\u5e03\u52a8\u6001\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f68\u8ff9\u9884\u6d4b\u4e2d\u7684\u5206\u5e03\u5916\u68c0\u6d4b\u6311\u6218\u3002"}}
{"id": "2509.13814", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13814", "abs": "https://arxiv.org/abs/2509.13814", "authors": ["Kartik Shinde", "Laurent Besacier", "Ondrej Bojar", "Thibaut Thonet", "Tirthankar Ghosal"], "title": "Findings of the Third Automatic Minuting (AutoMin) Challenge", "comment": "Automin 2025 Website: https://ufal.github.io/automin-2025/", "summary": "This paper presents the third edition of AutoMin, a shared task on automatic\nmeeting summarization into minutes. In 2025, AutoMin featured the main task of\nminuting, the creation of structured meeting minutes, as well as a new task:\nquestion answering (QA) based on meeting transcripts.\n  The minuting task covered two languages, English and Czech, and two domains:\nproject meetings and European Parliament sessions. The QA task focused solely\non project meetings and was available in two settings: monolingual QA in\nEnglish, and cross-lingual QA, where questions were asked and answered in Czech\nbased on English meetings.\n  Participation in 2025 was more limited compared to previous years, with only\none team joining the minuting task and two teams participating in QA. However,\nas organizers, we included multiple baseline systems to enable a comprehensive\nevaluation of current (2025) large language models (LLMs) on both tasks.", "AI": {"tldr": "AutoMin 2025\u4f1a\u8bae\u7eaa\u8981\u81ea\u52a8\u751f\u6210\u8bc4\u6d4b\u4efb\u52a1\uff0c\u5305\u542b\u7ed3\u6784\u5316\u4f1a\u8bae\u7eaa\u8981\u751f\u6210\u548c\u57fa\u4e8e\u4f1a\u8bae\u8f6c\u5f55\u7684\u95ee\u7b54\u4e24\u4e2a\u4efb\u52a1\uff0c\u8986\u76d6\u82f1\u8bed\u548c\u6377\u514b\u8bed\uff0c\u53c2\u4e0e\u56e2\u961f\u8f83\u5c11\u4f46\u5305\u542b\u591a\u4e2a\u57fa\u7ebf\u7cfb\u7edf\u8bc4\u4f30\u3002", "motivation": "\u63a8\u52a8\u81ea\u52a8\u4f1a\u8bae\u7eaa\u8981\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\uff0c\u7279\u522b\u662f\u5728\u591a\u8bed\u8a00\u548c\u8de8\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u5e94\u7528\uff0c\u8bc4\u4f30\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f1a\u8bae\u7eaa\u8981\u751f\u6210\u548c\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u8bbe\u7f6e\u4e24\u4e2a\u4e3b\u8981\u4efb\u52a1\uff1a1\uff09\u7ed3\u6784\u5316\u4f1a\u8bae\u7eaa\u8981\u751f\u6210\uff08minuting\uff09\uff0c\u8986\u76d6\u82f1\u8bed\u548c\u6377\u514b\u8bed\u7684\u9879\u76ee\u4f1a\u8bae\u548c\u6b27\u6d32\u8bae\u4f1a\u4f1a\u8bae\uff1b2\uff09\u95ee\u7b54\u4efb\u52a1\uff08QA\uff09\uff0c\u5305\u62ec\u82f1\u8bed\u5355\u8bed\u95ee\u7b54\u548c\u57fa\u4e8e\u82f1\u8bed\u4f1a\u8bae\u7684\u6377\u514b\u8bed\u8de8\u8bed\u8a00\u95ee\u7b54\u3002", "result": "2025\u5e74\u53c2\u4e0e\u5ea6\u8f83\u4f4e\uff0c\u53ea\u67091\u4e2a\u56e2\u961f\u53c2\u52a0\u7eaa\u8981\u751f\u6210\u4efb\u52a1\uff0c2\u4e2a\u56e2\u961f\u53c2\u52a0\u95ee\u7b54\u4efb\u52a1\u3002\u7ec4\u7ec7\u65b9\u63d0\u4f9b\u4e86\u591a\u4e2a\u57fa\u7ebf\u7cfb\u7edf\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\u3002", "conclusion": "\u5c3d\u7ba1\u53c2\u4e0e\u56e2\u961f\u6570\u91cf\u6709\u9650\uff0c\u4f46\u901a\u8fc7\u57fa\u7ebf\u7cfb\u7edf\u7684\u5f15\u5165\uff0c\u8be5\u8bc4\u6d4b\u4efb\u52a1\u4e3a\u8bc4\u4f30\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f1a\u8bae\u7eaa\u8981\u81ea\u52a8\u751f\u6210\u548c\u76f8\u5173\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u57fa\u51c6\u3002"}}
{"id": "2509.13586", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.MM", "I.2; I.4; I.7; H.3"], "pdf": "https://arxiv.org/pdf/2509.13586", "abs": "https://arxiv.org/abs/2509.13586", "authors": ["Nathalie Neptune", "Josiane Mothe"], "title": "Annotating Satellite Images of Forests with Keywords from a Specialized Corpus in the Context of Change Detection", "comment": null, "summary": "The Amazon rain forest is a vital ecosystem that plays a crucial role in\nregulating the Earth's climate and providing habitat for countless species.\nDeforestation in the Amazon is a major concern as it has a significant impact\non global carbon emissions and biodiversity. In this paper, we present a method\nfor detecting deforestation in the Amazon using image pairs from Earth\nobservation satellites. Our method leverages deep learning techniques to\ncompare the images of the same area at different dates and identify changes in\nthe forest cover. We also propose a visual semantic model that automatically\nannotates the detected changes with relevant keywords. The candidate annotation\nfor images are extracted from scientific documents related to the Amazon\nregion. We evaluate our approach on a dataset of Amazon image pairs and\ndemonstrate its effectiveness in detecting deforestation and generating\nrelevant annotations. Our method provides a useful tool for monitoring and\nstudying the impact of deforestation in the Amazon. While we focus on\nenvironment applications of our work by using images of deforestation in the\nAmazon rain forest to demonstrate the effectiveness of our proposed approach,\nit is generic enough to be applied to other domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e9a\u9a6c\u900a\u96e8\u6797\u780d\u4f10\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u536b\u661f\u56fe\u50cf\u5bf9\u6bd4\u548c\u89c6\u89c9\u8bed\u4e49\u6a21\u578b\u81ea\u52a8\u6807\u6ce8\u53d8\u5316\u533a\u57df\u3002", "motivation": "\u4e9a\u9a6c\u900a\u96e8\u6797\u5bf9\u5730\u7403\u6c14\u5019\u8c03\u8282\u548c\u751f\u7269\u591a\u6837\u6027\u81f3\u5173\u91cd\u8981\uff0c\u780d\u4f10\u6d3b\u52a8\u5bf9\u5168\u7403\u78b3\u6392\u653e\u548c\u751f\u6001\u7cfb\u7edf\u9020\u6210\u91cd\u5927\u5f71\u54cd\uff0c\u9700\u8981\u6709\u6548\u7684\u76d1\u6d4b\u5de5\u5177\u3002", "method": "\u5229\u7528\u5730\u7403\u89c2\u6d4b\u536b\u661f\u83b7\u53d6\u7684\u4e0d\u540c\u65f6\u95f4\u56fe\u50cf\u5bf9\uff0c\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u6bd4\u8f83\u540c\u4e00\u533a\u57df\u7684\u53d8\u5316\uff0c\u5e76\u7ed3\u5408\u79d1\u5b66\u6587\u732e\u63d0\u53d6\u5173\u952e\u8bcd\u6784\u5efa\u89c6\u89c9\u8bed\u4e49\u6a21\u578b\u8fdb\u884c\u81ea\u52a8\u6807\u6ce8\u3002", "result": "\u5728\u4e9a\u9a6c\u900a\u56fe\u50cf\u5bf9\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u51c6\u786e\u68c0\u6d4b\u68ee\u6797\u8986\u76d6\u53d8\u5316\u5e76\u751f\u6210\u76f8\u5173\u6ce8\u91ca\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u76d1\u6d4b\u548c\u7814\u7a76\u4e9a\u9a6c\u900a\u780d\u4f10\u5f71\u54cd\u63d0\u4f9b\u4e86\u6709\u7528\u5de5\u5177\uff0c\u867d\u7136\u805a\u7126\u73af\u5883\u5e94\u7528\uff0c\u4f46\u5177\u6709\u901a\u7528\u6027\u53ef\u6269\u5c55\u5230\u5176\u4ed6\u9886\u57df\u3002"}}
{"id": "2509.13835", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13835", "abs": "https://arxiv.org/abs/2509.13835", "authors": ["Minh Duc Bui", "Carolin Holtermann", "Valentin Hofmann", "Anne Lauscher", "Katharina von der Wense"], "title": "Large Language Models Discriminate Against Speakers of German Dialects", "comment": "Accepted to EMNLP 2025 Main", "summary": "Dialects represent a significant component of human culture and are found\nacross all regions of the world. In Germany, more than 40% of the population\nspeaks a regional dialect (Adler and Hansen, 2022). However, despite cultural\nimportance, individuals speaking dialects often face negative societal\nstereotypes. We examine whether such stereotypes are mirrored by large language\nmodels (LLMs). We draw on the sociolinguistic literature on dialect perception\nto analyze traits commonly associated with dialect speakers. Based on these\ntraits, we assess the dialect naming bias and dialect usage bias expressed by\nLLMs in two tasks: an association task and a decision task. To assess a model's\ndialect usage bias, we construct a novel evaluation corpus that pairs sentences\nfrom seven regional German dialects (e.g., Alemannic and Bavarian) with their\nstandard German counterparts. We find that: (1) in the association task, all\nevaluated LLMs exhibit significant dialect naming and dialect usage bias\nagainst German dialect speakers, reflected in negative adjective associations;\n(2) all models reproduce these dialect naming and dialect usage biases in their\ndecision making; and (3) contrary to prior work showing minimal bias with\nexplicit demographic mentions, we find that explicitly labeling linguistic\ndemographics--German dialect speakers--amplifies bias more than implicit cues\nlike dialect usage.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u5fb7\u56fd\u65b9\u8a00\u4f7f\u7528\u8005\u5b58\u5728\u663e\u8457\u7684\u547d\u540d\u504f\u89c1\u548c\u4f7f\u7528\u504f\u89c1\uff0c\u8868\u73b0\u4e3a\u8d1f\u9762\u5f62\u5bb9\u8bcd\u5173\u8054\uff0c\u5e76\u4e14\u5728\u51b3\u7b56\u4e2d\u91cd\u73b0\u8fd9\u4e9b\u504f\u89c1\u3002\u660e\u786e\u6807\u6ce8\u65b9\u8a00\u4f7f\u7528\u8005\u8eab\u4efd\u4f1a\u52a0\u5267\u504f\u89c1\u3002", "motivation": "\u7814\u7a76\u65b9\u8a00\u4f7f\u7528\u8005\u5728\u793e\u4f1a\u4e2d\u9762\u4e34\u7684\u8d1f\u9762\u523b\u677f\u5370\u8c61\u662f\u5426\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5f97\u5230\u53cd\u6620\uff0c\u5206\u6790\u6a21\u578b\u662f\u5426\u5bf9\u5fb7\u56fd\u65b9\u8a00\u4f7f\u7528\u8005\u5b58\u5728\u504f\u89c1\u3002", "method": "\u901a\u8fc7\u5173\u8054\u4efb\u52a1\u548c\u51b3\u7b56\u4efb\u52a1\u8bc4\u4f30LLMs\u7684\u65b9\u8a00\u547d\u540d\u504f\u89c1\u548c\u4f7f\u7528\u504f\u89c1\uff0c\u6784\u5efa\u5305\u542b7\u79cd\u5fb7\u56fd\u5730\u533a\u65b9\u8a00\u4e0e\u6807\u51c6\u5fb7\u8bed\u5bf9\u7167\u7684\u65b0\u578b\u8bc4\u4f30\u8bed\u6599\u5e93\u3002", "result": "\u6240\u6709\u8bc4\u4f30\u7684LLMs\u90fd\u8868\u73b0\u51fa\u663e\u8457\u7684\u65b9\u8a00\u547d\u540d\u548c\u4f7f\u7528\u504f\u89c1\uff0c\u6a21\u578b\u5728\u51b3\u7b56\u4e2d\u91cd\u73b0\u8fd9\u4e9b\u504f\u89c1\uff0c\u660e\u786e\u6807\u6ce8\u65b9\u8a00\u8eab\u4efd\u6bd4\u9690\u5f0f\u7ebf\u7d22\u66f4\u4f1a\u653e\u5927\u504f\u89c1\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u786e\u5b9e\u53cd\u6620\u4e86\u793e\u4f1a\u4e2d\u5bf9\u65b9\u8a00\u4f7f\u7528\u8005\u7684\u8d1f\u9762\u523b\u677f\u5370\u8c61\uff0c\u9700\u8981\u91c7\u53d6\u63aa\u65bd\u51cf\u5c11\u8fd9\u79cd\u8bed\u8a00\u504f\u89c1\u3002"}}
{"id": "2509.13590", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13590", "abs": "https://arxiv.org/abs/2509.13590", "authors": ["Samer Al-Hamadani"], "title": "Intelligent Healthcare Imaging Platform An VLM-Based Framework for Automated Medical Image Analysis and Clinical Report Generation", "comment": "32 pages, 14 figures, 6 tables", "summary": "The rapid advancement of artificial intelligence (AI) in healthcare imaging\nhas revolutionized diagnostic medicine and clinical decision-making processes.\nThis work presents an intelligent multimodal framework for medical image\nanalysis that leverages Vision-Language Models (VLMs) in healthcare\ndiagnostics. The framework integrates Google Gemini 2.5 Flash for automated\ntumor detection and clinical report generation across multiple imaging\nmodalities including CT, MRI, X-ray, and Ultrasound. The system combines visual\nfeature extraction with natural language processing to enable contextual image\ninterpretation, incorporating coordinate verification mechanisms and\nprobabilistic Gaussian modeling for anomaly distribution. Multi-layered\nvisualization techniques generate detailed medical illustrations, overlay\ncomparisons, and statistical representations to enhance clinical confidence,\nwith location measurement achieving 80 pixels average deviation. Result\nprocessing utilizes precise prompt engineering and textual analysis to extract\nstructured clinical information while maintaining interpretability.\nExperimental evaluations demonstrated high performance in anomaly detection\nacross multiple modalities. The system features a user-friendly Gradio\ninterface for clinical workflow integration and demonstrates zero-shot learning\ncapabilities to reduce dependence on large datasets. This framework represents\na significant advancement in automated diagnostic support and radiological\nworkflow efficiency, though clinical validation and multi-center evaluation are\nnecessary prior to widespread adoption.", "AI": {"tldr": "\u57fa\u4e8eGoogle Gemini 2.5 Flash\u7684\u591a\u6a21\u6001\u533b\u7597\u5f71\u50cf\u5206\u6790\u6846\u67b6\uff0c\u6574\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u5904\u7406\u6280\u672f\uff0c\u5b9e\u73b0\u80bf\u7624\u68c0\u6d4b\u548c\u4e34\u5e8a\u62a5\u544a\u751f\u6210\uff0c\u652f\u6301CT\u3001MRI\u3001X\u5c04\u7ebf\u548c\u8d85\u58f0\u7b49\u591a\u79cd\u5f71\u50cf\u6a21\u6001\u3002", "motivation": "\u533b\u7597\u5f71\u50cfAI\u5feb\u901f\u53d1\u5c55\u4f46\u7f3a\u4e4f\u6574\u5408\u591a\u6a21\u6001\u5f71\u50cf\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u667a\u80fd\u8bca\u65ad\u6846\u67b6\uff0c\u9700\u8981\u63d0\u9ad8\u8bca\u65ad\u6548\u7387\u548c\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u80fd\u529b\u3002", "method": "\u91c7\u7528Vision-Language Models\uff0c\u7ed3\u5408\u5750\u6807\u9a8c\u8bc1\u673a\u5236\u548c\u6982\u7387\u9ad8\u65af\u5efa\u6a21\u8fdb\u884c\u5f02\u5e38\u5206\u5e03\u5206\u6790\uff0c\u4f7f\u7528\u591a\u5c42\u53ef\u89c6\u5316\u6280\u672f\u548c\u7cbe\u51c6\u63d0\u793a\u5de5\u7a0b\u751f\u6210\u7ed3\u6784\u5316\u4e34\u5e8a\u4fe1\u606f\u3002", "result": "\u5728\u591a\u6a21\u6001\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f4d\u7f6e\u6d4b\u91cf\u8fbe\u523080\u50cf\u7d20\u5e73\u5747\u504f\u5dee\uff0c\u5177\u5907\u96f6\u6837\u672c\u5b66\u4e60\u80fd\u529b\uff0c\u51cf\u5c11\u5bf9\u5927\u6570\u636e\u96c6\u7684\u4f9d\u8d56\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u5316\u8bca\u65ad\u652f\u6301\u548c\u653e\u5c04\u5de5\u4f5c\u6d41\u7a0b\u6548\u7387\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u4e34\u5e8a\u9a8c\u8bc1\u548c\u591a\u4e2d\u5fc3\u8bc4\u4f30\u624d\u80fd\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2509.13869", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13869", "abs": "https://arxiv.org/abs/2509.13869", "authors": ["Yang Liu", "Chenhui Chu"], "title": "Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs", "comment": "38 pages, 31 figures", "summary": "Large language models (LLMs) can lead to undesired consequences when\nmisaligned with human values, especially in scenarios involving complex and\nsensitive social biases. Previous studies have revealed the misalignment of\nLLMs with human values using expert-designed or agent-based emulated bias\nscenarios. However, it remains unclear whether the alignment of LLMs with human\nvalues differs across different types of scenarios (e.g., scenarios containing\nnegative vs. non-negative questions). In this study, we investigate the\nalignment of LLMs with human values regarding social biases (HVSB) in different\ntypes of bias scenarios. Through extensive analysis of 12 LLMs from four model\nfamilies and four datasets, we demonstrate that LLMs with large model parameter\nscales do not necessarily have lower misalignment rate and attack success rate.\nMoreover, LLMs show a certain degree of alignment preference for specific types\nof scenarios and the LLMs from the same model family tend to have higher\njudgment consistency. In addition, we study the understanding capacity of LLMs\nwith their explanations of HVSB. We find no significant differences in the\nunderstanding of HVSB across LLMs. We also find LLMs prefer their own generated\nexplanations. Additionally, we endow smaller language models (LMs) with the\nability to explain HVSB. The generation results show that the explanations\ngenerated by the fine-tuned smaller LMs are more readable, but have a\nrelatively lower model agreeability.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u7c7b\u578b\u504f\u89c1\u573a\u666f\u4e2d\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5bf9\u9f50\u60c5\u51b5\uff0c\u53d1\u73b0\u5927\u53c2\u6570\u6a21\u578b\u4e0d\u4e00\u5b9a\u6709\u66f4\u597d\u7684\u5bf9\u9f50\u8868\u73b0\uff0c\u6a21\u578b\u5bf9\u7279\u5b9a\u573a\u666f\u7c7b\u578b\u6709\u5bf9\u9f50\u504f\u597d\uff0c\u4e14\u540c\u4e00\u6a21\u578b\u5bb6\u65cf\u7684\u5224\u65ad\u4e00\u81f4\u6027\u66f4\u9ad8\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u4e0e\u793e\u4f1a\u4ef7\u503c\u89c2\u5b58\u5728\u504f\u5dee\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u590d\u6742\u654f\u611f\u793e\u4f1a\u504f\u89c1\u7684\u573a\u666f\u4e2d\u3002\u4e4b\u524d\u7684\u7814\u7a76\u4f7f\u7528\u4e13\u5bb6\u8bbe\u8ba1\u6216\u57fa\u4e8e\u4ee3\u7406\u7684\u504f\u89c1\u573a\u666f\u63ed\u793a\u4e86\u8fd9\u79cd\u504f\u5dee\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u4e0d\u540c\u7c7b\u578b\u573a\u666f\u4e0b\u7684\u5bf9\u9f50\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6765\u81ea\u56db\u4e2a\u6a21\u578b\u5bb6\u65cf\u768412\u4e2aLLM\u548c\u56db\u4e2a\u6570\u636e\u96c6\uff0c\u7814\u7a76\u4e0d\u540c\u504f\u89c1\u573a\u666f\u7c7b\u578b\u4e0b\u7684\u5bf9\u9f50\u60c5\u51b5\uff0c\u5305\u62ec\u6a21\u578b\u53c2\u6570\u89c4\u6a21\u3001\u653b\u51fb\u6210\u529f\u7387\u3001\u5224\u65ad\u4e00\u81f4\u6027\u7b49\u6307\u6807\uff0c\u5e76\u7814\u7a76\u6a21\u578b\u5bf9\u4ef7\u503c\u89c2\u7684\u7406\u89e3\u80fd\u529b\u3002", "result": "\u5927\u53c2\u6570\u6a21\u578b\u4e0d\u4e00\u5b9a\u6709\u66f4\u4f4e\u7684\u5bf9\u9f50\u504f\u5dee\u7387\u548c\u653b\u51fb\u6210\u529f\u7387\uff1b\u6a21\u578b\u5bf9\u7279\u5b9a\u7c7b\u578b\u573a\u666f\u6709\u5bf9\u9f50\u504f\u597d\uff1b\u540c\u4e00\u6a21\u578b\u5bb6\u65cf\u5224\u65ad\u4e00\u81f4\u6027\u66f4\u9ad8\uff1b\u6a21\u578b\u5bf9\u4ef7\u503c\u89c2\u7406\u89e3\u65e0\u663e\u8457\u5dee\u5f02\uff1b\u6a21\u578b\u504f\u597d\u81ea\u8eab\u751f\u6210\u7684\u89e3\u91ca\uff1b\u5c0f\u6a21\u578b\u7ecf\u8fc7\u5fae\u8c03\u540e\u751f\u6210\u66f4\u6613\u8bfb\u4f46\u6a21\u578b\u8ba4\u540c\u5ea6\u8f83\u4f4e\u7684\u89e3\u91ca\u3002", "conclusion": "LLMs\u5728\u793e\u4f1a\u504f\u89c1\u4ef7\u503c\u89c2\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u590d\u6742\u6a21\u5f0f\uff0c\u6a21\u578b\u89c4\u6a21\u4e0d\u662f\u51b3\u5b9a\u6027\u56e0\u7d20\uff0c\u573a\u666f\u7c7b\u578b\u548c\u6a21\u578b\u5bb6\u65cf\u7279\u5f81\u5f71\u54cd\u5bf9\u9f50\u8868\u73b0\uff0c\u5c0f\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u5fae\u8c03\u83b7\u5f97\u89e3\u91ca\u80fd\u529b\u4f46\u9700\u8981\u5e73\u8861\u53ef\u8bfb\u6027\u548c\u6a21\u578b\u8ba4\u540c\u5ea6\u3002"}}
{"id": "2509.13605", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13605", "abs": "https://arxiv.org/abs/2509.13605", "authors": ["Ruochen Hou", "Gabriel I. Fernandez", "Alex Xu", "Dennis W. Hong"], "title": "A Generalization of CLAP from 3D Localization to Image Processing, A Connection With RANSAC & Hough Transforms", "comment": null, "summary": "In previous work, we introduced a 2D localization algorithm called CLAP,\nClustering to Localize Across $n$ Possibilities, which was used during our\nchampionship win in RoboCup 2024, an international autonomous humanoid soccer\ncompetition. CLAP is particularly recognized for its robustness against\noutliers, where clustering is employed to suppress noise and mitigate against\nerroneous feature matches. This clustering-based strategy provides an\nalternative to traditional outlier rejection schemes such as RANSAC, in which\ncandidates are validated by reprojection error across all data points. In this\npaper, CLAP is extended to a more general framework beyond 2D localization,\nspecifically to 3D localization and image stitching. We also show how CLAP,\nRANSAC, and Hough transforms are related. The generalization of CLAP is widely\napplicable to many different fields and can be a useful tool to deal with noise\nand uncertainty.", "AI": {"tldr": "CLAP\u7b97\u6cd5\u4ece2D\u5b9a\u4f4d\u6269\u5c55\u52303D\u5b9a\u4f4d\u548c\u56fe\u50cf\u62fc\u63a5\u7684\u901a\u7528\u6846\u67b6\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u7684\u9c81\u68d2\u5f02\u5e38\u503c\u5904\u7406\u65b9\u6cd5\uff0c\u4f5c\u4e3aRANSAC\u7b49\u4f20\u7edf\u65b9\u6cd5\u7684\u66ff\u4ee3\u65b9\u6848", "motivation": "\u5c06CLAP\u8fd9\u79cd\u5728RoboCup 2024\u4e2d\u8bc1\u660e\u6709\u6548\u76842D\u5b9a\u4f4d\u7b97\u6cd5\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u76843D\u5b9a\u4f4d\u548c\u56fe\u50cf\u62fc\u63a5\u5e94\u7528\u9886\u57df\uff0c\u63d0\u4f9b\u66f4\u901a\u7528\u7684\u566a\u58f0\u548c\u4e0d\u786e\u5b9a\u6027\u5904\u7406\u5de5\u5177", "method": "\u91c7\u7528\u57fa\u4e8e\u805a\u7c7b\u7684\u7b56\u7565\u6765\u6291\u5236\u566a\u58f0\u548c\u51cf\u5c11\u9519\u8bef\u7279\u5f81\u5339\u914d\uff0c\u901a\u8fc7\u805a\u7c7b\u5019\u9009\u89e3\u800c\u4e0d\u662f\u4f20\u7edf\u7684\u6570\u636e\u70b9\u91cd\u6295\u5f71\u8bef\u5dee\u9a8c\u8bc1", "result": "\u6210\u529f\u5c06CLAP\u6846\u67b6\u6269\u5c55\u52303D\u5b9a\u4f4d\u548c\u56fe\u50cf\u62fc\u63a5\u4efb\u52a1\uff0c\u5e76\u5efa\u7acb\u4e86CLAP\u4e0eRANSAC\u3001\u970d\u592b\u53d8\u6362\u4e4b\u95f4\u7684\u7406\u8bba\u8054\u7cfb", "conclusion": "CLAP\u7684\u901a\u7528\u5316\u6846\u67b6\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u5404\u79cd\u9886\u57df\u7684\u566a\u58f0\u548c\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u4eba\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.13879", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.13879", "abs": "https://arxiv.org/abs/2509.13879", "authors": ["Mariano Barone", "Antonio Romano", "Giuseppe Riccio", "Marco Postiglione", "Vincenzo Moscato"], "title": "Combining Evidence and Reasoning for Biomedical Fact-Checking", "comment": "Proceedings of the 48th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval, 2025", "summary": "Misinformation in healthcare, from vaccine hesitancy to unproven treatments,\nposes risks to public health and trust in medical systems. While machine\nlearning and natural language processing have advanced automated fact-checking,\nvalidating biomedical claims remains uniquely challenging due to complex\nterminology, the need for domain expertise, and the critical importance of\ngrounding in scientific evidence. We introduce CER (Combining Evidence and\nReasoning), a novel framework for biomedical fact-checking that integrates\nscientific evidence retrieval, reasoning via large language models, and\nsupervised veracity prediction. By integrating the text-generation capabilities\nof large language models with advanced retrieval techniques for high-quality\nbiomedical scientific evidence, CER effectively mitigates the risk of\nhallucinations, ensuring that generated outputs are grounded in verifiable,\nevidence-based sources. Evaluations on expert-annotated datasets (HealthFC,\nBioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising\ncross-dataset generalization. Code and data are released for transparency and\nreproducibility: https: //github.com/PRAISELab-PicusLab/CER.", "AI": {"tldr": "CER\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u751f\u7269\u533b\u5b66\u4e8b\u5b9e\u6838\u67e5\u6846\u67b6\uff0c\u7ed3\u5408\u79d1\u5b66\u8bc1\u636e\u68c0\u7d22\u3001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u76d1\u7763\u771f\u5b9e\u6027\u9884\u6d4b\uff0c\u5728\u591a\u4e2a\u4e13\u4e1a\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u533b\u7597\u9886\u57df\u7684\u9519\u8bef\u4fe1\u606f\uff08\u4ece\u75ab\u82d7\u72b9\u8c6b\u5230\u672a\u7ecf\u8bc1\u5b9e\u7684\u6cbb\u7597\u65b9\u6cd5\uff09\u5bf9\u516c\u5171\u536b\u751f\u548c\u533b\u7597\u7cfb\u7edf\u4fe1\u4efb\u6784\u6210\u98ce\u9669\u3002\u751f\u7269\u533b\u5b66\u58f0\u660e\u9a8c\u8bc1\u5177\u6709\u72ec\u7279\u6311\u6218\u6027\uff0c\u5305\u62ec\u590d\u6742\u672f\u8bed\u3001\u9700\u8981\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u4ee5\u53ca\u5fc5\u987b\u57fa\u4e8e\u79d1\u5b66\u8bc1\u636e\u3002", "method": "CER\u6846\u67b6\u6574\u5408\u79d1\u5b66\u8bc1\u636e\u68c0\u7d22\u3001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u76d1\u7763\u771f\u5b9e\u6027\u9884\u6d4b\u3002\u901a\u8fc7\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u751f\u6210\u80fd\u529b\u4e0e\u9ad8\u8d28\u91cf\u751f\u7269\u533b\u5b66\u79d1\u5b66\u8bc1\u636e\u7684\u5148\u8fdb\u68c0\u7d22\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\u98ce\u9669\u3002", "result": "\u5728\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u96c6\uff08HealthFC\u3001BioASQ-7b\u3001SciFact\uff09\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u6709\u524d\u666f\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CER\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u8bc1\u636e\u68c0\u7d22\u548c\u63a8\u7406\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u4e8b\u5b9e\u6838\u67e5\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u4ee5\u786e\u4fdd\u900f\u660e\u5ea6\u548c\u53ef\u91cd\u73b0\u6027\u3002"}}
{"id": "2509.13629", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13629", "abs": "https://arxiv.org/abs/2509.13629", "authors": ["Yue He", "Min Liu", "Qinghao Liu", "Jiazheng Wang", "Yaonan Wang", "Hang Zhang", "Xiang Chen"], "title": "SAMIR, an efficient registration framework via robust feature learning from SAM", "comment": null, "summary": "Image registration is a fundamental task in medical image analysis.\nDeformations are often closely related to the morphological characteristics of\ntissues, making accurate feature extraction crucial. Recent weakly supervised\nmethods improve registration by incorporating anatomical priors such as\nsegmentation masks or landmarks, either as inputs or in the loss function.\nHowever, such weak labels are often not readily available, limiting their\npractical use. Motivated by the strong representation learning ability of\nvisual foundation models, this paper introduces SAMIR, an efficient medical\nimage registration framework that utilizes the Segment Anything Model (SAM) to\nenhance feature extraction. SAM is pretrained on large-scale natural image\ndatasets and can learn robust, general-purpose visual representations. Rather\nthan using raw input images, we design a task-specific adaptation pipeline\nusing SAM's image encoder to extract structure-aware feature embeddings,\nenabling more accurate modeling of anatomical consistency and deformation\npatterns. We further design a lightweight 3D head to refine features within the\nembedding space, adapting to local deformations in medical images.\nAdditionally, we introduce a Hierarchical Feature Consistency Loss to guide\ncoarse-to-fine feature matching and improve anatomical alignment. Extensive\nexperiments demonstrate that SAMIR significantly outperforms state-of-the-art\nmethods on benchmark datasets for both intra-subject cardiac image registration\nand inter-subject abdomen CT image registration, achieving performance\nimprovements of 2.68% on ACDC and 6.44% on the abdomen dataset. The source code\nwill be publicly available on GitHub following the acceptance of this paper.", "AI": {"tldr": "SAMIR\u662f\u4e00\u4e2a\u5229\u7528Segment Anything Model\uff08SAM\uff09\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\u7684\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u6846\u67b6\uff0c\u901a\u8fc7SAM\u7684\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u63d0\u53d6\u7ed3\u6784\u611f\u77e5\u7279\u5f81\u5d4c\u5165\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea73D\u5934\u90e8\u548c\u5206\u5c42\u7279\u5f81\u4e00\u81f4\u6027\u635f\u5931\uff0c\u5728\u5fc3\u810f\u548c\u8179\u90e8CT\u56fe\u50cf\u914d\u51c6\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u4e2d\u53d8\u5f62\u4e0e\u7ec4\u7ec7\u5f62\u6001\u7279\u5f81\u5bc6\u5207\u76f8\u5173\uff0c\u9700\u8981\u51c6\u786e\u7684\u7279\u5f81\u63d0\u53d6\u3002\u73b0\u6709\u7684\u5f31\u76d1\u7763\u65b9\u6cd5\u4f9d\u8d56\u5206\u5272\u63a9\u7801\u6216\u5730\u6807\u7b49\u89e3\u5256\u5148\u9a8c\uff0c\u4f46\u8fd9\u4e9b\u6807\u6ce8\u5f80\u5f80\u96be\u4ee5\u83b7\u5f97\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u53d7\u5230\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5f3a\u5927\u8868\u793a\u5b66\u4e60\u80fd\u529b\u7684\u542f\u53d1\uff0c\u5e0c\u671b\u5229\u7528\u9884\u8bad\u7ec3\u7684SAM\u6a21\u578b\u6765\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\u3002", "method": "\u8bbe\u8ba1\u4e86\u4efb\u52a1\u7279\u5b9a\u7684\u9002\u914d\u7ba1\u9053\uff0c\u4f7f\u7528SAM\u7684\u56fe\u50cf\u7f16\u7801\u5668\u63d0\u53d6\u7ed3\u6784\u611f\u77e5\u7279\u5f81\u5d4c\u5165\uff1b\u8bbe\u8ba1\u4e86\u8f7b\u91cf\u7ea73D\u5934\u90e8\u6765\u7ec6\u5316\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u7279\u5f81\uff0c\u9002\u5e94\u533b\u5b66\u56fe\u50cf\u7684\u5c40\u90e8\u53d8\u5f62\uff1b\u5f15\u5165\u4e86\u5206\u5c42\u7279\u5f81\u4e00\u81f4\u6027\u635f\u5931\u6765\u6307\u5bfc\u4ece\u7c97\u5230\u7ec6\u7684\u7279\u5f81\u5339\u914d\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSAMIR\u5728\u5fc3\u810f\u56fe\u50cf\u914d\u51c6\u548c\u8179\u90e8CT\u56fe\u50cf\u914d\u51c6\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728ACDC\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u63d0\u53472.68%\uff0c\u5728\u8179\u90e8\u6570\u636e\u96c6\u4e0a\u63d0\u53476.44%\u3002", "conclusion": "SAMIR\u6846\u67b6\u6210\u529f\u5229\u7528\u4e86\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u6765\u589e\u5f3a\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u7684\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u7684\u5f31\u76d1\u7763\u6807\u6ce8\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5177\u6709\u5f88\u597d\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.13888", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.13888", "abs": "https://arxiv.org/abs/2509.13888", "authors": ["Mariano Barone", "Antonio Romano", "Giuseppe Riccio", "Marco Postiglione", "Vincenzo Moscato"], "title": "Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification", "comment": null, "summary": "Misinformation in healthcare, from vaccine hesitancy to unproven treatments,\nposes risks to public health and trust in medical systems. While machine\nlearning and natural language processing have advanced automated fact-checking,\nvalidating biomedical claims remains uniquely challenging due to complex\nterminology, the need for domain expertise, and the critical importance of\ngrounding in scientific evidence. We introduce CER (Combining Evidence and\nReasoning), a novel framework for biomedical fact-checking that integrates\nscientific evidence retrieval, reasoning via large language models, and\nsupervised veracity prediction. By integrating the text-generation capabilities\nof large language models with advanced retrieval techniques for high-quality\nbiomedical scientific evidence, CER effectively mitigates the risk of\nhallucinations, ensuring that generated outputs are grounded in verifiable,\nevidence-based sources. Evaluations on expert-annotated datasets (HealthFC,\nBioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising\ncross-dataset generalization. Code and data are released for transparency and\nreproducibility: https://github.com/PRAISELab-PicusLab/CER", "AI": {"tldr": "CER\u6846\u67b6\u7ed3\u5408\u79d1\u5b66\u8bc1\u636e\u68c0\u7d22\u3001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u76d1\u7763\u771f\u5b9e\u6027\u9884\u6d4b\uff0c\u7528\u4e8e\u751f\u7269\u533b\u5b66\u4e8b\u5b9e\u6838\u67e5\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd", "motivation": "\u533b\u7597\u9886\u57df\u4e2d\u7684\u9519\u8bef\u4fe1\u606f\uff08\u5982\u75ab\u82d7\u72b9\u8c6b\u548c\u672a\u7ecf\u8bc1\u5b9e\u7684\u6cbb\u7597\u65b9\u6cd5\uff09\u5bf9\u516c\u5171\u536b\u751f\u548c\u533b\u7597\u7cfb\u7edf\u4fe1\u4efb\u6784\u6210\u98ce\u9669\uff0c\u751f\u7269\u533b\u5b66\u4e8b\u5b9e\u6838\u67e5\u56e0\u4e13\u4e1a\u672f\u8bed\u590d\u6742\u3001\u9700\u8981\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u4e14\u5fc5\u987b\u57fa\u4e8e\u79d1\u5b66\u8bc1\u636e\u800c\u5177\u6709\u72ec\u7279\u6311\u6218", "method": "\u63d0\u51faCER\u6846\u67b6\uff0c\u6574\u5408\u79d1\u5b66\u8bc1\u636e\u68c0\u7d22\u3001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u76d1\u7763\u771f\u5b9e\u6027\u9884\u6d4b\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u751f\u6210\u80fd\u529b\u548c\u9ad8\u8d28\u91cf\u751f\u7269\u533b\u5b66\u79d1\u5b66\u8bc1\u636e\u68c0\u7d22\u6280\u672f\u6765\u51cf\u5c11\u5e7b\u89c9\u98ce\u9669", "result": "\u5728\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u96c6\uff08HealthFC\u3001BioASQ-7b\u3001SciFact\uff09\u4e0a\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u548c\u826f\u597d\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b", "conclusion": "CER\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u751f\u7269\u533b\u5b66\u4e8b\u5b9e\u6838\u67e5\u7684\u6311\u6218\uff0c\u786e\u4fdd\u8f93\u51fa\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u7684\u8bc1\u636e\u6765\u6e90\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u4ee5\u786e\u4fdd\u900f\u660e\u5ea6\u548c\u53ef\u590d\u73b0\u6027"}}
{"id": "2509.13631", "categories": ["cs.CV", "cs.DC", "14J60", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.13631", "abs": "https://arxiv.org/abs/2509.13631", "authors": ["Yuvraj Dutta", "Aaditya Sikder", "Basabdatta Palit"], "title": "Federated Learning for Deforestation Detection: A Distributed Approach with Satellite Imagery", "comment": "6 pages, 7 figures, accepted at IEEE INDISCON 2025", "summary": "Accurate identification of deforestation from satellite images is essential\nin order to understand the geographical situation of an area. This paper\nintroduces a new distributed approach to identify as well as locate\ndeforestation across different clients using Federated Learning (FL). Federated\nLearning enables distributed network clients to collaboratively train a model\nwhile maintaining data privacy and security of the active users. In our\nframework, a client corresponds to an edge satellite center responsible for\nlocal data processing. Moreover, FL provides an advantage over centralized\ntraining method which requires combining data, thereby compromising with data\nsecurity of the clients. Our framework leverages the FLOWER framework with RAY\nframework to execute the distributed learning workload. Furthermore, efficient\nclient spawning is ensured by RAY as it can select definite amount of users to\ncreate an emulation environment. Our FL framework uses YOLOS-small (a Vision\nTransformer variant), Faster R-CNN with a ResNet50 backbone, and Faster R-CNN\nwith a MobileNetV3 backbone models trained and tested on publicly available\ndatasets. Our approach provides us a different view for image\nsegmentation-based tasks on satellite imagery.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u5206\u5e03\u5f0f\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u536b\u661f\u56fe\u50cf\u4e2d\u8bc6\u522b\u548c\u5b9a\u4f4d\u68ee\u6797\u780d\u4f10\uff0c\u5728\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u591a\u5ba2\u6237\u7aef\u534f\u4f5c\u8bad\u7ec3\u3002", "motivation": "\u4f20\u7edf\u7684\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u65b9\u6cd5\u9700\u8981\u5408\u5e76\u6570\u636e\uff0c\u4f1a\u635f\u5bb3\u5ba2\u6237\u7aef\u7684\u6570\u636e\u5b89\u5168\u548c\u9690\u79c1\u3002\u536b\u661f\u56fe\u50cf\u5904\u7406\u6d89\u53ca\u591a\u4e2a\u8fb9\u7f18\u4e2d\u5fc3\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u534f\u4f5c\u8bad\u7ec3\u53c8\u80fd\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528FLOWER\u6846\u67b6\u548cRAY\u6846\u67b6\u5b9e\u73b0\u8054\u90a6\u5b66\u4e60\uff0c\u91c7\u7528YOLOS-small\u3001Faster R-CNN with ResNet50\u548cFaster R-CNN with MobileNetV3\u4e09\u79cd\u6a21\u578b\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc6\u522b\u548c\u5b9a\u4f4d\u68ee\u6797\u780d\u4f10\uff0c\u540c\u65f6\u786e\u4fdd\u5404\u8fb9\u7f18\u536b\u661f\u4e2d\u5fc3\u7684\u6570\u636e\u9690\u79c1\u548c\u5b89\u5168\u3002", "conclusion": "\u8054\u90a6\u5b66\u4e60\u4e3a\u536b\u661f\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u5728\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u4e86\u5206\u5e03\u5f0f\u534f\u4f5c\u5b66\u4e60\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.13905", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13905", "abs": "https://arxiv.org/abs/2509.13905", "authors": ["Domenico Meconi", "Simone Stirpe", "Federico Martelli", "Leonardo Lavalle", "Roberto Navigli"], "title": "Do Large Language Models Understand Word Senses?", "comment": "20 pages, to be published in EMNLP2025", "summary": "Understanding the meaning of words in context is a fundamental capability for\nLarge Language Models (LLMs). Despite extensive evaluation efforts, the extent\nto which LLMs show evidence that they truly grasp word senses remains\nunderexplored. In this paper, we address this gap by evaluating both i) the\nWord Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs,\ncomparing their performance to state-of-the-art systems specifically designed\nfor the task, and ii) the ability of two top-performing open- and closed-source\nLLMs to understand word senses in three generative settings: definition\ngeneration, free-form explanation, and example generation. Notably, we find\nthat, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve\nperformance on par with specialized WSD systems, while also demonstrating\ngreater robustness across domains and levels of difficulty. In the generation\ntasks, results reveal that LLMs can explain the meaning of words in context up\nto 98\\% accuracy, with the highest performance observed in the free-form\nexplanation task, which best aligns with their generative capabilities.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u6307\u4ee4\u8c03\u4f18LLM\u5728\u8bcd\u4e49\u6d88\u6b67\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u5e76\u4e0e\u4e13\u95e8\u7cfb\u7edf\u8fdb\u884c\u6bd4\u8f83\uff0c\u540c\u65f6\u6d4b\u8bd5\u4e86LLM\u5728\u5b9a\u4e49\u751f\u6210\u3001\u81ea\u7531\u89e3\u91ca\u548c\u793a\u4f8b\u751f\u6210\u4e09\u79cd\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8bcd\u4e49\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u8bcd\u4e49\u7406\u89e3\u65b9\u9762\u5df2\u6709\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u4f46\u5176\u662f\u5426\u771f\u6b63\u638c\u63e1\u8bcd\u4e49\u4ecd\u7f3a\u4e4f\u6df1\u5165\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7cfb\u7edf\u8bc4\u4f30LLM\u7684\u8bcd\u4e49\u6d88\u6b67\u80fd\u529b\u548c\u751f\u6210\u5f0f\u8bcd\u4e49\u7406\u89e3\u80fd\u529b\u3002", "method": "\u8bc4\u4f30\u6307\u4ee4\u8c03\u4f18LLM\u7684\u8bcd\u4e49\u6d88\u6b67\u80fd\u529b\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u4e13\u95e8\u7cfb\u7edf\u8fdb\u884c\u5bf9\u6bd4\uff1b\u6d4b\u8bd5\u4e24\u4e2a\u9876\u7ea7\u5f00\u6e90\u548c\u95ed\u6e90LLM\u5728\u4e09\u79cd\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff1a\u5b9a\u4e49\u751f\u6210\u3001\u81ea\u7531\u89e3\u91ca\u548c\u793a\u4f8b\u751f\u6210\u3002", "result": "\u5728\u8bcd\u4e49\u6d88\u6b67\u4efb\u52a1\u4e2d\uff0cGPT-4o\u548cDeepSeek-V3\u7b49\u9886\u5148\u6a21\u578b\u8fbe\u5230\u4e0e\u4e13\u95e8\u7cfb\u7edf\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4e14\u5728\u4e0d\u540c\u9886\u57df\u548c\u96be\u5ea6\u7ea7\u522b\u4e0a\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002\u5728\u751f\u6210\u4efb\u52a1\u4e2d\uff0cLLM\u80fd\u4ee5\u9ad8\u8fbe98%\u7684\u51c6\u786e\u7387\u89e3\u91ca\u4e0a\u4e0b\u6587\u4e2d\u7684\u8bcd\u4e49\uff0c\u5176\u4e2d\u81ea\u7531\u89e3\u91ca\u4efb\u52a1\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "LLM\u4e0d\u4ec5\u5728\u8bcd\u4e49\u6d88\u6b67\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e13\u95e8\u7cfb\u7edf\u7684\u6c34\u5e73\uff0c\u8fd8\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u751f\u6210\u5f0f\u8bcd\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u81ea\u7531\u89e3\u91ca\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u8fd9\u4e0e\u5176\u751f\u6210\u80fd\u529b\u9ad8\u5ea6\u5339\u914d\u3002"}}
{"id": "2509.13652", "categories": ["cs.CV", "I.4.8; I.4.5"], "pdf": "https://arxiv.org/pdf/2509.13652", "abs": "https://arxiv.org/abs/2509.13652", "authors": ["Yumin Li", "Dylan Campbell"], "title": "Gaussian Alignment for Relative Camera Pose Estimation via Single-View Reconstruction", "comment": "12 pages, 4 figures, accepted by AJCAI 2025", "summary": "Estimating metric relative camera pose from a pair of images is of great\nimportance for 3D reconstruction and localisation. However, conventional\ntwo-view pose estimation methods are not metric, with camera translation known\nonly up to a scale, and struggle with wide baselines and textureless or\nreflective surfaces. This paper introduces GARPS, a training-free framework\nthat casts this problem as the direct alignment of two independently\nreconstructed 3D scenes. GARPS leverages a metric monocular depth estimator and\na Gaussian scene reconstructor to obtain a metric 3D Gaussian Mixture Model\n(GMM) for each image. It then refines an initial pose from a feed-forward\ntwo-view pose estimator by optimising a differentiable GMM alignment objective.\nThis objective jointly considers geometric structure, view-independent colour,\nanisotropic covariance, and semantic feature consistency, and is robust to\nocclusions and texture-poor regions without requiring explicit 2D\ncorrespondences. Extensive experiments on the Real\\-Estate10K dataset\ndemonstrate that GARPS outperforms both classical and state-of-the-art\nlearning-based methods, including MASt3R. These results highlight the potential\nof bridging single-view perception with multi-view geometry to achieve robust\nand metric relative pose estimation.", "AI": {"tldr": "GARPS\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u4e24\u89c6\u56fe\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u5bf9\u9f50\u4e24\u4e2a\u72ec\u7acb\u91cd\u5efa\u76843D\u9ad8\u65af\u573a\u666f\u6765\u5b9e\u73b0\u5ea6\u91cf\u76f8\u5bf9\u59ff\u6001\u4f30\u8ba1\uff0c\u5728Real-Estate10K\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u4e24\u89c6\u56fe\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u65e0\u6cd5\u63d0\u4f9b\u5ea6\u91cf\u5c3a\u5ea6\u4fe1\u606f\uff0c\u4e14\u5728\u5bbd\u57fa\u7ebf\u548c\u7eb9\u7406\u8d2b\u4e4f\u533a\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u63d0\u4f9b\u5ea6\u91cf\u5c3a\u5ea6\u4e14\u5bf9\u7eb9\u7406\u53d8\u5316\u9c81\u68d2\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5ea6\u91cf\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5668\u548c\u9ad8\u65af\u573a\u666f\u91cd\u5efa\u5668\u4e3a\u6bcf\u5f20\u56fe\u50cf\u751f\u6210\u5ea6\u91cf3D\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff0c\u7136\u540e\u901a\u8fc7\u4f18\u5316\u53ef\u5fae\u5206GMM\u5bf9\u9f50\u76ee\u6807\u6765\u7cbe\u5316\u521d\u59cb\u59ff\u6001\u4f30\u8ba1\uff0c\u8be5\u76ee\u6807\u7efc\u5408\u8003\u8651\u51e0\u4f55\u7ed3\u6784\u3001\u989c\u8272\u3001\u534f\u65b9\u5dee\u548c\u8bed\u4e49\u7279\u5f81\u4e00\u81f4\u6027\u3002", "result": "\u5728Real-Estate10K\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGARPS\u8d85\u8d8a\u4e86\u7ecf\u5178\u65b9\u6cd5\u548c\u6700\u5148\u8fdb\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u5305\u62ecMASt3R\u3002", "conclusion": "\u901a\u8fc7\u6865\u63a5\u5355\u89c6\u56fe\u611f\u77e5\u548c\u591a\u89c6\u56fe\u51e0\u4f55\uff0cGARPS\u5c55\u793a\u4e86\u5b9e\u73b0\u9c81\u68d2\u5ea6\u91cf\u76f8\u5bf9\u59ff\u6001\u4f30\u8ba1\u7684\u6f5c\u529b\uff0c\u65e0\u9700\u663e\u5f0f2D\u5bf9\u5e94\u5173\u7cfb\u5373\u53ef\u5904\u7406\u906e\u6321\u548c\u7eb9\u7406\u8d2b\u4e4f\u533a\u57df\u3002"}}
{"id": "2509.13930", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13930", "abs": "https://arxiv.org/abs/2509.13930", "authors": ["Dayeon Ki", "Marine Carpuat", "Paul McNamee", "Daniel Khashabi", "Eugene Yang", "Dawn Lawrie", "Kevin Duh"], "title": "Linguistic Nepotism: Trading-off Quality for Language Preference in Multilingual RAG", "comment": "33 pages, 20 figures", "summary": "Multilingual Retrieval-Augmented Generation (mRAG) systems enable language\nmodels to answer knowledge-intensive queries with citation-supported responses\nacross languages. While such systems have been proposed, an open questions is\nwhether the mixture of different document languages impacts generation and\ncitation in unintended ways. To investigate, we introduce a controlled\nmethodology using model internals to measure language preference while holding\nother factors such as document relevance constant. Across eight languages and\nsix open-weight models, we find that models preferentially cite English sources\nwhen queries are in English, with this bias amplified for lower-resource\nlanguages and for documents positioned mid-context. Crucially, we find that\nmodels sometimes trade-off document relevance for language preference,\nindicating that citation choices are not always driven by informativeness\nalone. Our findings shed light on how language models leverage multilingual\ncontext and influence citation behavior.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u591a\u8bed\u8a00\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u5b58\u5728\u82f1\u8bed\u504f\u597d\u504f\u89c1\uff0c\u6a21\u578b\u503e\u5411\u4e8e\u5f15\u7528\u82f1\u8bed\u6587\u6863\u800c\u975e\u5176\u4ed6\u8bed\u8a00\u7684\u76f8\u5173\u6587\u6863\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u6587\u6863\u4f4d\u4e8e\u4e0a\u4e0b\u6587\u4e2d\u95f4\u4f4d\u7f6e\u65f6\uff0c\u8fd9\u79cd\u504f\u89c1\u66f4\u52a0\u660e\u663e\u3002", "motivation": "\u7814\u7a76\u591a\u8bed\u8a00\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u4e2d\u4e0d\u540c\u6587\u6863\u8bed\u8a00\u7684\u6df7\u5408\u662f\u5426\u4f1a\u5bf9\u751f\u6210\u548c\u5f15\u7528\u4ea7\u751f\u610f\u5916\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u8bed\u8a00\u504f\u597d\u65b9\u9762\u662f\u5426\u5b58\u5728\u504f\u89c1\u3002", "method": "\u91c7\u7528\u53d7\u63a7\u65b9\u6cd5\uff0c\u5229\u7528\u6a21\u578b\u5185\u90e8\u673a\u5236\u6765\u8861\u91cf\u8bed\u8a00\u504f\u597d\uff0c\u540c\u65f6\u4fdd\u6301\u6587\u6863\u76f8\u5173\u6027\u7b49\u5176\u4ed6\u56e0\u7d20\u4e0d\u53d8\u3002\u7814\u7a76\u6db5\u76d68\u79cd\u8bed\u8a00\u548c6\u4e2a\u5f00\u6e90\u6a21\u578b\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u5728\u82f1\u8bed\u67e5\u8be2\u65f6\u4f18\u5148\u5f15\u7528\u82f1\u8bed\u6765\u6e90\uff0c\u8fd9\u79cd\u504f\u89c1\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u6587\u6863\u4f4d\u4e8e\u4e0a\u4e0b\u6587\u4e2d\u95f4\u4f4d\u7f6e\u65f6\u66f4\u52a0\u660e\u663e\u3002\u6a21\u578b\u6709\u65f6\u4f1a\u727a\u7272\u6587\u6863\u76f8\u5173\u6027\u6765\u6ee1\u8db3\u8bed\u8a00\u504f\u597d\u3002", "conclusion": "\u591a\u8bed\u8a00\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u7684\u5f15\u7528\u9009\u62e9\u5e76\u4e0d\u603b\u662f\u7531\u4fe1\u606f\u6027\u9a71\u52a8\uff0c\u8bed\u8a00\u504f\u597d\u663e\u8457\u5f71\u54cd\u6a21\u578b\u7684\u5f15\u7528\u884c\u4e3a\uff0c\u8fd9\u5bf9\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5229\u7528\u591a\u8bed\u8a00\u4e0a\u4e0b\u6587\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2509.13662", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13662", "abs": "https://arxiv.org/abs/2509.13662", "authors": ["Yulan Guo", "Longguang Wang", "Wendong Mao", "Xiaoyu Dong", "Yingqian Wang", "Li Liu", "Wei An"], "title": "Deep Lookup Network", "comment": null, "summary": "Convolutional neural networks are constructed with massive operations with\ndifferent types and are highly computationally intensive. Among these\noperations, multiplication operation is higher in computational complexity and\nusually requires {more} energy consumption with longer inference time than\nother operations, which hinders the deployment of convolutional neural networks\non mobile devices. In many resource-limited edge devices, complicated\noperations can be calculated via lookup tables to reduce computational cost.\nMotivated by this, in this paper, we introduce a generic and efficient lookup\noperation which can be used as a basic operation for the construction of neural\nnetworks. Instead of calculating the multiplication of weights and activation\nvalues, simple yet efficient lookup operations are adopted to compute their\nresponses. To enable end-to-end optimization of the lookup operation, we\nconstruct the lookup tables in a differentiable manner and propose several\ntraining strategies to promote their convergence. By replacing computationally\nexpensive multiplication operations with our lookup operations, we develop\nlookup networks for the image classification, image super-resolution, and point\ncloud classification tasks. It is demonstrated that our lookup networks can\nbenefit from the lookup operations to achieve higher efficiency in terms of\nenergy consumption and inference speed while maintaining competitive\nperformance to vanilla convolutional networks. Extensive experiments show that\nour lookup networks produce state-of-the-art performance on different tasks\n(both classification and regression tasks) and different data types (both\nimages and point clouds).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u67e5\u627e\u8868\u7684\u9ad8\u6548\u795e\u7ecf\u7f51\u7edc\u64cd\u4f5c\uff0c\u7528\u7b80\u5355\u7684\u67e5\u627e\u64cd\u4f5c\u66ff\u4ee3\u8ba1\u7b97\u5bc6\u96c6\u7684\u4e58\u6cd5\u8fd0\u7b97\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u80fd\u6548\u548c\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u4e58\u6cd5\u8fd0\u7b97\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u80fd\u8017\u5927\uff0c\u963b\u788d\u4e86\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u3002\u53d7\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4f7f\u7528\u67e5\u627e\u8868\u7b80\u5316\u590d\u6742\u8ba1\u7b97\u7684\u542f\u53d1\uff0c\u5e0c\u671b\u7528\u67e5\u627e\u64cd\u4f5c\u66ff\u4ee3\u4f20\u7edf\u4e58\u6cd5\u8fd0\u7b97\u3002", "method": "\u5f15\u5165\u901a\u7528\u7684\u53ef\u5fae\u5206\u67e5\u627e\u64cd\u4f5c\uff0c\u6784\u5efa\u53ef\u5fae\u67e5\u627e\u8868\u5e76\u901a\u8fc7\u591a\u79cd\u8bad\u7ec3\u7b56\u7565\u8fdb\u884c\u7aef\u5230\u7aef\u4f18\u5316\uff0c\u7528\u67e5\u627e\u64cd\u4f5c\u66ff\u4ee3\u6743\u91cd\u548c\u6fc0\u6d3b\u503c\u7684\u4e58\u6cd5\u8ba1\u7b97\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u8d85\u5206\u8fa8\u7387\u548c\u70b9\u4e91\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u67e5\u627e\u7f51\u7edc\u5728\u80fd\u8017\u548c\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u6548\u7387\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u539f\u59cb\u5377\u79ef\u7f51\u7edc\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u6570\u636e\u7c7b\u578b\u4e0a\u90fd\u8fbe\u5230\u4e86\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u67e5\u627e\u64cd\u4f5c\u662f\u4e00\u79cd\u6709\u6548\u7684\u795e\u7ecf\u7f51\u7edc\u57fa\u7840\u64cd\u4f5c\uff0c\u80fd\u591f\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13980", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13980", "abs": "https://arxiv.org/abs/2509.13980", "authors": ["Sami Ul Haq", "Chinonso Cynthia Osuji", "Sheila Castilho", "Brian Davis"], "title": "Long-context Reference-based MT Quality Estimation", "comment": null, "summary": "In this paper, we present our submission to the Tenth Conference on Machine\nTranslation (WMT25) Shared Task on Automated Translation Quality Evaluation.\n  Our systems are built upon the COMET framework and trained to predict\nsegment-level Error Span Annotation (ESA) scores using augmented long-context\ndata.\n  To construct long-context training data, we concatenate in-domain,\nhuman-annotated sentences and compute a weighted average of their scores.\n  We integrate multiple human judgment datasets (MQM, SQM, and DA) by\nnormalising their scales and train multilingual regression models to predict\nquality scores from the source, hypothesis, and reference translations.\n  Experimental results show that incorporating long-context information\nimproves correlations with human judgments compared to models trained only on\nshort segments.", "AI": {"tldr": "\u57fa\u4e8eCOMET\u6846\u67b6\u6784\u5efa\u7684\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u7cfb\u7edf\uff0c\u901a\u8fc7\u957f\u4e0a\u4e0b\u6587\u6570\u636e\u589e\u5f3a\u8bad\u7ec3\uff0c\u9884\u6d4b\u9519\u8bef\u8de8\u5ea6\u6807\u6ce8\u5206\u6570\uff0c\u5728WMT25\u5171\u4eab\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4ec5\u4f7f\u7528\u77ed\u7247\u6bb5\u8bad\u7ec3\u7684\u6a21\u578b", "motivation": "\u89e3\u51b3\u4f20\u7edf\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\u4ec5\u57fa\u4e8e\u77ed\u7247\u6bb5\u8bad\u7ec3\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u5229\u7528\u957f\u4e0a\u4e0b\u6587\u4fe1\u606f\u6765\u66f4\u597d\u5730\u9884\u6d4b\u7ffb\u8bd1\u8d28\u91cf\uff0c\u63d0\u9ad8\u4e0e\u4eba\u5de5\u8bc4\u4f30\u7684\u76f8\u5173\u6027", "method": "\u4f7f\u7528COMET\u6846\u67b6\uff0c\u901a\u8fc7\u62fc\u63a5\u9886\u57df\u5185\u4eba\u5de5\u6807\u6ce8\u53e5\u5b50\u6784\u5efa\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u6570\u636e\uff0c\u8ba1\u7b97\u52a0\u6743\u5e73\u5747\u5206\u6570\uff0c\u6574\u5408\u591a\u79cd\u4eba\u5de5\u8bc4\u4f30\u6570\u636e\u96c6\uff08MQM\u3001SQM\u3001DA\uff09\u5e76\u8fdb\u884c\u5c3a\u5ea6\u5f52\u4e00\u5316\uff0c\u8bad\u7ec3\u591a\u8bed\u8a00\u56de\u5f52\u6a21\u578b", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u878d\u5165\u957f\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u6a21\u578b\u76f8\u6bd4\u4ec5\u4f7f\u7528\u77ed\u7247\u6bb5\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u4e0e\u4eba\u5de5\u8bc4\u4f30\u7684\u76f8\u5173\u6027\u6709\u6240\u63d0\u9ad8", "conclusion": "\u957f\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u6574\u5408\u80fd\u591f\u6709\u6548\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e3a\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u6539\u8fdb\u65b9\u5411"}}
{"id": "2509.13676", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13676", "abs": "https://arxiv.org/abs/2509.13676", "authors": ["Xiaobo Yang", "Xiaojin Gong"], "title": "Re-purposing SAM into Efficient Visual Projectors for MLLM-Based Referring Image Segmentation", "comment": null, "summary": "Recently, Referring Image Segmentation (RIS) frameworks that pair the\nMultimodal Large Language Model (MLLM) with the Segment Anything Model (SAM)\nhave achieved impressive results. However, adapting MLLM to segmentation is\ncomputationally intensive, primarily due to visual token redundancy. We observe\nthat traditional patch-wise visual projectors struggle to strike a balance\nbetween reducing the number of visual tokens and preserving semantic clarity,\noften retaining overly long token sequences to avoid performance drops.\nInspired by text tokenizers, we propose a novel semantic visual projector that\nleverages semantic superpixels generated by SAM to identify \"visual words\" in\nan image. By compressing and projecting semantic superpixels as visual tokens,\nour approach adaptively shortens the token sequence according to scene\ncomplexity while minimizing semantic loss in compression. To mitigate loss of\ninformation, we propose a semantic superpixel positional embedding to\nstrengthen MLLM's awareness of superpixel geometry and position, alongside a\nsemantic superpixel aggregator to preserve both fine-grained details inside\nsuperpixels and global context outside. Experiments show that our method cuts\nvisual tokens by 93% without compromising performance, notably speeding up MLLM\ntraining and inference, and outperforming existing compressive visual\nprojectors on RIS.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eSAM\u8bed\u4e49\u8d85\u50cf\u7d20\u7684\u89c6\u89c9\u6295\u5f71\u5668\uff0c\u5c06\u89c6\u89c9token\u51cf\u5c1193%\u800c\u4e0d\u635f\u5931\u6027\u80fd\uff0c\u663e\u8457\u52a0\u901fMLLM\u8bad\u7ec3\u548c\u63a8\u7406", "motivation": "\u4f20\u7edfpatch-wise\u89c6\u89c9\u6295\u5f71\u5668\u5728\u51cf\u5c11\u89c6\u89c9token\u6570\u91cf\u548c\u4fdd\u6301\u8bed\u4e49\u6e05\u6670\u5ea6\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u901a\u5e38\u4fdd\u7559\u8fc7\u957f\u7684token\u5e8f\u5217\u4ee5\u907f\u514d\u6027\u80fd\u4e0b\u964d", "method": "\u5229\u7528SAM\u751f\u6210\u7684\u8bed\u4e49\u8d85\u50cf\u7d20\u8bc6\u522b\u56fe\u50cf\u4e2d\u7684\"\u89c6\u89c9\u8bcd\u6c47\"\uff0c\u901a\u8fc7\u538b\u7f29\u548c\u6295\u5f71\u8bed\u4e49\u8d85\u50cf\u7d20\u4f5c\u4e3a\u89c6\u89c9token\uff0c\u6839\u636e\u573a\u666f\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u7f29\u77edtoken\u5e8f\u5217\uff1b\u63d0\u51fa\u8bed\u4e49\u8d85\u50cf\u7d20\u4f4d\u7f6e\u5d4c\u5165\u548c\u805a\u5408\u5668\u6765\u4fdd\u6301\u51e0\u4f55\u4f4d\u7f6e\u4fe1\u606f\u548c\u7ec6\u8282", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728Referring Image Segmentation\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u538b\u7f29\u89c6\u89c9\u6295\u5f71\u5668\uff0c\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u91cf", "conclusion": "\u57fa\u4e8e\u8bed\u4e49\u8d85\u50cf\u7d20\u7684\u89c6\u89c9\u6295\u5f71\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9token\u5197\u4f59\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387"}}
{"id": "2509.13990", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.13990", "abs": "https://arxiv.org/abs/2509.13990", "authors": ["Colin Hong", "Xu Guo", "Anand Chaanan Singh", "Esha Choukse", "Dmitrii Ustiugov"], "title": "Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency", "comment": "Accepted by EMNLP 2025 (Oral), 9 pages", "summary": "Recently, Test-Time Scaling (TTS) has gained increasing attention for\nimproving LLM reasoning performance at test time without retraining the model.\nA notable TTS technique is Self-Consistency (SC), which generates multiple\nreasoning chains in parallel and selects the final answer via majority voting.\nWhile effective, the order-of-magnitude computational overhead limits its broad\ndeployment. Prior attempts to accelerate SC mainly rely on model-based\nconfidence scores or heuristics with limited empirical support. For the first\ntime, we theoretically and empirically analyze the inefficiencies of SC and\nreveal actionable opportunities for improvement. Building on these insights, we\npropose Slim-SC, a step-wise pruning strategy that identifies and removes\nredundant chains using inter-chain similarity at the thought level. Experiments\non three STEM reasoning datasets and two recent LLM architectures show that\nSlim-SC reduces inference latency and KVC usage by up to 45% and 26%,\nrespectively, with R1-Distill, while maintaining or improving accuracy, thus\noffering a simple yet efficient TTS alternative for SC.", "AI": {"tldr": "Slim-SC\u662f\u4e00\u79cd\u901a\u8fc7\u601d\u7ef4\u7ea7\u94fe\u95f4\u76f8\u4f3c\u6027\u8bc6\u522b\u548c\u79fb\u9664\u5197\u4f59\u94fe\u7684\u9010\u6b65\u526a\u679d\u7b56\u7565\uff0c\u53ef\u5728\u4fdd\u6301\u6216\u63d0\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u5c06\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e45%\uff0cKVC\u4f7f\u7528\u91cf\u51cf\u5c1126%\u3002", "motivation": "\u81ea\u4e00\u81f4\u6027(SC)\u6280\u672f\u867d\u7136\u6709\u6548\uff0c\u4f46\u5176\u6570\u91cf\u7ea7\u7684\u8ba1\u7b97\u5f00\u9500\u9650\u5236\u4e86\u5e7f\u6cdb\u90e8\u7f72\u3002\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6a21\u578b\u7f6e\u4fe1\u5ea6\u5206\u6570\u6216\u7ecf\u9a8c\u652f\u6301\u6709\u9650\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faSlim-SC\u65b9\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u7814\u7a76\u63ed\u793aSC\u7684\u4f4e\u6548\u6027\uff0c\u5e76\u57fa\u4e8e\u601d\u7ef4\u7ea7\u94fe\u95f4\u76f8\u4f3c\u6027\u8bbe\u8ba1\u9010\u6b65\u526a\u679d\u7b56\u7565\uff0c\u8bc6\u522b\u548c\u79fb\u9664\u5197\u4f59\u63a8\u7406\u94fe\u3002", "result": "\u5728\u4e09\u4e2aSTEM\u63a8\u7406\u6570\u636e\u96c6\u548c\u4e24\u79cd\u6700\u65b0LLM\u67b6\u6784\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSlim-SC\u5728\u4fdd\u6301\u6216\u63d0\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e45%\uff0cKVC\u4f7f\u7528\u91cf\u51cf\u5c1126%\u3002", "conclusion": "Slim-SC\u4e3a\u81ea\u4e00\u81f4\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u9ad8\u6548\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u66ff\u4ee3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2509.13681", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13681", "abs": "https://arxiv.org/abs/2509.13681", "authors": ["Hang Li", "Dianmo Sheng", "Qiankun Dong", "Zichun Wang", "Zhiwei Xu", "Tao Li"], "title": "FishBEV: Distortion-Resilient Bird's Eye View Segmentation with Surround-View Fisheye Cameras", "comment": "8 pages, 4 figures", "summary": "As a cornerstone technique for autonomous driving, Bird's Eye View (BEV)\nsegmentation has recently achieved remarkable progress with pinhole cameras.\nHowever, it is non-trivial to extend the existing methods to fisheye cameras\nwith severe geometric distortion, ambiguous multi-view correspondences and\nunstable temporal dynamics, all of which significantly degrade BEV performance.\nTo address these challenges, we propose FishBEV, a novel BEV segmentation\nframework specifically tailored for fisheye cameras. This framework introduces\nthree complementary innovations, including a Distortion-Resilient Multi-scale\nExtraction (DRME) backbone that learns robust features under distortion while\npreserving scale consistency, an Uncertainty-aware Spatial Cross-Attention\n(U-SCA) mechanism that leverages uncertainty estimation for reliable cross-view\nalignment, a Distance-aware Temporal Self-Attention (D-TSA) module that\nadaptively balances near field details and far field context to ensure temporal\ncoherence. Extensive experiments on the Synwoodscapes dataset demonstrate that\nFishBEV consistently outperforms SOTA baselines, regarding the performance\nevaluation of FishBEV on the surround-view fisheye BEV segmentation tasks.", "AI": {"tldr": "FishBEV\u662f\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u9c7c\u773c\u76f8\u673a\u7684BEV\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u521b\u65b0\u6a21\u5757\u89e3\u51b3\u9c7c\u773c\u76f8\u673a\u7684\u51e0\u4f55\u7578\u53d8\u3001\u591a\u89c6\u56fe\u5bf9\u5e94\u6a21\u7cca\u548c\u65f6\u95f4\u52a8\u6001\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u5728Synwoodscapes\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709BEV\u5206\u5272\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u9488\u5b54\u76f8\u673a\uff0c\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u5b58\u5728\u4e25\u91cd\u51e0\u4f55\u7578\u53d8\u3001\u591a\u89c6\u56fe\u5bf9\u5e94\u6a21\u7cca\u548c\u65f6\u95f4\u52a8\u6001\u4e0d\u7a33\u5b9a\u95ee\u9898\u7684\u9c7c\u773c\u76f8\u673a\uff0c\u8fd9\u4e9b\u56e0\u7d20\u663e\u8457\u964d\u4f4e\u4e86BEV\u6027\u80fd\u3002", "method": "\u63d0\u51faFishBEV\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u521b\u65b0\u6a21\u5757\uff1a1) \u6297\u7578\u53d8\u591a\u5c3a\u5ea6\u63d0\u53d6(DRME)\u4e3b\u5e72\u7f51\u7edc\uff0c\u5728\u7578\u53d8\u4e0b\u5b66\u4e60\u9c81\u68d2\u7279\u5f81\u5e76\u4fdd\u6301\u5c3a\u5ea6\u4e00\u81f4\u6027\uff1b2) \u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7a7a\u95f4\u4ea4\u53c9\u6ce8\u610f\u529b(U-SCA)\u673a\u5236\uff0c\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5b9e\u73b0\u53ef\u9760\u7684\u8de8\u89c6\u56fe\u5bf9\u9f50\uff1b3) \u8ddd\u79bb\u611f\u77e5\u65f6\u95f4\u81ea\u6ce8\u610f\u529b(D-TSA)\u6a21\u5757\uff0c\u81ea\u9002\u5e94\u5e73\u8861\u8fd1\u573a\u7ec6\u8282\u548c\u8fdc\u573a\u4e0a\u4e0b\u6587\u4ee5\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5728Synwoodscapes\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFishBEV\u5728\u73af\u89c6\u9c7c\u773cBEV\u5206\u5272\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FishBEV\u662f\u9488\u5bf9\u9c7c\u773c\u76f8\u673aBEV\u5206\u5272\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684\u4e09\u4e2a\u4e92\u8865\u521b\u65b0\u6a21\u5757\u6210\u529f\u89e3\u51b3\u4e86\u9c7c\u773c\u76f8\u673a\u7279\u6709\u7684\u6311\u6218\uff0c\u5728\u6027\u80fd\u8bc4\u4f30\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u8868\u73b0\u3002"}}
{"id": "2509.14004", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14004", "abs": "https://arxiv.org/abs/2509.14004", "authors": ["Minjia Mao", "Bowen Yin", "Yu Zhu", "Xiao Fang"], "title": "Early Stopping Chain-of-thoughts in Large Language Models", "comment": null, "summary": "Reasoning large language models (LLMs) have demonstrated superior capacities\nin solving complicated problems by generating long chain-of-thoughts (CoT), but\nsuch a lengthy CoT incurs high inference costs. In this study, we introduce\nES-CoT, an inference-time method that shortens CoT generation by detecting\nanswer convergence and stopping early with minimal performance loss. At the end\nof each reasoning step, we prompt the LLM to output its current final answer,\ndenoted as a step answer. We then track the run length of consecutive identical\nstep answers as a measure of answer convergence. Once the run length exhibits a\nsharp increase and exceeds a minimum threshold, the generation is terminated.\nWe provide both empirical and theoretical support for this heuristic: step\nanswers steadily converge to the final answer, and large run-length jumps\nreliably mark this convergence. Experiments on five reasoning datasets across\nthree LLMs show that ES-CoT reduces the number of inference tokens by about\n41\\% on average while maintaining accuracy comparable to standard CoT. Further,\nES-CoT integrates seamlessly with self-consistency prompting and remains robust\nacross hyperparameter choices, highlighting it as a practical and effective\napproach for efficient reasoning.", "AI": {"tldr": "ES-CoT\u662f\u4e00\u79cd\u63a8\u7406\u65f6\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u6d4b\u7b54\u6848\u6536\u655b\u6027\u6765\u63d0\u524d\u505c\u6b62\u601d\u7ef4\u94fe\u751f\u6210\uff0c\u5e73\u5747\u51cf\u5c1141%\u7684\u63a8\u7406token\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6807\u51c6CoT\u76f8\u5f53\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u51b3\u590d\u6742\u95ee\u9898\u65f6\u9700\u8981\u751f\u6210\u957f\u601d\u7ef4\u94fe\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u63a8\u7406\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u63a8\u7406\u5f00\u9500\u7684\u65b9\u6cd5\u3002", "method": "\u5728\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u7ed3\u675f\u65f6\u63d0\u793aLLM\u8f93\u51fa\u5f53\u524d\u6700\u7ec8\u7b54\u6848\uff08\u6b65\u9aa4\u7b54\u6848\uff09\uff0c\u8ddf\u8e2a\u8fde\u7eed\u76f8\u540c\u6b65\u9aa4\u7b54\u6848\u7684\u8fd0\u884c\u957f\u5ea6\u4f5c\u4e3a\u6536\u655b\u5ea6\u91cf\uff0c\u5f53\u8fd0\u884c\u957f\u5ea6\u51fa\u73b0\u6025\u5267\u589e\u52a0\u5e76\u8d85\u8fc7\u6700\u5c0f\u9608\u503c\u65f6\u7ec8\u6b62\u751f\u6210\u3002", "result": "\u5728\u4e09\u4e2aLLM\u7684\u4e94\u4e2a\u63a8\u7406\u6570\u636e\u96c6\u4e0a\uff0cES-CoT\u5e73\u5747\u51cf\u5c11\u7ea641%\u7684\u63a8\u7406token\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6807\u51c6CoT\u76f8\u5f53\u7684\u51c6\u786e\u6027\uff0c\u5e76\u80fd\u65e0\u7f1d\u96c6\u6210\u81ea\u4e00\u81f4\u6027\u63d0\u793a\u3002", "conclusion": "ES-CoT\u662f\u4e00\u79cd\u5b9e\u7528\u6709\u6548\u7684\u63a8\u7406\u6548\u7387\u63d0\u5347\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u6d4b\u7b54\u6848\u6536\u655b\u6027\u5b9e\u73b0\u65e9\u671f\u505c\u6b62\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002"}}
{"id": "2509.13687", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13687", "abs": "https://arxiv.org/abs/2509.13687", "authors": ["Kaniz Fatema", "Emad A. Mohammed", "Sukhjit Singh Sehra"], "title": "Taylor-Series Expanded Kolmogorov-Arnold Network for Medical Imaging Classification", "comment": null, "summary": "Effective and interpretable classification of medical images is a challenge\nin computer-aided diagnosis, especially in resource-limited clinical settings.\nThis study introduces spline-based Kolmogorov-Arnold Networks (KANs) for\naccurate medical image classification with limited, diverse datasets. The\nmodels include SBTAYLOR-KAN, integrating B-splines with Taylor series;\nSBRBF-KAN, combining B-splines with Radial Basis Functions; and SBWAVELET-KAN,\nembedding B-splines in Morlet wavelet transforms. These approaches leverage\nspline-based function approximation to capture both local and global\nnonlinearities. The models were evaluated on brain MRI, chest X-rays,\ntuberculosis X-rays, and skin lesion images without preprocessing,\ndemonstrating the ability to learn directly from raw data. Extensive\nexperiments, including cross-dataset validation and data reduction analysis,\nshowed strong generalization and stability. SBTAYLOR-KAN achieved up to 98.93%\naccuracy, with a balanced F1-score, maintaining over 86% accuracy using only\n30% of the training data across three datasets. Despite class imbalance in the\nskin cancer dataset, experiments on both imbalanced and balanced versions\nshowed SBTAYLOR-KAN outperforming other models, achieving 68.22% accuracy.\nUnlike traditional CNNs, which require millions of parameters (e.g., ResNet50\nwith 24.18M), SBTAYLOR-KAN achieves comparable performance with just 2,872\ntrainable parameters, making it more suitable for constrained medical\nenvironments. Gradient-weighted Class Activation Mapping (Grad-CAM) was used\nfor interpretability, highlighting relevant regions in medical images. This\nframework provides a lightweight, interpretable, and generalizable solution for\nmedical image classification, addressing the challenges of limited datasets and\ndata-scarce scenarios in clinical AI applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e09\u79cd\u57fa\u4e8e\u6837\u6761\u7684Kolmogorov-Arnold\u7f51\u7edc\uff08KANs\uff09\u7528\u4e8e\u533b\u7597\u56fe\u50cf\u5206\u7c7b\uff1aSBTAYLOR-KAN\u3001SBRBF-KAN\u548cSBWAVELET-KAN\uff0c\u80fd\u591f\u5728\u6709\u9650\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5206\u7c7b\uff0c\u4e14\u53c2\u6570\u91cf\u6781\u5c11\uff08\u4ec52872\u4e2a\u53c2\u6570\uff09\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u4e34\u5e8a\u73af\u5883\u3002", "motivation": "\u89e3\u51b3\u533b\u7597\u56fe\u50cf\u5206\u7c7b\u5728\u8d44\u6e90\u6709\u9650\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u96c6\u6709\u9650\u4e14\u591a\u6837\u5316\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u5f00\u53d1\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u4e14\u8f7b\u91cf\u5316\u7684\u5206\u7c7b\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u57fa\u4e8e\u6837\u6761\u7684KAN\u6a21\u578b\uff1aSBTAYLOR-KAN\uff08B\u6837\u6761+\u6cf0\u52d2\u7ea7\u6570\uff09\u3001SBRBF-KAN\uff08B\u6837\u6761+\u5f84\u5411\u57fa\u51fd\u6570\uff09\u3001SBWAVELET-KAN\uff08B\u6837\u6761+Morlet\u5c0f\u6ce2\u53d8\u6362\uff09\uff0c\u5229\u7528\u6837\u6761\u57fa\u51fd\u6570\u903c\u8fd1\u6765\u6355\u83b7\u5c40\u90e8\u548c\u5168\u5c40\u975e\u7ebf\u6027\u7279\u5f81\u3002", "result": "SBTAYLOR-KAN\u5728\u591a\u4e2a\u533b\u7597\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8fbe\u523098.93%\u7684\u51c6\u786e\u7387\uff0c\u4f7f\u7528\u4ec530%\u8bad\u7ec3\u6570\u636e\u4ecd\u4fdd\u630186%\u4ee5\u4e0a\u51c6\u786e\u7387\uff0c\u5728\u76ae\u80a4\u764c\u6570\u636e\u96c6\u4e0a\u8fbe\u523068.22%\u51c6\u786e\u7387\uff0c\u53c2\u6570\u91cf\u4ec5\u4e3a2872\u4e2a\uff08\u76f8\u6bd4ResNet50\u76842418\u4e07\u4e2a\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u533b\u7597\u56fe\u50cf\u5206\u7c7b\u63d0\u4f9b\u4e86\u8f7b\u91cf\u5316\u3001\u53ef\u89e3\u91ca\u4e14\u6cdb\u5316\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u5408\u6570\u636e\u7a00\u7f3a\u7684\u4e34\u5e8aAI\u5e94\u7528\u573a\u666f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfCNN\u6a21\u578b\u53c2\u6570\u91cf\u5927\u3001\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\u7684\u95ee\u9898\u3002"}}
{"id": "2509.14008", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14008", "abs": "https://arxiv.org/abs/2509.14008", "authors": ["Hasan Abed Al Kader Hammoud", "Mohammad Zbeeb", "Bernard Ghanem"], "title": "Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale", "comment": "Technical Report", "summary": "We present Hala, a family of Arabic-centric instruction and translation\nmodels built with our translate-and-tune pipeline. We first compress a strong\nAR$\\leftrightarrow$EN teacher to FP8 (yielding $\\sim$2$\\times$ higher\nthroughput with no quality loss) and use it to create high-fidelity bilingual\nsupervision. A lightweight language model LFM2-1.2B is then fine-tuned on this\ndata and used to translate high-quality English instruction sets into Arabic,\nproducing a million-scale corpus tailored to instruction following. We train\nHala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to\nbalance Arabic specialization with base-model strengths. On Arabic-centric\nbenchmarks, Hala achieves state-of-the-art results within both the \"nano\"\n($\\leq$2B) and \"small\" (7-9B) categories, outperforming their bases. We release\nmodels, data, evaluation, and recipes to accelerate research in Arabic NLP.", "AI": {"tldr": "Hala\u662f\u4e00\u4e2a\u963f\u62c9\u4f2f\u8bed\u4e3a\u4e2d\u5fc3\u7684\u6307\u4ee4\u548c\u7ffb\u8bd1\u6a21\u578b\u5bb6\u65cf\uff0c\u901a\u8fc7\u7ffb\u8bd1\u8c03\u4f18\u6d41\u7a0b\u6784\u5efa\uff0c\u5728\u963f\u62c9\u4f2f\u8bed\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u963f\u62c9\u4f2f\u8bedNLP\u9886\u57df\u9ad8\u8d28\u91cf\u6307\u4ee4\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u963f\u62c9\u4f2f\u8bed\u7684\u6307\u4ee4\u8ddf\u968f\u6a21\u578b\u3002", "method": "\u4f7f\u7528FP8\u538b\u7f29\u7684AR\u2194EN\u6559\u5e08\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u53cc\u8bed\u76d1\u7763\u6570\u636e\uff0c\u7136\u540e\u7528\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u7ffb\u8bd1\u82f1\u6587\u6307\u4ee4\u96c6\u5230\u963f\u62c9\u4f2f\u8bed\uff0c\u6700\u540e\u8bad\u7ec3\u4e0d\u540c\u89c4\u6a21\u7684Hala\u6a21\u578b\u5e76\u5e94\u7528slerp\u5408\u5e76\u6280\u672f\u3002", "result": "\u5728\u963f\u62c9\u4f2f\u8bed\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHala\u5728\"nano\"(\u22642B)\u548c\"small\"(7-9B)\u7c7b\u522b\u4e2d\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u5176\u57fa\u7840\u6a21\u578b\u3002", "conclusion": "Hala\u6a21\u578b\u7cfb\u5217\u4e3a\u963f\u62c9\u4f2f\u8bedNLP\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u53d1\u5e03\u4e86\u6a21\u578b\u3001\u6570\u636e\u548c\u914d\u65b9\u4ee5\u52a0\u901f\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2509.13711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13711", "abs": "https://arxiv.org/abs/2509.13711", "authors": ["Qiuyu Tang", "Joshua Krinsky", "Aparna Bharati"], "title": "StyleProtect: Safeguarding Artistic Identity in Fine-tuned Diffusion Models", "comment": null, "summary": "The rapid advancement of generative models, particularly diffusion-based\napproaches, has inadvertently facilitated their potential for misuse. Such\nmodels enable malicious exploiters to replicate artistic styles that capture an\nartist's creative labor, personal vision, and years of dedication in an\ninexpensive manner. This has led to a rise in the need and exploration of\nmethods for protecting artworks against style mimicry. Although generic\ndiffusion models can easily mimic an artistic style, finetuning amplifies this\ncapability, enabling the model to internalize and reproduce the style with\nhigher fidelity and control. We hypothesize that certain cross-attention layers\nexhibit heightened sensitivity to artistic styles. Sensitivity is measured\nthrough activation strengths of attention layers in response to style and\ncontent representations, and assessing their correlations with features\nextracted from external models. Based on our findings, we introduce an\nefficient and lightweight protection strategy, StyleProtect, that achieves\neffective style defense against fine-tuned diffusion models by updating only\nselected cross-attention layers. Our experiments utilize a carefully curated\nartwork dataset based on WikiArt, comprising representative works from 30\nartists known for their distinctive and influential styles and cartoon\nanimations from the Anita dataset. The proposed method demonstrates promising\nperformance in safeguarding unique styles of artworks and anime from malicious\ndiffusion customization, while maintaining competitive imperceptibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faStyleProtect\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u66f4\u65b0\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\u6765\u6709\u6548\u9632\u5fa1\u9488\u5bf9\u827a\u672f\u98ce\u683c\u7684\u6076\u610f\u6269\u6563\u6a21\u578b\u6a21\u4eff\u653b\u51fb", "motivation": "\u751f\u6210\u6a21\u578b\u7279\u522b\u662f\u6269\u6563\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u4f7f\u5f97\u6076\u610f\u4f7f\u7528\u8005\u80fd\u591f\u5ec9\u4ef7\u590d\u5236\u827a\u672f\u5bb6\u7684\u72ec\u7279\u98ce\u683c\uff0c\u8fd9\u5bfc\u81f4\u4e86\u5bf9\u827a\u672f\u4f5c\u54c1\u98ce\u683c\u6a21\u4eff\u4fdd\u62a4\u65b9\u6cd5\u7684\u9700\u6c42\u589e\u52a0", "method": "\u7814\u7a76\u53d1\u73b0\u67d0\u4e9b\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\u5bf9\u827a\u672f\u98ce\u683c\u7279\u522b\u654f\u611f\uff0c\u57fa\u4e8e\u6b64\u63d0\u51faStyleProtect\u65b9\u6cd5\uff0c\u4ec5\u66f4\u65b0\u9009\u5b9a\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\u6765\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u98ce\u683c\u4fdd\u62a4", "result": "\u5b9e\u9a8c\u4f7f\u7528\u57fa\u4e8eWikiArt\u768430\u4f4d\u827a\u672f\u5bb6\u4f5c\u54c1\u6570\u636e\u96c6\u548cAnita\u52a8\u753b\u6570\u636e\u96c6\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u4fdd\u62a4\u827a\u672f\u4f5c\u54c1\u548c\u52a8\u6f2b\u7684\u72ec\u7279\u98ce\u683c\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u597d\u7684\u4e0d\u53ef\u611f\u77e5\u6027", "conclusion": "StyleProtect\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u8f7b\u91cf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u9632\u5fa1\u7ecf\u8fc7\u5fae\u8c03\u7684\u6269\u6563\u6a21\u578b\u5bf9\u827a\u672f\u98ce\u683c\u7684\u6076\u610f\u5b9a\u5236\u6a21\u4eff"}}
{"id": "2509.14023", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.14023", "abs": "https://arxiv.org/abs/2509.14023", "authors": ["Sami Ul Haq", "Sheila Castilho", "Yvette Graham"], "title": "Audio-Based Crowd-Sourced Evaluation of Machine Translation Quality", "comment": "Accepted at WMT2025 (ENNLP) for oral presented", "summary": "Machine Translation (MT) has achieved remarkable performance, with growing\ninterest in speech translation and multimodal approaches. However, despite\nthese advancements, MT quality assessment remains largely text centric,\ntypically relying on human experts who read and compare texts. Since many\nreal-world MT applications (e.g Google Translate Voice Mode, iFLYTEK\nTranslator) involve translation being spoken rather printed or read, a more\nnatural way to assess translation quality would be through speech as opposed\ntext-only evaluations. This study compares text-only and audio-based\nevaluations of 10 MT systems from the WMT General MT Shared Task, using\ncrowd-sourced judgments collected via Amazon Mechanical Turk. We additionally,\nperformed statistical significance testing and self-replication experiments to\ntest reliability and consistency of audio-based approach. Crowd-sourced\nassessments based on audio yield rankings largely consistent with text only\nevaluations but, in some cases, identify significant differences between\ntranslation systems. We attribute this to speech richer, more natural modality\nand propose incorporating speech-based assessments into future MT evaluation\nframeworks.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u6587\u672c\u548c\u97f3\u9891\u4e24\u79cd\u65b9\u5f0f\u8bc4\u4f30\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\uff0c\u53d1\u73b0\u57fa\u4e8e\u97f3\u9891\u7684\u8bc4\u4f30\u80fd\u66f4\u81ea\u7136\u5730\u53cd\u6620\u771f\u5b9e\u4f7f\u7528\u573a\u666f\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u80fd\u8bc6\u522b\u51fa\u6587\u672c\u8bc4\u4f30\u65e0\u6cd5\u53d1\u73b0\u7684\u7cfb\u7edf\u5dee\u5f02", "motivation": "\u5f53\u524d\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u65b9\u5f0f\uff0c\u4f46\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\u6d89\u53ca\u8bed\u97f3\u7ffb\u8bd1\uff0c\u9700\u8981\u66f4\u81ea\u7136\u7684\u8bed\u97f3\u8bc4\u4f30\u65b9\u5f0f\u6765\u66f4\u597d\u5730\u53cd\u6620\u771f\u5b9e\u4f7f\u7528\u6548\u679c", "method": "\u4f7f\u7528Amazon Mechanical Turk\u6536\u96c6\u4f17\u5305\u8bc4\u4f30\uff0c\u6bd4\u8f8310\u4e2aWMT\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u7684\u6587\u672c\u548c\u97f3\u9891\u8bc4\u4f30\u7ed3\u679c\uff0c\u5e76\u8fdb\u884c\u7edf\u8ba1\u663e\u8457\u6027\u6d4b\u8bd5\u548c\u81ea\u6211\u590d\u5236\u5b9e\u9a8c", "result": "\u57fa\u4e8e\u97f3\u9891\u7684\u8bc4\u4f30\u7ed3\u679c\u4e0e\u6587\u672c\u8bc4\u4f30\u57fa\u672c\u4e00\u81f4\uff0c\u4f46\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u80fd\u8bc6\u522b\u51fa\u663e\u8457\u7684\u7cfb\u7edf\u5dee\u5f02\uff0c\u8bed\u97f3\u6a21\u6001\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u81ea\u7136\u7684\u4fe1\u606f", "conclusion": "\u5efa\u8bae\u5c06\u57fa\u4e8e\u8bed\u97f3\u7684\u8bc4\u4f30\u7eb3\u5165\u672a\u6765\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u66f4\u597d\u5730\u53cd\u6620\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2d\u7684\u7ffb\u8bd1\u8d28\u91cf"}}
{"id": "2509.13713", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13713", "abs": "https://arxiv.org/abs/2509.13713", "authors": ["Tae-Wook Um", "Ki-Hyeon Kim", "Hyun-Duck Choi", "Hyo-Sung Ahn"], "title": "UM-Depth : Uncertainty Masked Self-Supervised Monocular Depth Estimation with Visual Odometry", "comment": null, "summary": "Monocular depth estimation has been increasingly adopted in robotics and\nautonomous driving for its ability to infer scene geometry from a single\ncamera. In self-supervised monocular depth estimation frameworks, the network\njointly generates and exploits depth and pose estimates during training,\nthereby eliminating the need for depth labels. However, these methods remain\nchallenged by uncertainty in the input data, such as low-texture or dynamic\nregions, which can cause reduced depth accuracy. To address this, we introduce\nUM-Depth, a framework that combines motion- and uncertainty-aware refinement to\nenhance depth accuracy at dynamic object boundaries and in textureless regions.\nSpecifically, we develop a teacherstudent training strategy that embeds\nuncertainty estimation into both the training pipeline and network\narchitecture, thereby strengthening supervision where photometric signals are\nweak. Unlike prior motion-aware approaches that incur inference-time overhead\nand rely on additional labels or auxiliary networks for real-time generation,\nour method uses optical flow exclusively within the teacher network during\ntraining, which eliminating extra labeling demands and any runtime cost.\nExtensive experiments on the KITTI and Cityscapes datasets demonstrate the\neffectiveness of our uncertainty-aware refinement. Overall, UM-Depth achieves\nstate-of-the-art results in both self-supervised depth and pose estimation on\nthe KITTI datasets.", "AI": {"tldr": "UM-Depth\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u8fd0\u52a8\u611f\u77e5\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u7ec6\u5316\u65b9\u6cd5\uff0c\u5728\u52a8\u6001\u7269\u4f53\u8fb9\u754c\u548c\u7eb9\u7406\u7f3a\u5931\u533a\u57df\u63d0\u9ad8\u6df1\u5ea6\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u65e0\u9700\u989d\u5916\u6807\u6ce8\u548c\u63a8\u7406\u65f6\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u76d1\u7763\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u5728\u4f4e\u7eb9\u7406\u533a\u57df\u548c\u52a8\u6001\u533a\u57df\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u6df1\u5ea6\u7cbe\u5ea6\u4e0b\u964d\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u8fd9\u4e9b\u4e0d\u786e\u5b9a\u6027\u5e76\u63d0\u9ad8\u7cbe\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5e08\u751f\u8bad\u7ec3\u7b56\u7565\uff0c\u5c06\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5d4c\u5165\u8bad\u7ec3\u6d41\u7a0b\u548c\u7f51\u7edc\u67b6\u6784\u4e2d\uff0c\u5728\u5149\u5ea6\u4fe1\u53f7\u5f31\u7684\u533a\u57df\u52a0\u5f3a\u76d1\u7763\u3002\u4ec5\u5728\u6559\u5b66\u7f51\u7edc\u8bad\u7ec3\u65f6\u4f7f\u7528\u5149\u6d41\uff0c\u907f\u514d\u989d\u5916\u6807\u6ce8\u548c\u8fd0\u884c\u65f6\u5f00\u9500\u3002", "result": "\u5728KITTI\u548cCityscapes\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7ec6\u5316\u65b9\u9762\u6709\u6548\uff0c\u5728KITTI\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u81ea\u76d1\u7763\u6df1\u5ea6\u548c\u59ff\u6001\u4f30\u8ba1\u7684\u6700\u5148\u8fdb\u7ed3\u679c\u3002", "conclusion": "UM-Depth\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u81ea\u76d1\u7763\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u989d\u5916\u63a8\u7406\u6210\u672c\u3002"}}
{"id": "2509.14031", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14031", "abs": "https://arxiv.org/abs/2509.14031", "authors": ["Pawe\u0142 M\u0105ka", "Yusuf Can Semerci", "Jan Scholtes", "Gerasimos Spanakis"], "title": "You Are What You Train: Effects of Data Composition on Training Context-aware Machine Translation Models", "comment": "EMNLP 2025 main conference", "summary": "Achieving human-level translations requires leveraging context to ensure\ncoherence and handle complex phenomena like pronoun disambiguation. Sparsity of\ncontextually rich examples in the standard training data has been hypothesized\nas the reason for the difficulty of context utilization. In this work, we\nsystematically validate this claim in both single- and multilingual settings by\nconstructing training datasets with a controlled proportions of contextually\nrelevant examples. We demonstrate a strong association between training data\nsparsity and model performance confirming sparsity as a key bottleneck.\nImportantly, we reveal that improvements in one contextual phenomenon do no\ngeneralize to others. While we observe some cross-lingual transfer, it is not\nsignificantly higher between languages within the same sub-family. Finally, we\npropose and empirically evaluate two training strategies designed to leverage\nthe available data. These strategies improve context utilization, resulting in\naccuracy gains of up to 6 and 8 percentage points on the ctxPro evaluation in\nsingle- and multilingual settings respectively.", "AI": {"tldr": "\u8bba\u6587\u9a8c\u8bc1\u4e86\u8bad\u7ec3\u6570\u636e\u4e2d\u4e0a\u4e0b\u6587\u4e30\u5bcc\u6837\u672c\u7684\u7a00\u758f\u6027\u662f\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u96be\u4ee5\u6709\u6548\u5229\u7528\u4e0a\u4e0b\u6587\u7684\u5173\u952e\u74f6\u9888\uff0c\u5e76\u901a\u8fc7\u6784\u5efa\u63a7\u5236\u6bd4\u4f8b\u7684\u6570\u636e\u96c6\u8bc1\u5b9e\u4e86\u7a00\u758f\u6027\u4e0e\u6027\u80fd\u7684\u5f3a\u5173\u8054\u6027\u3002", "motivation": "\u6807\u51c6\u8bad\u7ec3\u6570\u636e\u4e2d\u4e0a\u4e0b\u6587\u4e30\u5bcc\u6837\u672c\u7684\u7a00\u758f\u6027\u88ab\u8ba4\u4e3a\u662f\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u96be\u4ee5\u5b9e\u73b0\u4eba\u7c7b\u6c34\u5e73\u7ffb\u8bd1\uff08\u5982\u786e\u4fdd\u8fde\u8d2f\u6027\u548c\u5904\u7406\u4ee3\u8bcd\u6d88\u6b67\u7b49\u590d\u6742\u73b0\u8c61\uff09\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u9700\u8981\u7cfb\u7edf\u9a8c\u8bc1\u8fd9\u4e00\u5047\u8bbe\u3002", "method": "\u6784\u5efa\u4e86\u5177\u6709\u53d7\u63a7\u6bd4\u4f8b\u4e0a\u4e0b\u6587\u76f8\u5173\u793a\u4f8b\u7684\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5728\u5355\u8bed\u548c\u591a\u8bed\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u7cfb\u7edf\u6027\u9a8c\u8bc1\uff0c\u5e76\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u4e24\u79cd\u65e8\u5728\u5145\u5206\u5229\u7528\u53ef\u7528\u6570\u636e\u7684\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7a00\u758f\u6027\u4e0e\u6a21\u578b\u6027\u80fd\u5b58\u5728\u5f3a\u5173\u8054\uff0c\u786e\u8ba4\u7a00\u758f\u6027\u662f\u5173\u952e\u74f6\u9888\uff1b\u4e0d\u540c\u4e0a\u4e0b\u6587\u73b0\u8c61\u7684\u6539\u8fdb\u65e0\u6cd5\u76f8\u4e92\u6cdb\u5316\uff1b\u8de8\u8bed\u8a00\u8fc1\u79fb\u6709\u9650\uff1b\u63d0\u51fa\u7684\u8bad\u7ec3\u7b56\u7565\u5728\u5355\u8bed\u548c\u591a\u8bed\u8bbe\u7f6e\u4e0b\u5206\u522b\u5b9e\u73b0\u4e86\u6700\u9ad86%\u548c8%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u8bad\u7ec3\u6570\u636e\u7684\u7a00\u758f\u6027\u662f\u673a\u5668\u7ffb\u8bd1\u7684\u5173\u952e\u9650\u5236\u56e0\u7d20\uff0c\u9700\u8981\u9488\u5bf9\u6027\u7684\u8bad\u7ec3\u7b56\u7565\u6765\u6539\u5584\u4e0a\u4e0b\u6587\u5229\u7528\u80fd\u529b\uff0c\u4f46\u4e0d\u540c\u4e0a\u4e0b\u6587\u73b0\u8c61\u9700\u8981\u5206\u522b\u5904\u7406\uff0c\u8de8\u8bed\u8a00\u8fc1\u79fb\u6548\u679c\u6709\u9650\u3002"}}
{"id": "2509.13722", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13722", "abs": "https://arxiv.org/abs/2509.13722", "authors": ["Dingwei Zhang", "Dong Zhang", "Jinhui Tang"], "title": "Mitigating Query Selection Bias in Referring Video Object Segmentation", "comment": null, "summary": "Recently, query-based methods have achieved remarkable performance in\nReferring Video Object Segmentation (RVOS) by using textual static object\nqueries to drive cross-modal alignment. However, these static queries are\neasily misled by distractors with similar appearance or motion, resulting in\n\\emph{query selection bias}. To address this issue, we propose Triple Query\nFormer (TQF), which factorizes the referring query into three specialized\ncomponents: an appearance query for static attributes, an intra-frame\ninteraction query for spatial relations, and an inter-frame motion query for\ntemporal association. Instead of relying solely on textual embeddings, our\nqueries are dynamically constructed by integrating both linguistic cues and\nvisual guidance. Furthermore, we introduce two motion-aware aggregation modules\nthat enhance object token representations: Intra-frame Interaction Aggregation\nincorporates position-aware interactions among objects within a single frame,\nwhile Inter-frame Motion Aggregation leverages trajectory-guided alignment\nacross frames to ensure temporal coherence. Extensive experiments on multiple\nRVOS benchmarks demonstrate the advantages of TQF and the effectiveness of our\nstructured query design and motion-aware aggregation modules.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Triple Query Former (TQF)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5f15\u7528\u67e5\u8be2\u5206\u89e3\u4e3a\u4e09\u4e2a\u4e13\u95e8\u7ec4\u4ef6\u6765\u89e3\u51b3RVOS\u4e2d\u7684\u67e5\u8be2\u9009\u62e9\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u67e5\u8be2\u7684RVOS\u65b9\u6cd5\u4f7f\u7528\u9759\u6001\u6587\u672c\u67e5\u8be2\u8fdb\u884c\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u4f46\u5bb9\u6613\u88ab\u5916\u89c2\u6216\u8fd0\u52a8\u76f8\u4f3c\u7684\u5e72\u6270\u7269\u8bef\u5bfc\uff0c\u5bfc\u81f4\u67e5\u8be2\u9009\u62e9\u504f\u5dee\u95ee\u9898\u3002", "method": "\u5c06\u5f15\u7528\u67e5\u8be2\u5206\u89e3\u4e3a\u4e09\u4e2a\u4e13\u95e8\u7ec4\u4ef6\uff1a\u5916\u89c2\u67e5\u8be2\uff08\u9759\u6001\u5c5e\u6027\uff09\u3001\u5e27\u5185\u4ea4\u4e92\u67e5\u8be2\uff08\u7a7a\u95f4\u5173\u7cfb\uff09\u548c\u5e27\u95f4\u8fd0\u52a8\u67e5\u8be2\uff08\u65f6\u95f4\u5173\u8054\uff09\uff1b\u5f15\u5165\u4e24\u4e2a\u8fd0\u52a8\u611f\u77e5\u805a\u5408\u6a21\u5757\uff1a\u5e27\u5185\u4ea4\u4e92\u805a\u5408\u548c\u5e27\u95f4\u8fd0\u52a8\u805a\u5408\u3002", "result": "\u5728\u591a\u4e2aRVOS\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86TQF\u65b9\u6cd5\u7684\u4f18\u52bf\u4ee5\u53ca\u7ed3\u6784\u5316\u67e5\u8be2\u8bbe\u8ba1\u548c\u8fd0\u52a8\u611f\u77e5\u805a\u5408\u6a21\u5757\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5f15\u7528\u67e5\u8be2\u5206\u89e3\u4e3a\u4e09\u4e2a\u4e13\u95e8\u7ec4\u4ef6\u5e76\u7ed3\u5408\u8fd0\u52a8\u611f\u77e5\u805a\u5408\uff0cTQF\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u67e5\u8be2\u9009\u62e9\u504f\u5dee\u95ee\u9898\uff0c\u5728RVOS\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.14034", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14034", "abs": "https://arxiv.org/abs/2509.14034", "authors": ["Zijie Lin", "Bryan Hooi"], "title": "Enhancing Multi-Agent Debate System Performance via Confidence Expression", "comment": "EMNLP'25 Findings", "summary": "Generative Large Language Models (LLMs) have demonstrated remarkable\nperformance across a wide range of tasks. Recent research has introduced\nMulti-Agent Debate (MAD) systems, which leverage multiple LLMs to simulate\nhuman debate and thereby improve task performance. However, while some LLMs may\npossess superior knowledge or reasoning capabilities for specific tasks, they\noften struggle to clearly communicate this advantage during debates, in part\ndue to a lack of confidence expression. Moreover, inappropriate confidence\nexpression can cause agents in MAD systems to either stubbornly maintain\nincorrect beliefs or converge prematurely on suboptimal answers, ultimately\nreducing debate effectiveness and overall system performance. To address these\nchallenges, we propose incorporating confidence expression into MAD systems to\nallow LLMs to explicitly communicate their confidence levels. To validate this\napproach, we develop ConfMAD, a MAD framework that integrates confidence\nexpression throughout the debate process. Experimental results demonstrate the\neffectiveness of our method, and we further analyze how confidence influences\ndebate dynamics, offering insights into the design of confidence-aware MAD\nsystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ConfMAD\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u7cfb\u7edf\u4e2d\u5f15\u5165\u7f6e\u4fe1\u5ea6\u8868\u8fbe\u673a\u5236\uff0c\u89e3\u51b3\u4e86LLMs\u5728\u8fa9\u8bba\u4e2d\u96be\u4ee5\u6709\u6548\u6c9f\u901a\u77e5\u8bc6\u4f18\u52bf\u548c\u907f\u514d\u8fc7\u65e9\u6536\u655b\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fa9\u8bba\u6548\u679c\u548c\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MAD\u7cfb\u7edf\u4e2d\uff0c\u867d\u7136\u67d0\u4e9bLLMs\u5bf9\u7279\u5b9a\u4efb\u52a1\u5177\u6709\u66f4\u4f18\u7684\u77e5\u8bc6\u6216\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u7f6e\u4fe1\u5ea6\u8868\u8fbe\uff0c\u96be\u4ee5\u5728\u8fa9\u8bba\u4e2d\u6e05\u6670\u4f20\u8fbe\u8fd9\u79cd\u4f18\u52bf\uff0c\u540c\u65f6\u4e0d\u6070\u5f53\u7684\u7f6e\u4fe1\u5ea6\u8868\u8fbe\u4f1a\u5bfc\u81f4\u667a\u80fd\u4f53\u56fa\u6267\u575a\u6301\u9519\u8bef\u4fe1\u5ff5\u6216\u8fc7\u65e9\u6536\u655b\u4e8e\u6b21\u4f18\u7b54\u6848\uff0c\u964d\u4f4e\u8fa9\u8bba\u6548\u679c\u3002", "method": "\u63d0\u51faConfMAD\u6846\u67b6\uff0c\u5728\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u8fc7\u7a0b\u4e2d\u96c6\u6210\u7f6e\u4fe1\u5ea6\u8868\u8fbe\u673a\u5236\uff0c\u4f7fLLMs\u80fd\u591f\u660e\u786e\u4f20\u8fbe\u5176\u7f6e\u4fe1\u5ea6\u6c34\u5e73\uff0c\u4ece\u800c\u6539\u5584\u8fa9\u8bba\u52a8\u6001\u548c\u51b3\u7b56\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u5e76\u8fdb\u4e00\u6b65\u5206\u6790\u4e86\u7f6e\u4fe1\u5ea6\u5982\u4f55\u5f71\u54cd\u8fa9\u8bba\u52a8\u6001\uff0c\u4e3a\u8bbe\u8ba1\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7684MAD\u7cfb\u7edf\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "conclusion": "\u5f15\u5165\u7f6e\u4fe1\u5ea6\u8868\u8fbe\u80fd\u591f\u663e\u8457\u63d0\u5347\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u7cfb\u7edf\u7684\u6027\u80fd\uff0cConfMAD\u6846\u67b6\u4e3a\u89e3\u51b3LLMs\u5728\u534f\u4f5c\u8fa9\u8bba\u4e2d\u7684\u6c9f\u901a\u548c\u6536\u655b\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13747", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13747", "abs": "https://arxiv.org/abs/2509.13747", "authors": ["Ming Dai", "Wenxuan Cheng", "Jiang-Jiang Liu", "Lingfeng Yang", "Zhenhua Feng", "Wankou Yang", "Jingdong Wang"], "title": "Improving Generalized Visual Grounding with Instance-aware Joint Learning", "comment": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI) in September 2025", "summary": "Generalized visual grounding tasks, including Generalized Referring\nExpression Comprehension (GREC) and Segmentation (GRES), extend the classical\nvisual grounding paradigm by accommodating multi-target and non-target\nscenarios. Specifically, GREC focuses on accurately identifying all referential\nobjects at the coarse bounding box level, while GRES aims for achieve\nfine-grained pixel-level perception. However, existing approaches typically\ntreat these tasks independently, overlooking the benefits of jointly training\nGREC and GRES to ensure consistent multi-granularity predictions and streamline\nthe overall process. Moreover, current methods often treat GRES as a semantic\nsegmentation task, neglecting the crucial role of instance-aware capabilities\nand the necessity of ensuring consistent predictions between instance-level\nboxes and masks. To address these limitations, we propose InstanceVG, a\nmulti-task generalized visual grounding framework equipped with instance-aware\ncapabilities, which leverages instance queries to unify the joint and\nconsistency predictions of instance-level boxes and masks. To the best of our\nknowledge, InstanceVG is the first framework to simultaneously tackle both GREC\nand GRES while incorporating instance-aware capabilities into generalized\nvisual grounding. To instantiate the framework, we assign each instance query a\nprior reference point, which also serves as an additional basis for target\nmatching. This design facilitates consistent predictions of points, boxes, and\nmasks for the same instance. Extensive experiments obtained on ten datasets\nacross four tasks demonstrate that InstanceVG achieves state-of-the-art\nperformance, significantly surpassing the existing methods in various\nevaluation metrics. The code and model will be publicly available at\nhttps://github.com/Dmmm1997/InstanceVG.", "AI": {"tldr": "InstanceVG\u662f\u4e00\u4e2a\u591a\u4efb\u52a1\u901a\u7528\u89c6\u89c9\u5b9a\u4f4d\u6846\u67b6\uff0c\u9996\u6b21\u540c\u65f6\u5904\u7406GREC\u548cGRES\u4efb\u52a1\uff0c\u901a\u8fc7\u5b9e\u4f8b\u67e5\u8be2\u7edf\u4e00\u5b9e\u4f8b\u7ea7\u8fb9\u754c\u6846\u548c\u63a9\u7801\u7684\u8054\u5408\u4e00\u81f4\u6027\u9884\u6d4b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u72ec\u7acb\u5904\u7406GREC\u548cGRES\u4efb\u52a1\uff0c\u5ffd\u7565\u4e86\u8054\u5408\u8bad\u7ec3\u7684\u597d\u5904\uff0c\u4e14\u5c06GRES\u89c6\u4e3a\u8bed\u4e49\u5206\u5272\u4efb\u52a1\uff0c\u5ffd\u89c6\u4e86\u5b9e\u4f8b\u611f\u77e5\u80fd\u529b\u548c\u5b9e\u4f8b\u7ea7\u8fb9\u754c\u6846\u4e0e\u63a9\u7801\u4e00\u81f4\u6027\u9884\u6d4b\u7684\u91cd\u8981\u6027\u3002", "method": "\u63d0\u51faInstanceVG\u6846\u67b6\uff0c\u4f7f\u7528\u5b9e\u4f8b\u67e5\u8be2\u7edf\u4e00\u5b9e\u4f8b\u7ea7\u8fb9\u754c\u6846\u548c\u63a9\u7801\u7684\u8054\u5408\u4e00\u81f4\u6027\u9884\u6d4b\uff0c\u4e3a\u6bcf\u4e2a\u5b9e\u4f8b\u67e5\u8be2\u5206\u914d\u5148\u9a8c\u53c2\u8003\u70b9\uff0c\u4fc3\u8fdb\u540c\u4e00\u5b9e\u4f8b\u7684\u70b9\u3001\u8fb9\u754c\u6846\u548c\u63a9\u7801\u7684\u4e00\u81f4\u6027\u9884\u6d4b\u3002", "result": "\u572810\u4e2a\u6570\u636e\u96c6\u4e0a\u76844\u4e2a\u4efb\u52a1\u5b9e\u9a8c\u4e2d\uff0cInstanceVG\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u5404\u79cd\u8bc4\u4f30\u6307\u6807\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "InstanceVG\u662f\u7b2c\u4e00\u4e2a\u540c\u65f6\u5904\u7406GREC\u548cGRES\u4efb\u52a1\u5e76\u878d\u5165\u5b9e\u4f8b\u611f\u77e5\u80fd\u529b\u7684\u901a\u7528\u89c6\u89c9\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u4f8b\u67e5\u8be2\u548c\u53c2\u8003\u70b9\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u591a\u7c92\u5ea6\u9884\u6d4b\u7684\u4e00\u81f4\u6027\uff0c\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2509.14036", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14036", "abs": "https://arxiv.org/abs/2509.14036", "authors": ["Zekang Liu", "Wei Feng", "Fanhua Shang", "Lianyu Hu", "Jichao Feng", "Liqing Gao"], "title": "SSL-SSAW: Self-Supervised Learning with Sigmoid Self-Attention Weighting for Question-Based Sign Language Translation", "comment": null, "summary": "Sign Language Translation (SLT) bridges the communication gap between deaf\npeople and hearing people, where dialogue provides crucial contextual cues to\naid in translation. Building on this foundational concept, this paper proposes\nQuestion-based Sign Language Translation (QB-SLT), a novel task that explores\nthe efficient integration of dialogue. Unlike gloss (sign language\ntranscription) annotations, dialogue naturally occurs in communication and is\neasier to annotate. The key challenge lies in aligning multimodality features\nwhile leveraging the context of the question to improve translation. To address\nthis issue, we propose a cross-modality Self-supervised Learning with Sigmoid\nSelf-attention Weighting (SSL-SSAW) fusion method for sign language\ntranslation. Specifically, we employ contrastive learning to align\nmultimodality features in QB-SLT, then introduce a Sigmoid Self-attention\nWeighting (SSAW) module for adaptive feature extraction from question and sign\nlanguage sequences. Additionally, we leverage available question text through\nself-supervised learning to enhance representation and translation\ncapabilities. We evaluated our approach on newly constructed CSL-Daily-QA and\nPHOENIX-2014T-QA datasets, where SSL-SSAW achieved SOTA performance. Notably,\neasily accessible question assistance can achieve or even surpass the\nperformance of gloss assistance. Furthermore, visualization results demonstrate\nthe effectiveness of incorporating dialogue in improving translation quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u95ee\u9898\u7684\u624b\u8bed\u7ffb\u8bd1(QB-SLT)\u65b0\u4efb\u52a1\uff0c\u901a\u8fc7\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\uff0c\u5f00\u53d1\u4e86SSL-SSAW\u8de8\u6a21\u6001\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u65b0\u5efa\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u624b\u8bed\u7ffb\u8bd1\u9700\u8981\u5bf9\u8bdd\u63d0\u4f9b\u4e0a\u4e0b\u6587\u7ebf\u7d22\uff0c\u4f46\u4f20\u7edf\u7684\u624b\u8bed\u6807\u6ce8(gloss)\u6210\u672c\u9ad8\u4e14\u4e0d\u81ea\u7136\u3002\u5bf9\u8bdd\u5728\u4ea4\u6d41\u4e2d\u81ea\u7136\u53d1\u751f\u4e14\u66f4\u5bb9\u6613\u6807\u6ce8\uff0c\u56e0\u6b64\u63a2\u7d22\u5982\u4f55\u6709\u6548\u6574\u5408\u5bf9\u8bdd\u4fe1\u606f\u6765\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u8de8\u6a21\u6001\u81ea\u76d1\u7763\u5b66\u4e60\u4e0eSigmoid\u81ea\u6ce8\u610f\u529b\u52a0\u6743(SSL-SSAW)\u878d\u5408\u65b9\u6cd5\uff1a1)\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u591a\u6a21\u6001\u7279\u5f81\uff1b2)SSAW\u6a21\u5757\u81ea\u9002\u5e94\u63d0\u53d6\u95ee\u9898\u548c\u624b\u8bed\u5e8f\u5217\u7279\u5f81\uff1b3)\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u5229\u7528\u95ee\u9898\u6587\u672c\u589e\u5f3a\u8868\u793a\u80fd\u529b\u3002", "result": "\u5728\u65b0\u5efa\u7684CSL-Daily-QA\u548cPHOENIX-2014T-QA\u6570\u636e\u96c6\u4e0a\u8fbe\u5230state-of-the-art\u6027\u80fd\uff0c\u6613\u83b7\u53d6\u7684\u95ee\u9898\u8f85\u52a9\u53ef\u4ee5\u8fbe\u5230\u751a\u81f3\u8d85\u8d8a\u4f20\u7edfgloss\u8f85\u52a9\u7684\u6548\u679c\uff0c\u53ef\u89c6\u5316\u7ed3\u679c\u8bc1\u660e\u4e86\u5bf9\u8bdd\u6574\u5408\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u57fa\u4e8e\u95ee\u9898\u7684\u5bf9\u8bdd\u6574\u5408\u4e3a\u624b\u8bed\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u63d0\u51fa\u7684SSL-SSAW\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\u548c\u4e0a\u4e0b\u6587\u5229\u7528\u7684\u6311\u6218\uff0c\u4e3a\u624b\u8bed\u7ffb\u8bd1\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.13754", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13754", "abs": "https://arxiv.org/abs/2509.13754", "authors": ["Hao Yin", "Xin Man", "Feiyu Chen", "Jie Shao", "Heng Tao Shen"], "title": "Cross-modal Full-mode Fine-grained Alignment for Text-to-Image Person Retrieval", "comment": null, "summary": "Text-to-Image Person Retrieval (TIPR) is a cross-modal matching task that\naims to retrieve the most relevant person images based on a given text query.\nThe key challenge in TIPR lies in achieving effective alignment between textual\nand visual modalities within a common latent space. To address this challenge,\nprior approaches incorporate attention mechanisms for implicit cross-modal\nlocal alignment. However, they lack the ability to verify whether all local\nfeatures are correctly aligned. Moreover, existing methods primarily focus on\nhard negative samples during model updates, with the goal of refining\ndistinctions between positive and negative pairs, often neglecting incorrectly\nmatched positive pairs. To alleviate these issues, we propose FMFA, a\ncross-modal Full-Mode Fine-grained Alignment framework, which enhances global\nmatching through explicit fine-grained alignment and existing implicit\nrelational reasoning -- hence the term ``full-mode\" -- without requiring\nadditional supervision. Specifically, we design an Adaptive Similarity\nDistribution Matching (A-SDM) module to rectify unmatched positive sample\npairs. A-SDM adaptively pulls the unmatched positive pairs closer in the joint\nembedding space, thereby achieving more precise global alignment. Additionally,\nwe introduce an Explicit Fine-grained Alignment (EFA) module, which makes up\nfor the lack of verification capability of implicit relational reasoning. EFA\nstrengthens explicit cross-modal fine-grained interactions by sparsifying the\nsimilarity matrix and employs a hard coding method for local alignment. Our\nproposed method is evaluated on three public datasets, achieving\nstate-of-the-art performance among all global matching methods. Our code is\navailable at https://github.com/yinhao1102/FMFA.", "AI": {"tldr": "FMFA\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u548c\u81ea\u9002\u5e94\u76f8\u4f3c\u5ea6\u5206\u5e03\u5339\u914d\uff0c\u89e3\u51b3\u4e86\u6587\u672c-\u56fe\u50cf\u4eba\u5458\u68c0\u7d22\u4e2d\u5c40\u90e8\u7279\u5f81\u5bf9\u9f50\u9a8c\u8bc1\u4e0d\u8db3\u548c\u6b63\u6837\u672c\u5bf9\u8bef\u5339\u914d\u7684\u95ee\u9898\uff0c\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6587\u672c-\u56fe\u50cf\u4eba\u5458\u68c0\u7d22\u4e2d\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5c40\u90e8\u7279\u5f81\u5bf9\u9f50\u9a8c\u8bc1\u80fd\u529b\uff0c\u4ee5\u53ca\u8fc7\u5ea6\u5173\u6ce8\u786c\u8d1f\u6837\u672c\u800c\u5ffd\u89c6\u8bef\u5339\u914d\u6b63\u6837\u672c\u5bf9\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5168\u6a21\u5f0f\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u6846\u67b6FMFA\uff0c\u5305\u542b\u81ea\u9002\u5e94\u76f8\u4f3c\u5ea6\u5206\u5e03\u5339\u914d\u6a21\u5757(A-SDM)\u6765\u4fee\u6b63\u672a\u5339\u914d\u7684\u6b63\u6837\u672c\u5bf9\uff0c\u4ee5\u53ca\u663e\u5f0f\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u6a21\u5757(EFA)\u901a\u8fc7\u7a00\u758f\u5316\u76f8\u4f3c\u5ea6\u77e9\u9635\u548c\u786c\u7f16\u7801\u65b9\u6cd5\u52a0\u5f3a\u5c40\u90e8\u5bf9\u9f50\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u6240\u6709\u5168\u5c40\u5339\u914d\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "FMFA\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u548c\u9690\u5f0f\u76f8\u7ed3\u5408\u7684\u5168\u6a21\u5f0f\u5bf9\u9f50\u65b9\u5f0f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8de8\u6a21\u6001\u68c0\u7d22\u7684\u7cbe\u5ea6\uff0c\u65e0\u9700\u989d\u5916\u76d1\u7763\u5373\u53ef\u5b9e\u73b0\u66f4\u597d\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u6548\u679c\u3002"}}
{"id": "2509.14128", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.14128", "abs": "https://arxiv.org/abs/2509.14128", "authors": ["Monica Sekoyan", "Nithin Rao Koluguri", "Nune Tadevosyan", "Piotr Zelasko", "Travis Bartley", "Nick Karpov", "Jagadeesh Balam", "Boris Ginsburg"], "title": "Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance Models for Multilingual ASR and AST", "comment": "Mini Version of it Submitted to ICASSP 2026", "summary": "This report introduces Canary-1B-v2, a fast, robust multilingual model for\nAutomatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built\nwith a FastConformer encoder and Transformer decoder, it supports 25 languages\nprimarily European. The model was trained on 1.7M hours of total data samples,\nincluding Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce\nhallucinations for ASR and AST. We describe its two-stage pre-training and\nfine-tuning process with dynamic data balancing, as well as experiments with an\nnGPT encoder. Results show nGPT scales well with massive data, while\nFastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the\nNeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable\nsegment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2\noutperforms Whisper-large-v3 on English ASR while being 10x faster, and\ndelivers competitive multilingual ASR and AST performance against larger models\nlike Seamless-M4T-v2-large and LLM-based systems. We also release\nParakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the\nsame 25 languages with just 600M parameters.", "AI": {"tldr": "Canary-1B-v2\u662f\u4e00\u4e2a\u5feb\u901f\u3001\u9c81\u68d2\u7684\u591a\u8bed\u8a00\u8bed\u97f3\u8bc6\u522b\u548c\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\uff0c\u652f\u630125\u79cd\u6b27\u6d32\u8bed\u8a00\uff0c\u6bd4Whisper-large-v3\u5feb10\u500d\u4e14\u6027\u80fd\u66f4\u597d\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u9ad8\u6548\u7684\u591a\u8bed\u8a00\u8bed\u97f3\u5904\u7406\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5904\u7406\u901f\u5ea6\uff0c\u5e76\u51cf\u5c11\u8bed\u97f3\u8bc6\u522b\u548c\u7ffb\u8bd1\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u91c7\u7528FastConformer\u7f16\u7801\u5668\u548cTransformer\u89e3\u7801\u5668\u67b6\u6784\uff0c\u8fdb\u884c\u4e24\u9636\u6bb5\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\uff0c\u4f7f\u7528\u52a8\u6001\u6570\u636e\u5e73\u8861\u6280\u672f\uff0c\u5e76\u52a0\u5165\u975e\u8bed\u97f3\u97f3\u9891\u6570\u636e\u51cf\u5c11\u5e7b\u89c9\u3002", "result": "\u5728\u82f1\u8bedASR\u4e0a\u8d85\u8d8aWhisper-large-v3\u4e14\u901f\u5ea6\u5feb10\u500d\uff0c\u5728\u591a\u8bed\u8a00ASR\u548cAST\u4efb\u52a1\u4e0a\u4e0eSeamless-M4T-v2-large\u7b49\u66f4\u5927\u6a21\u578b\u7ade\u4e89\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Canary-1B-v2\u8bc1\u660e\u4e86\u5728\u8bed\u97f3\u5904\u7406\u4efb\u52a1\u4e2d\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\u53ef\u4ee5\u5728\u8f83\u5c0f\u6a21\u578b\u89c4\u6a21\u4e0b\u5b9e\u73b0\u5353\u8d8a\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2509.13756", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13756", "abs": "https://arxiv.org/abs/2509.13756", "authors": ["Yuqi Yang", "Dongliang Chang", "Yuanchen Fang", "Yi-Zhe SonG", "Zhanyu Ma", "Jun Guo"], "title": "Controllable-Continuous Color Editing in Diffusion Model via Color Mapping", "comment": null, "summary": "In recent years, text-driven image editing has made significant progress.\nHowever, due to the inherent ambiguity and discreteness of natural language,\ncolor editing still faces challenges such as insufficient precision and\ndifficulty in achieving continuous control. Although linearly interpolating the\nembedding vectors of different textual descriptions can guide the model to\ngenerate a sequence of images with varying colors, this approach lacks precise\ncontrol over the range of color changes in the output images. Moreover, the\nrelationship between the interpolation coefficient and the resulting image\ncolor is unknown and uncontrollable. To address these issues, we introduce a\ncolor mapping module that explicitly models the correspondence between the text\nembedding space and image RGB values. This module predicts the corresponding\nembedding vector based on a given RGB value, enabling precise color control of\nthe generated images while maintaining semantic consistency. Users can specify\na target RGB range to generate images with continuous color variations within\nthe desired range, thereby achieving finer-grained, continuous, and\ncontrollable color editing. Experimental results demonstrate that our method\nperforms well in terms of color continuity and controllability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u989c\u8272\u6620\u5c04\u6a21\u5757\u6765\u89e3\u51b3\u6587\u672c\u9a71\u52a8\u56fe\u50cf\u7f16\u8f91\u4e2d\u989c\u8272\u63a7\u5236\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5efa\u7acb\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\u4e0e\u56fe\u50cfRGB\u503c\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u5b9e\u73b0\u7cbe\u786e\u8fde\u7eed\u7684\u989c\u8272\u63a7\u5236\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u7684\u6a21\u7cca\u6027\u548c\u79bb\u6563\u6027\u5bfc\u81f4\u6587\u672c\u9a71\u52a8\u7684\u56fe\u50cf\u989c\u8272\u7f16\u8f91\u5b58\u5728\u7cbe\u5ea6\u4e0d\u8db3\u548c\u96be\u4ee5\u8fde\u7eed\u63a7\u5236\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u7cbe\u786e\u63a7\u5236\u989c\u8272\u53d8\u5316\u8303\u56f4\u548c\u63d2\u503c\u7cfb\u6570\u4e0e\u56fe\u50cf\u989c\u8272\u7684\u5173\u7cfb\u3002", "method": "\u5f15\u5165\u989c\u8272\u6620\u5c04\u6a21\u5757\uff0c\u663e\u5f0f\u5efa\u6a21\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\u4e0e\u56fe\u50cfRGB\u503c\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u901a\u8fc7\u7ed9\u5b9aRGB\u503c\u9884\u6d4b\u5bf9\u5e94\u7684\u5d4c\u5165\u5411\u91cf\uff0c\u5728\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u7cbe\u786e\u7684\u989c\u8272\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u989c\u8272\u8fde\u7eed\u6027\u548c\u53ef\u63a7\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u6307\u5b9a\u76ee\u6807RGB\u8303\u56f4\u751f\u6210\u5177\u6709\u671f\u671b\u8303\u56f4\u5185\u8fde\u7eed\u989c\u8272\u53d8\u5316\u7684\u56fe\u50cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u7ec6\u7c92\u5ea6\u3001\u8fde\u7eed\u4e14\u53ef\u63a7\u7684\u989c\u8272\u7f16\u8f91\uff0c\u89e3\u51b3\u4e86\u6587\u672c\u9a71\u52a8\u56fe\u50cf\u989c\u8272\u7f16\u8f91\u4e2d\u7684\u7cbe\u5ea6\u548c\u8fde\u7eed\u63a7\u5236\u96be\u9898\u3002"}}
{"id": "2509.14161", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.14161", "abs": "https://arxiv.org/abs/2509.14161", "authors": ["Brian Yan", "Injy Hamed", "Shuichiro Shimizu", "Vasista Lodagala", "William Chen", "Olga Iakovenko", "Bashar Talafha", "Amir Hussein", "Alexander Polok", "Kalvin Chang", "Dominik Klement", "Sara Althubaiti", "Puyuan Peng", "Matthew Wiesner", "Thamar Solorio", "Ahmed Ali", "Sanjeev Khudanpur", "Shinji Watanabe", "Chih-Chen Chen", "Zhen Wu", "Karim Benharrak", "Anuj Diwan", "Samuele Cornell", "Eunjung Yeo", "Kwanghee Choi", "Carlos Carvalho", "Karen Rosero"], "title": "CS-FLEURS: A Massively Multilingual and Code-Switched Speech Dataset", "comment": null, "summary": "We present CS-FLEURS, a new dataset for developing and evaluating\ncode-switched speech recognition and translation systems beyond high-resourced\nlanguages. CS-FLEURS consists of 4 test sets which cover in total 113 unique\ncode-switched language pairs across 52 languages: 1) a 14 X-English language\npair set with real voices reading synthetically generated code-switched\nsentences, 2) a 16 X-English language pair set with generative text-to-speech\n3) a 60 {Arabic, Mandarin, Hindi, Spanish}-X language pair set with the\ngenerative text-to-speech, and 4) a 45 X-English lower-resourced language pair\ntest set with concatenative text-to-speech. Besides the four test sets,\nCS-FLEURS also provides a training set with 128 hours of generative\ntext-to-speech data across 16 X-English language pairs. Our hope is that\nCS-FLEURS helps to broaden the scope of future code-switched speech research.\nDataset link: https://huggingface.co/datasets/byan/cs-fleurs.", "AI": {"tldr": "CS-FLEURS\u662f\u4e00\u4e2a\u65b0\u7684\u4ee3\u7801\u5207\u6362\u8bed\u97f3\u8bc6\u522b\u548c\u7ffb\u8bd1\u6570\u636e\u96c6\uff0c\u5305\u542b4\u4e2a\u6d4b\u8bd5\u96c6\uff0c\u8986\u76d6113\u79cd\u8bed\u8a00\u5bf9\u768452\u79cd\u8bed\u8a00\uff0c\u65e8\u5728\u6269\u5c55\u4ee3\u7801\u5207\u6362\u8bed\u97f3\u7814\u7a76\u8303\u56f4", "motivation": "\u4e3a\u5f00\u53d1\u548c\u8bc4\u4f30\u4ee3\u7801\u5207\u6362\u8bed\u97f3\u8bc6\u522b\u548c\u7ffb\u8bd1\u7cfb\u7edf\u63d0\u4f9b\u6570\u636e\u96c6\u652f\u6301\uff0c\u7279\u522b\u662f\u9488\u5bf9\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e4b\u5916\u7684\u8bed\u8a00\uff0c\u63a8\u52a8\u4ee3\u7801\u5207\u6362\u8bed\u97f3\u7814\u7a76\u7684\u5e7f\u6cdb\u53d1\u5c55", "method": "\u6784\u5efa\u5305\u542b4\u4e2a\u6d4b\u8bd5\u96c6\u7684\u6570\u636e\u96c6\uff1a1\uff0914\u79cdX-\u82f1\u8bed\u8bed\u8a00\u5bf9\u7684\u771f\u5b9e\u8bed\u97f3\u5408\u6210\u4ee3\u7801\u5207\u6362\u53e5\u5b50\uff1b2\uff0916\u79cdX-\u82f1\u8bed\u8bed\u8a00\u5bf9\u7684\u751f\u6210\u5f0f\u6587\u672c\u8f6c\u8bed\u97f3\uff1b3\uff0960\u79cd{\u963f\u62c9\u4f2f\u8bed\u3001\u666e\u901a\u8bdd\u3001\u5370\u5730\u8bed\u3001\u897f\u73ed\u7259\u8bed}-X\u8bed\u8a00\u5bf9\u7684\u751f\u6210\u5f0f\u6587\u672c\u8f6c\u8bed\u97f3\uff1b4\uff0945\u79cdX-\u82f1\u8bed\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u7684\u62fc\u63a5\u5f0f\u6587\u672c\u8f6c\u8bed\u97f3\u3002\u540c\u65f6\u63d0\u4f9b128\u5c0f\u65f6\u7684\u8bad\u7ec3\u6570\u636e", "result": "\u6210\u529f\u521b\u5efa\u4e86CS-FLEURS\u6570\u636e\u96c6\uff0c\u5305\u542b113\u79cd\u72ec\u7279\u4ee3\u7801\u5207\u6362\u8bed\u8a00\u5bf9\uff0c\u8986\u76d652\u79cd\u8bed\u8a00\uff0c\u4e3a\u4ee3\u7801\u5207\u6362\u8bed\u97f3\u7814\u7a76\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u8d44\u6e90", "conclusion": "CS-FLEURS\u6570\u636e\u96c6\u5c06\u6709\u52a9\u4e8e\u62d3\u5bbd\u672a\u6765\u4ee3\u7801\u5207\u6362\u8bed\u97f3\u7814\u7a76\u7684\u8303\u56f4\uff0c\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u591a\u8bed\u8a00\u8bed\u97f3\u5904\u7406\u7cfb\u7edf\u63d0\u4f9b\u91cd\u8981\u8d44\u6e90"}}
{"id": "2509.13760", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13760", "abs": "https://arxiv.org/abs/2509.13760", "authors": ["Jinwoo Jeon", "JunHyeok Oh", "Hayeong Lee", "Byung-Jun Lee"], "title": "Iterative Prompt Refinement for Safer Text-to-Image Generation", "comment": null, "summary": "Text-to-Image (T2I) models have made remarkable progress in generating images\nfrom text prompts, but their output quality and safety still depend heavily on\nhow prompts are phrased. Existing safety methods typically refine prompts using\nlarge language models (LLMs), but they overlook the images produced, which can\nresult in unsafe outputs or unnecessary changes to already safe prompts. To\naddress this, we propose an iterative prompt refinement algorithm that uses\nVision Language Models (VLMs) to analyze both the input prompts and the\ngenerated images. By leveraging visual feedback, our method refines prompts\nmore effectively, improving safety while maintaining user intent and\nreliability comparable to existing LLM-based approaches. Additionally, we\nintroduce a new dataset labeled with both textual and visual safety signals\nusing off-the-shelf multi-modal LLM, enabling supervised fine-tuning.\nExperimental results demonstrate that our approach produces safer outputs\nwithout compromising alignment with user intent, offering a practical solution\nfor generating safer T2I content. Our code is available at\nhttps://github.com/ku-dmlab/IPR. \\textbf{\\textcolor{red}WARNING: This paper\ncontains examples of harmful or inappropriate images generated by models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8fed\u4ee3\u63d0\u793a\u8bcd\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u6587\u672c\u63d0\u793a\u548c\u751f\u6210\u56fe\u50cf\u6765\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u8f93\u51fa\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5927\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u63d0\u793a\u8bcd\uff0c\u4f46\u5ffd\u7565\u4e86\u751f\u6210\u7684\u56fe\u50cf\u5185\u5bb9\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e0d\u5b89\u5168\u8f93\u51fa\u6216\u5bf9\u5df2\u5b89\u5168\u63d0\u793a\u8fdb\u884c\u4e0d\u5fc5\u8981\u7684\u4fee\u6539\u3002", "method": "\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u8fed\u4ee3\u5206\u6790\u8f93\u5165\u63d0\u793a\u548c\u751f\u6210\u56fe\u50cf\uff0c\u901a\u8fc7\u89c6\u89c9\u53cd\u9988\u66f4\u6709\u6548\u5730\u4f18\u5316\u63d0\u793a\u8bcd\uff0c\u540c\u65f6\u5f15\u5165\u5305\u542b\u6587\u672c\u548c\u89c6\u89c9\u5b89\u5168\u4fe1\u53f7\u7684\u65b0\u6570\u636e\u96c6\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u4ea7\u751f\u66f4\u5b89\u5168\u7684\u8f93\u51fa\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u7528\u6237\u610f\u56fe\u7684\u4e00\u81f4\u6027\uff0c\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u53ef\u4e0e\u73b0\u6709LLM\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u751f\u6210\u66f4\u5b89\u5168\u7684\u6587\u672c\u5230\u56fe\u50cf\u5185\u5bb9\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u89c6\u89c9\u53cd\u9988\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5b89\u5168\u6027\u548c\u7528\u6237\u610f\u56fe\u4fdd\u6301\u7684\u5e73\u8861\u3002"}}
{"id": "2509.14171", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14171", "abs": "https://arxiv.org/abs/2509.14171", "authors": ["Yifan Liu", "Wenkuan Zhao", "Shanshan Zhong", "Jinghui Qin", "Mingfu Liang", "Zhongzhan Huang", "Wushao Wen"], "title": "AssoCiAm: A Benchmark for Evaluating Association Thinking while Circumventing Ambiguity", "comment": null, "summary": "Recent advancements in multimodal large language models (MLLMs) have garnered\nsignificant attention, offering a promising pathway toward artificial general\nintelligence (AGI). Among the essential capabilities required for AGI,\ncreativity has emerged as a critical trait for MLLMs, with association serving\nas its foundation. Association reflects a model' s ability to think creatively,\nmaking it vital to evaluate and understand. While several frameworks have been\nproposed to assess associative ability, they often overlook the inherent\nambiguity in association tasks, which arises from the divergent nature of\nassociations and undermines the reliability of evaluations. To address this\nissue, we decompose ambiguity into two types-internal ambiguity and external\nambiguity-and introduce AssoCiAm, a benchmark designed to evaluate associative\nability while circumventing the ambiguity through a hybrid computational\nmethod. We then conduct extensive experiments on MLLMs, revealing a strong\npositive correlation between cognition and association. Additionally, we\nobserve that the presence of ambiguity in the evaluation process causes MLLMs'\nbehavior to become more random-like. Finally, we validate the effectiveness of\nour method in ensuring more accurate and reliable evaluations. See Project Page\nfor the data and codes.", "AI": {"tldr": "\u63d0\u51fa\u4e86AssoCiAm\u57fa\u51c6\uff0c\u901a\u8fc7\u5206\u89e3\u6b67\u4e49\u4e3a\u5185\u90e8\u548c\u5916\u90e8\u6b67\u4e49\uff0c\u4f7f\u7528\u6df7\u5408\u8ba1\u7b97\u65b9\u6cd5\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8054\u60f3\u80fd\u529b\uff0c\u53d1\u73b0\u8ba4\u77e5\u4e0e\u8054\u60f3\u80fd\u529b\u6b63\u76f8\u5173\uff0c\u6b67\u4e49\u4f1a\u5bfc\u81f4\u6a21\u578b\u884c\u4e3a\u66f4\u968f\u673a\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u5ffd\u89c6\u4e86\u8054\u60f3\u4efb\u52a1\u4e2d\u56fa\u6709\u7684\u6b67\u4e49\u6027\uff0c\u8fd9\u79cd\u6b67\u4e49\u6e90\u4e8e\u8054\u60f3\u7684\u53d1\u6563\u6027\uff0c\u4f1a\u524a\u5f31\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002", "method": "\u5c06\u6b67\u4e49\u5206\u89e3\u4e3a\u5185\u90e8\u6b67\u4e49\u548c\u5916\u90e8\u6b67\u4e49\uff0c\u5f15\u5165AssoCiAm\u57fa\u51c6\uff0c\u91c7\u7528\u6df7\u5408\u8ba1\u7b97\u65b9\u6cd5\u6765\u89c4\u907f\u6b67\u4e49\u5e76\u8bc4\u4f30\u8054\u60f3\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8ba4\u77e5\u4e0e\u8054\u60f3\u80fd\u529b\u5b58\u5728\u5f3a\u6b63\u76f8\u5173\uff0c\u6b67\u4e49\u4f1a\u5bfc\u81f4MLLMs\u884c\u4e3a\u66f4\u968f\u673a\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "AssoCiAm\u57fa\u51c6\u80fd\u591f\u63d0\u4f9b\u66f4\u51c6\u786e\u53ef\u9760\u7684\u8054\u60f3\u80fd\u529b\u8bc4\u4f30\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u4e2d\u7684\u6b67\u4e49\u95ee\u9898\u3002"}}
{"id": "2509.13762", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13762", "abs": "https://arxiv.org/abs/2509.13762", "authors": ["Kai Chen", "Jin Xiao", "Leheng Zhang", "Kexuan Shi", "Shuhang Gu"], "title": "Task-Aware Image Signal Processor for Advanced Visual Perception", "comment": null, "summary": "In recent years, there has been a growing trend in computer vision towards\nexploiting RAW sensor data, which preserves richer information compared to\nconventional low-bit RGB images. Early studies mainly focused on enhancing\nvisual quality, while more recent efforts aim to leverage the abundant\ninformation in RAW data to improve the performance of visual perception tasks\nsuch as object detection and segmentation. However, existing approaches still\nface two key limitations: large-scale ISP networks impose heavy computational\noverhead, while methods based on tuning traditional ISP pipelines are\nrestricted by limited representational capacity.To address these issues, we\npropose Task-Aware Image Signal Processing (TA-ISP), a compact RAW-to-RGB\nframework that produces task-oriented representations for pretrained vision\nmodels. Instead of heavy dense convolutional pipelines, TA-ISP predicts a small\nset of lightweight, multi-scale modulation operators that act at global,\nregional, and pixel scales to reshape image statistics across different spatial\nextents. This factorized control significantly expands the range of spatially\nvarying transforms that can be represented while keeping memory usage,\ncomputation, and latency tightly constrained. Evaluated on several RAW-domain\ndetection and segmentation benchmarks under both daytime and nighttime\nconditions, TA-ISP consistently improves downstream accuracy while markedly\nreducing parameter count and inference time, making it well suited for\ndeployment on resource-constrained devices.", "AI": {"tldr": "\u63d0\u51faTA-ISP\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u591a\u5c3a\u5ea6\u8c03\u5236\u7b97\u5b50\u66ff\u4ee3\u4f20\u7edf\u5bc6\u96c6\u5377\u79efISP\u7f51\u7edc\uff0c\u5728\u4fdd\u6301\u4f4e\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u63d0\u5347RAW\u6570\u636e\u7684\u89c6\u89c9\u611f\u77e5\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RAW\u6570\u636e\u5904\u7406\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u5927\u89c4\u6a21ISP\u7f51\u7edc\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u800c\u57fa\u4e8e\u4f20\u7edfISP\u6d41\u6c34\u7ebf\u8c03\u4f18\u7684\u65b9\u6cd5\u8868\u793a\u80fd\u529b\u6709\u9650\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u4f4e\u8ba1\u7b97\u6210\u672c\u53c8\u80fd\u63d0\u5347\u611f\u77e5\u4efb\u52a1\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4efb\u52a1\u611f\u77e5\u56fe\u50cf\u4fe1\u53f7\u5904\u7406(TA-ISP)\u6846\u67b6\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u591a\u5c3a\u5ea6\u8c03\u5236\u7b97\u5b50\uff08\u5168\u5c40\u3001\u533a\u57df\u548c\u50cf\u7d20\u5c3a\u5ea6\uff09\u6765\u91cd\u5851\u56fe\u50cf\u7edf\u8ba1\u4fe1\u606f\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u5bc6\u96c6\u5377\u79ef\u6d41\u6c34\u7ebf\u3002", "result": "\u5728\u591a\u4e2aRAW\u57df\u68c0\u6d4b\u548c\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTA-ISP\u5728\u767d\u5929\u548c\u591c\u95f4\u6761\u4ef6\u4e0b\u90fd\u80fd\u6301\u7eed\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u7387\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u53c2\u6570\u6570\u91cf\u548c\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "TA-ISP\u6846\u67b6\u9002\u5408\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\uff0c\u901a\u8fc7\u56e0\u5b50\u5316\u63a7\u5236\u663e\u8457\u6269\u5c55\u4e86\u7a7a\u95f4\u53d8\u5316\u53d8\u6362\u7684\u8868\u793a\u8303\u56f4\uff0c\u540c\u65f6\u4e25\u683c\u63a7\u5236\u5185\u5b58\u4f7f\u7528\u3001\u8ba1\u7b97\u91cf\u548c\u5ef6\u8fdf\u3002"}}
{"id": "2509.14180", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7; J.4"], "pdf": "https://arxiv.org/pdf/2509.14180", "abs": "https://arxiv.org/abs/2509.14180", "authors": ["Akhil Theerthala"], "title": "Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs", "comment": "24 pages, 11 figures. The paper presents a novel framework for\n  generating a personal finance dataset. The resulting fine-tuned model and\n  dataset are publicly available", "summary": "Personalized financial advice requires consideration of user goals,\nconstraints, risk tolerance, and jurisdiction. Prior LLM work has focused on\nsupport systems for investors and financial planners. Simultaneously, numerous\nrecent studies examine broader personal finance tasks, including budgeting,\ndebt management, retirement, and estate planning, through agentic pipelines\nthat incur high maintenance costs, yielding less than 25% of their expected\nfinancial returns. In this study, we introduce a novel and reproducible\nframework that integrates relevant financial context with behavioral finance\nstudies to construct supervision data for end-to-end advisors. Using this\nframework, we create a 19k sample reasoning dataset and conduct a comprehensive\nfine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test\nsplit and a blind LLM-jury study, we demonstrate that through careful data\ncuration and behavioral integration, our 8B model achieves performance\ncomparable to significantly larger baselines (14-32B parameters) across factual\naccuracy, fluency, and personalization metrics while incurring 80% lower costs\nthan the larger counterparts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u91d1\u878d\u987e\u95ee\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u91d1\u878d\u80cc\u666f\u548c\u884c\u4e3a\u91d1\u878d\u5b66\u7814\u7a76\u6765\u6784\u5efa\u76d1\u7763\u6570\u636e\uff0c\u8bad\u7ec3\u4e86\u4e00\u4e2a8B\u53c2\u6570\u7684\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u6210\u672c\u3002", "motivation": "\u4e2a\u6027\u5316\u91d1\u878d\u5efa\u8bae\u9700\u8981\u8003\u8651\u7528\u6237\u76ee\u6807\u3001\u7ea6\u675f\u3001\u98ce\u9669\u627f\u53d7\u80fd\u529b\u548c\u53f8\u6cd5\u7ba1\u8f96\u533a\u3002\u73b0\u6709\u7684LLM\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u6295\u8d44\u8005\u652f\u6301\u7cfb\u7edf\uff0c\u800c\u5176\u4ed6\u7814\u7a76\u901a\u8fc7\u9ad8\u7ef4\u62a4\u6210\u672c\u7684\u4ee3\u7406\u7ba1\u9053\u5904\u7406\u4e2a\u4eba\u7406\u8d22\u4efb\u52a1\uff0c\u4f46\u8d22\u52a1\u56de\u62a5\u7387\u4e0d\u8db325%\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u6846\u67b6\uff0c\u6574\u5408\u76f8\u5173\u91d1\u878d\u80cc\u666f\u548c\u884c\u4e3a\u91d1\u878d\u5b66\u7814\u7a76\u6765\u6784\u5efa\u76d1\u7763\u6570\u636e\uff0c\u521b\u5efa\u4e8619k\u6837\u672c\u7684\u63a8\u7406\u6570\u636e\u96c6\uff0c\u5e76\u5bf9Qwen-3-8B\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u5fae\u8c03\u3002", "result": "\u901a\u8fc7\u4fdd\u7559\u6d4b\u8bd5\u96c6\u548c\u76f2\u6d4bLLM\u8bc4\u5ba1\u7814\u7a76\u663e\u793a\uff0c8B\u6a21\u578b\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u6d41\u7545\u6027\u548c\u4e2a\u6027\u5316\u6307\u6807\u4e0a\u4e0e\u66f4\u5927\u7684\u57fa\u7ebf\u6a21\u578b\uff0814-32B\u53c2\u6570\uff09\u8868\u73b0\u76f8\u5f53\uff0c\u540c\u65f6\u6210\u672c\u964d\u4f4e\u4e8680%\u3002", "conclusion": "\u901a\u8fc7\u7cbe\u5fc3\u7684\u6570\u636e\u7b56\u5212\u548c\u884c\u4e3a\u6574\u5408\uff0c\u8f83\u5c0f\u7684\u6a21\u578b\u53ef\u4ee5\u5728\u91d1\u878d\u987e\u95ee\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e0e\u66f4\u5927\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u6210\u672c\u3002"}}
{"id": "2509.13766", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13766", "abs": "https://arxiv.org/abs/2509.13766", "authors": ["Huichun Liu", "Xiaosong Li", "Yang Liu", "Xiaoqi Cheng", "Haishu Tan"], "title": "NDLPNet: A Location-Aware Nighttime Deraining Network and a Real-World Benchmark Dataset", "comment": null, "summary": "Visual degradation caused by rain streak artifacts in low-light conditions\nsignificantly hampers the performance of nighttime surveillance and autonomous\nnavigation. Existing image deraining techniques are primarily designed for\ndaytime conditions and perform poorly under nighttime illumination due to the\nspatial heterogeneity of rain distribution and the impact of light-dependent\nstripe visibility. In this paper, we propose a novel Nighttime Deraining\nLocation-enhanced Perceptual Network(NDLPNet) that effectively captures the\nspatial positional information and density distribution of rain streaks in\nlow-light environments. Specifically, we introduce a Position Perception Module\n(PPM) to capture and leverage spatial contextual information from input data,\nenhancing the model's capability to identify and recalibrate the importance of\ndifferent feature channels. The proposed nighttime deraining network can\neffectively remove the rain streaks as well as preserve the crucial background\ninformation. Furthermore, We construct a night scene rainy (NSR) dataset\ncomprising 900 image pairs, all based on real-world nighttime scenes, providing\na new benchmark for nighttime deraining task research. Extensive qualitative\nand quantitative experimental evaluations on both existing datasets and the NSR\ndataset consistently demonstrate our method outperform the state-of-the-art\n(SOTA) methods in nighttime deraining tasks. The source code and dataset is\navailable at https://github.com/Feecuin/NDLPNet.", "AI": {"tldr": "\u63d0\u51faNDLPNet\u7f51\u7edc\u7528\u4e8e\u591c\u95f4\u56fe\u50cf\u53bb\u96e8\uff0c\u901a\u8fc7\u4f4d\u7f6e\u611f\u77e5\u6a21\u5757\u6355\u6349\u96e8\u6761\u7eb9\u7a7a\u95f4\u4fe1\u606f\uff0c\u5728\u4f4e\u5149\u73af\u5883\u4e0b\u6709\u6548\u53bb\u9664\u96e8\u75d5\u5e76\u4fdd\u7559\u80cc\u666f\u7ec6\u8282\uff0c\u6784\u5efa\u4e86900\u5bf9\u771f\u5b9e\u591c\u95f4\u96e8\u666f\u6570\u636e\u96c6NSR\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u53bb\u96e8\u6280\u672f\u4e3b\u8981\u9488\u5bf9\u767d\u5929\u6761\u4ef6\uff0c\u5728\u591c\u95f4\u4f4e\u5149\u73af\u5883\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u96e8\u5206\u5e03\u7684\u7a7a\u95f4\u5f02\u8d28\u6027\u548c\u5149\u7ebf\u4f9d\u8d56\u7684\u6761\u7eb9\u53ef\u89c1\u6027\u5f71\u54cd\u663e\u8457\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u591c\u95f4\u6761\u4ef6\u7684\u53bb\u96e8\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u591c\u95f4\u53bb\u96e8\u4f4d\u7f6e\u589e\u5f3a\u611f\u77e5\u7f51\u7edc(NDLPNet)\uff0c\u5305\u542b\u4f4d\u7f6e\u611f\u77e5\u6a21\u5757(PPM)\u6765\u6355\u83b7\u7a7a\u95f4\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u589e\u5f3a\u6a21\u578b\u8bc6\u522b\u548c\u91cd\u65b0\u6821\u51c6\u4e0d\u540c\u7279\u5f81\u901a\u9053\u91cd\u8981\u6027\u7684\u80fd\u529b\u3002", "result": "\u5728\u73b0\u6709\u6570\u636e\u96c6\u548c\u65b0\u5efa\u7684NSR\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u5747\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591c\u95f4\u53bb\u96e8\u4efb\u52a1\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "NDLPNet\u80fd\u6709\u6548\u53bb\u9664\u591c\u95f4\u96e8\u6761\u7eb9\u540c\u65f6\u4fdd\u7559\u5173\u952e\u80cc\u666f\u4fe1\u606f\uff0c\u6784\u5efa\u7684NSR\u6570\u636e\u96c6\u4e3a\u591c\u95f4\u53bb\u96e8\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4e\u5149\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2509.14197", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.14197", "abs": "https://arxiv.org/abs/2509.14197", "authors": ["Vahid Ghafouri", "Robert McNeil", "Teodor Yankov", "Madeleine Sumption", "Luc Rocher", "Scott A. Hale", "Adam Mahdi"], "title": "Framing Migration: A Computational Analysis of UK Parliamentary Discourse", "comment": null, "summary": "We present a large-scale computational analysis of migration-related\ndiscourse in UK parliamentary debates spanning over 75 years and compare it\nwith US congressional discourse. Using open-weight LLMs, we annotate each\nstatement with high-level stances toward migrants and track the net tone toward\nmigrants across time and political parties. For the UK, we extend this with a\nsemi-automated framework for extracting fine-grained narrative frames to\ncapture nuances of migration discourse. Our findings show that, while US\ndiscourse has grown increasingly polarised, UK parliamentary attitudes remain\nrelatively aligned across parties, with a persistent ideological gap between\nLabour and the Conservatives, reaching its most negative level in 2025. The\nanalysis of narrative frames in the UK parliamentary statements reveals a shift\ntoward securitised narratives such as border control and illegal immigration,\nwhile longer-term integration-oriented frames such as social integration have\ndeclined. Moreover, discussions of national law about immigration have been\nreplaced over time by international law and human rights, revealing nuances in\ndiscourse trends. Taken together broadly, our findings demonstrate how LLMs can\nsupport scalable, fine-grained discourse analysis in political and historical\ncontexts.", "AI": {"tldr": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5206\u6790\u82f1\u7f8e\u8bae\u4f1a75\u5e74\u79fb\u6c11\u8fa9\u8bba\uff0c\u53d1\u73b0\u7f8e\u56fd\u653f\u6cbb\u6781\u5316\u52a0\u5267\u800c\u82f1\u56fd\u515a\u6d3e\u7acb\u573a\u76f8\u5bf9\u4e00\u81f4\uff0c\u4f46\u4fdd\u5b88\u515a\u4e0e\u5de5\u515a\u610f\u8bc6\u5f62\u6001\u5dee\u8ddd\u57282025\u5e74\u8fbe\u5230\u6700\u8d1f\u9762\u6c34\u5e73\u3002\u82f1\u56fd\u79fb\u6c11\u8bdd\u8bed\u8f6c\u5411\u5b89\u5168\u5316\u53d9\u4e8b\uff0c\u957f\u671f\u6574\u5408\u6846\u67b6\u51cf\u5c11\u3002", "motivation": "\u901a\u8fc7\u5927\u89c4\u6a21\u8ba1\u7b97\u5206\u6790\u6bd4\u8f83\u82f1\u7f8e\u8bae\u4f1a\u79fb\u6c11\u8bdd\u8bed\u7684\u957f\u671f\u6f14\u53d8\uff0c\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u5728\u653f\u6cbb\u5386\u53f2\u8bed\u5883\u4e2d\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8bdd\u8bed\u5206\u6790\u7684\u53ef\u884c\u6027\u3002", "method": "\u4f7f\u7528\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u82f1\u56fd\u8bae\u4f1a75\u5e74\u8fa9\u8bba\u548c\u7f8e\u56fd\u56fd\u4f1a\u8fa9\u8bba\u8fdb\u884c\u6807\u6ce8\uff0c\u8bc6\u522b\u5bf9\u79fb\u6c11\u7684\u9ad8\u7ea7\u7acb\u573a\u6001\u5ea6\uff1b\u5bf9\u82f1\u56fd\u6570\u636e\u91c7\u7528\u534a\u81ea\u52a8\u5316\u6846\u67b6\u63d0\u53d6\u7ec6\u7c92\u5ea6\u53d9\u4e8b\u6846\u67b6\u3002", "result": "\u7f8e\u56fd\u79fb\u6c11\u8bdd\u8bed\u65e5\u76ca\u6781\u5316\uff0c\u82f1\u56fd\u5404\u515a\u6d3e\u7acb\u573a\u76f8\u5bf9\u4e00\u81f4\u4f46\u4fdd\u5b88\u515a\u4e0e\u5de5\u515a\u610f\u8bc6\u5f62\u6001\u5dee\u8ddd\u57282025\u5e74\u8fbe\u5230\u6700\u8d1f\u9762\uff1b\u82f1\u56fd\u8bdd\u8bed\u8f6c\u5411\u8fb9\u5883\u7ba1\u63a7\u548c\u975e\u6cd5\u79fb\u6c11\u7b49\u5b89\u5168\u5316\u53d9\u4e8b\uff0c\u793e\u4f1a\u6574\u5408\u7b49\u957f\u671f\u6846\u67b6\u51cf\u5c11\uff1b\u79fb\u6c11\u8ba8\u8bba\u4ece\u56fd\u5185\u6cd5\u8f6c\u5411\u56fd\u9645\u6cd5\u548c\u4eba\u6743\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u652f\u6301\u653f\u6cbb\u5386\u53f2\u8bed\u5883\u4e2d\u53ef\u6269\u5c55\u7684\u7ec6\u7c92\u5ea6\u8bdd\u8bed\u5206\u6790\uff0c\u63ed\u793a\u4e86\u79fb\u6c11\u8bdd\u8bed\u7684\u957f\u671f\u6f14\u53d8\u8d8b\u52bf\u548c\u56fd\u522b\u5dee\u5f02\u3002"}}
{"id": "2509.13767", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13767", "abs": "https://arxiv.org/abs/2509.13767", "authors": ["Daiqi Liu", "Tom\u00e1s Arias-Vergara", "Johannes Enk", "Fangxu Xing", "Maureen Stone", "Jerry L. Prince", "Jana Hutter", "Andreas Maier", "Jonghye Woo", "Paula Andrea P\u00e9rez-Toro"], "title": "VocSegMRI: Multimodal Learning for Precise Vocal Tract Segmentation in Real-time MRI", "comment": "Preprint submitted to ICASSP", "summary": "Accurately segmenting articulatory structures in real-time magnetic resonance\nimaging (rtMRI) remains challenging, as most existing methods rely almost\nentirely on visual cues. Yet synchronized acoustic and phonological signals\nprovide complementary context that can enrich visual information and improve\nprecision. In this paper, we introduce VocSegMRI, a multimodal framework that\nintegrates video, audio, and phonological inputs through cross-attention fusion\nfor dynamic feature alignment. To further enhance cross-modal representation,\nwe incorporate a contrastive learning objective that improves segmentation\nperformance even when the audio modality is unavailable at inference. Evaluated\non a sub-set of USC-75 rtMRI dataset, our approach achieves state-of-the-art\nperformance, with a Dice score of 0.95 and a 95th percentile Hausdorff Distance\n(HD_95) of 4.20 mm, outperforming both unimodal and multimodal baselines.\nAblation studies confirm the contributions of cross-attention and contrastive\nlearning to segmentation precision and robustness. These results highlight the\nvalue of integrative multimodal modeling for accurate vocal tract analysis.", "AI": {"tldr": "VocSegMRI\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u878d\u5408\u89c6\u9891\u3001\u97f3\u9891\u548c\u8bed\u97f3\u5b66\u8f93\u5165\u6765\u63d0\u5347\u5b9e\u65f6MRI\u4e2d\u53d1\u97f3\u7ed3\u6784\u5206\u5272\u7684\u7cbe\u5ea6\uff0c\u5373\u4f7f\u63a8\u7406\u65f6\u6ca1\u6709\u97f3\u9891\u4e5f\u80fd\u4fdd\u6301\u826f\u597d\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u7ebf\u7d22\uff0c\u4f46\u540c\u6b65\u7684\u58f0\u5b66\u548c\u8bed\u97f3\u5b66\u4fe1\u53f7\u80fd\u63d0\u4f9b\u4e92\u8865\u4fe1\u606f\u6765\u4e30\u5bcc\u89c6\u89c9\u4fe1\u606f\u5e76\u63d0\u9ad8\u5206\u5272\u7cbe\u5ea6\u3002", "method": "\u5f15\u5165\u8de8\u6ce8\u610f\u529b\u878d\u5408\u673a\u5236\u6574\u5408\u89c6\u9891\u3001\u97f3\u9891\u548c\u8bed\u97f3\u5b66\u8f93\u5165\uff0c\u5e76\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u6765\u589e\u5f3a\u8de8\u6a21\u6001\u8868\u793a\uff0c\u5373\u4f7f\u5728\u63a8\u7406\u65f6\u7f3a\u5c11\u97f3\u9891\u6a21\u6001\u4e5f\u80fd\u4fdd\u6301\u826f\u597d\u6027\u80fd\u3002", "result": "\u5728USC-75 rtMRI\u6570\u636e\u96c6\u5b50\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1aDice\u5206\u65700.95\uff0c95% Hausdorff\u8ddd\u79bb4.20mm\uff0c\u4f18\u4e8e\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u57fa\u7ebf\u3002", "conclusion": "\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u8de8\u6ce8\u610f\u529b\u548c\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u5206\u5272\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u7684\u8d21\u732e\uff0c\u51f8\u663e\u4e86\u6574\u5408\u591a\u6a21\u6001\u5efa\u6a21\u5728\u51c6\u786e\u58f0\u9053\u5206\u6790\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2509.14233", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14233", "abs": "https://arxiv.org/abs/2509.14233", "authors": ["Alejandro Hern\u00e1ndez-Cano", "Alexander H\u00e4gele", "Allen Hao Huang", "Angelika Romanou", "Antoni-Joan Solergibert", "Barna Pasztor", "Bettina Messmer", "Dhia Garbaya", "Eduard Frank \u010eurech", "Ido Hakimi", "Juan Garc\u00eda Giraldo", "Mete Ismayilzada", "Negar Foroutan", "Skander Moalla", "Tiancheng Chen", "Vinko Sabol\u010dec", "Yixuan Xu", "Michael Aerni", "Badr AlKhamissi", "Ines Altemir Marinas", "Mohammad Hossein Amani", "Matin Ansaripour", "Ilia Badanin", "Harold Benoit", "Emanuela Boros", "Nicholas Browning", "Fabian B\u00f6sch", "Maximilian B\u00f6ther", "Niklas Canova", "Camille Challier", "Clement Charmillot", "Jonathan Coles", "Jan Deriu", "Arnout Devos", "Lukas Drescher", "Daniil Dzenhaliou", "Maud Ehrmann", "Dongyang Fan", "Simin Fan", "Silin Gao", "Miguel Gila", "Mar\u00eda Grandury", "Diba Hashemi", "Alexander Hoyle", "Jiaming Jiang", "Mark Klein", "Andrei Kucharavy", "Anastasiia Kucherenko", "Frederike L\u00fcbeck", "Roman Machacek", "Theofilos Manitaras", "Andreas Marfurt", "Kyle Matoba", "Simon Matrenok", "Henrique Mendonc\u00e7a", "Fawzi Roberto Mohamed", "Syrielle Montariol", "Luca Mouchel", "Sven Najem-Meyer", "Jingwei Ni", "Gennaro Oliva", "Matteo Pagliardini", "Elia Palme", "Andrei Panferov", "L\u00e9o Paoletti", "Marco Passerini", "Ivan Pavlov", "Auguste Poiroux", "Kaustubh Ponkshe", "Nathan Ranchin", "Javi Rando", "Mathieu Sauser", "Jakhongir Saydaliev", "Muhammad Ali Sayfiddinov", "Marian Schneider", "Stefano Schuppli", "Marco Scialanga", "Andrei Semenov", "Kumar Shridhar", "Raghav Singhal", "Anna Sotnikova", "Alexander Sternfeld", "Ayush Kumar Tarun", "Paul Teiletche", "Jannis Vamvas", "Xiaozhe Yao", "Hao Zhao Alexander Ilic", "Ana Klimovic", "Andreas Krause", "Caglar Gulcehre", "David Rosenthal", "Elliott Ash", "Florian Tram\u00e8r", "Joost VandeVondele", "Livio Veraldi", "Martin Rajman", "Thomas Schulthess", "Torsten Hoefler", "Antoine Bosselut", "Martin Jaggi", "Imanol Schlag"], "title": "Apertus: Democratizing Open and Compliant LLMs for Global Language Environments", "comment": null, "summary": "We present Apertus, a fully open suite of large language models (LLMs)\ndesigned to address two systemic shortcomings in today's open model ecosystem:\ndata compliance and multilingual representation. Unlike many prior models that\nrelease weights without reproducible data pipelines or regard for content-owner\nrights, Apertus models are pretrained exclusively on openly available data,\nretroactively respecting robots.txt exclusions and filtering for\nnon-permissive, toxic, and personally identifiable content. To mitigate risks\nof memorization, we adopt the Goldfish objective during pretraining, strongly\nsuppressing verbatim recall of data while retaining downstream task\nperformance. The Apertus models also expand multilingual coverage, training on\n15T tokens from over 1800 languages, with ~40% of pretraining data allocated to\nnon-English content. Released at 8B and 70B scales, Apertus approaches\nstate-of-the-art results among fully open models on multilingual benchmarks,\nrivalling or surpassing open-weight counterparts. Beyond model weights, we\nrelease all scientific artifacts from our development cycle with a permissive\nlicense, including data preparation scripts, checkpoints, evaluation suites,\nand training code, enabling transparent audit and extension.", "AI": {"tldr": "Apertus\u662f\u4e00\u4e2a\u5b8c\u5168\u5f00\u6e90\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5957\u4ef6\uff0c\u4e13\u6ce8\u4e8e\u6570\u636e\u5408\u89c4\u6027\u548c\u591a\u8bed\u8a00\u8868\u793a\uff0c\u4f7f\u7528\u516c\u5f00\u53ef\u7528\u6570\u636e\u8bad\u7ec3\uff0c\u5c0a\u91cdrobots.txt\u6392\u9664\u89c4\u5219\uff0c\u91c7\u7528Goldfish\u76ee\u6807\u6291\u5236\u6570\u636e\u8bb0\u5fc6\uff0c\u8986\u76d61800\u591a\u79cd\u8bed\u8a00\uff0c\u57288B\u548c70B\u89c4\u6a21\u4e0a\u8fbe\u5230\u63a5\u8fd1\u6700\u5148\u8fdb\u7684\u591a\u8bed\u8a00\u57fa\u51c6\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u5f00\u6e90\u6a21\u578b\u751f\u6001\u7cfb\u7edf\u4e2d\u4e24\u4e2a\u7cfb\u7edf\u6027\u7f3a\u9677\uff1a\u6570\u636e\u5408\u89c4\u6027\uff08\u8bb8\u591a\u6a21\u578b\u53d1\u5e03\u6743\u91cd\u4f46\u7f3a\u4e4f\u53ef\u590d\u73b0\u7684\u6570\u636e\u7ba1\u9053\u6216\u5ffd\u89c6\u5185\u5bb9\u6240\u6709\u8005\u6743\u5229\uff09\u548c\u591a\u8bed\u8a00\u8868\u793a\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u5b8c\u5168\u516c\u5f00\u53ef\u7528\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5c0a\u91cdrobots.txt\u6392\u9664\u89c4\u5219\uff0c\u8fc7\u6ee4\u975e\u8bb8\u53ef\u3001\u6709\u6bd2\u548c\u4e2a\u4eba\u8eab\u4efd\u4fe1\u606f\u5185\u5bb9\uff1b\u91c7\u7528Goldfish\u76ee\u6807\u6291\u5236\u9010\u5b57\u8bb0\u5fc6\uff1b\u572815T tokens\u4e0a\u8bad\u7ec3\uff0c\u8986\u76d61800\u591a\u79cd\u8bed\u8a00\uff0c\u5176\u4e2d\u7ea640%\u4e3a\u975e\u82f1\u8bed\u5185\u5bb9\u3002", "result": "Apertus\u6a21\u578b\u57288B\u548c70B\u89c4\u6a21\u4e0a\u63a5\u8fd1\u5b8c\u5168\u5f00\u6e90\u6a21\u578b\u7684\u6700\u5148\u8fdb\u591a\u8bed\u8a00\u57fa\u51c6\u7ed3\u679c\uff0c\u4e0e\u5f00\u6e90\u6743\u91cd\u5bf9\u5e94\u6a21\u578b\u76f8\u5f53\u6216\u8d85\u8d8a\u3002", "conclusion": "Apertus\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b8c\u5168\u900f\u660e\u7684\u5f00\u6e90LLM\u89e3\u51b3\u65b9\u6848\uff0c\u4e0d\u4ec5\u53d1\u5e03\u6a21\u578b\u6743\u91cd\uff0c\u8fd8\u5305\u62ec\u6570\u636e\u51c6\u5907\u811a\u672c\u3001\u68c0\u67e5\u70b9\u3001\u8bc4\u4f30\u5957\u4ef6\u548c\u8bad\u7ec3\u4ee3\u7801\u7b49\u6240\u6709\u79d1\u5b66\u6210\u679c\uff0c\u652f\u6301\u900f\u660e\u5ba1\u8ba1\u548c\u6269\u5c55\u3002"}}
{"id": "2509.13768", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13768", "abs": "https://arxiv.org/abs/2509.13768", "authors": ["Jianhui Chang"], "title": "Generative Image Coding with Diffusion Prior", "comment": null, "summary": "As generative technologies advance, visual content has evolved into a complex\nmix of natural and AI-generated images, driving the need for more efficient\ncoding techniques that prioritize perceptual quality. Traditional codecs and\nlearned methods struggle to maintain subjective quality at high compression\nratios, while existing generative approaches face challenges in visual fidelity\nand generalization. To this end, we propose a novel generative coding framework\nleveraging diffusion priors to enhance compression performance at low bitrates.\nOur approach employs a pre-optimized encoder to generate generalized\ncompressed-domain representations, integrated with the pretrained model's\ninternal features via a lightweight adapter and an attentive fusion module.\nThis framework effectively leverages existing pretrained diffusion models and\nenables efficient adaptation to different pretrained models for new\nrequirements with minimal retraining costs. We also introduce a distribution\nrenormalization method to further enhance reconstruction fidelity. Extensive\nexperiments show that our method (1) outperforms existing methods in visual\nfidelity across low bitrates, (2) improves compression performance by up to 79%\nover H.266/VVC, and (3) offers an efficient solution for AI-generated content\nwhile being adaptable to broader content types.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6269\u6563\u5148\u9a8c\u7684\u751f\u6210\u5f0f\u7f16\u7801\u6846\u67b6\uff0c\u5728\u4f4e\u7801\u7387\u4e0b\u901a\u8fc7\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u63d0\u5347\u538b\u7f29\u6027\u80fd\uff0c\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u538b\u7f29\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5", "motivation": "\u968f\u7740\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\uff0c\u89c6\u89c9\u5185\u5bb9\u6df7\u5408\u4e86\u81ea\u7136\u548cAI\u751f\u6210\u56fe\u50cf\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u7f16\u7801\u6280\u672f\u6765\u4fdd\u6301\u611f\u77e5\u8d28\u91cf\u3002\u4f20\u7edf\u7f16\u89e3\u7801\u5668\u548c\u5b66\u4e60\u65b9\u6cd5\u5728\u9ad8\u538b\u7f29\u6bd4\u4e0b\u96be\u4ee5\u7ef4\u6301\u4e3b\u89c2\u8d28\u91cf\uff0c\u73b0\u6709\u751f\u6210\u65b9\u6cd5\u9762\u4e34\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u6cdb\u5316\u6027\u6311\u6218", "method": "\u4f7f\u7528\u9884\u4f18\u5316\u7f16\u7801\u5668\u751f\u6210\u5e7f\u4e49\u538b\u7f29\u57df\u8868\u793a\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u548c\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u5185\u90e8\u7279\u5f81\u96c6\u6210\u3002\u5f15\u5165\u5206\u5e03\u91cd\u5f52\u4e00\u5316\u65b9\u6cd5\u589e\u5f3a\u91cd\u5efa\u4fdd\u771f\u5ea6\uff0c\u53ef\u9ad8\u6548\u9002\u914d\u4e0d\u540c\u9884\u8bad\u7ec3\u6a21\u578b", "result": "\u65b9\u6cd5\u5728\u4f4e\u7801\u7387\u4e0b\u89c6\u89c9\u4fdd\u771f\u5ea6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u538b\u7f29\u6027\u80fd\u6bd4H.266/VVC\u63d0\u5347\u9ad8\u8fbe79%\uff0c\u4e3aAI\u751f\u6210\u5185\u5bb9\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u4e14\u53ef\u9002\u914d\u66f4\u5e7f\u6cdb\u5185\u5bb9\u7c7b\u578b", "conclusion": "\u63d0\u51fa\u7684\u751f\u6210\u5f0f\u7f16\u7801\u6846\u67b6\u6709\u6548\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u5728\u4f4e\u7801\u7387\u538b\u7f29\u4e2d\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u538b\u7f29\u6548\u7387\uff0c\u5177\u6709\u5f88\u597d\u7684\u9002\u5e94\u6027\u548c\u5b9e\u7528\u6027"}}
{"id": "2509.13769", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13769", "abs": "https://arxiv.org/abs/2509.13769", "authors": ["Yuechen Luo", "Fang Li", "Shaoqing Xu", "Zhiyi Lai", "Lei Yang", "Qimao Chen", "Ziang Luo", "Zixun Xie", "Shengyin Jiang", "Jiaxin Liu", "Long Chen", "Bing Wang", "Zhi-xin Yang"], "title": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving", "comment": null, "summary": "While reasoning technology like Chain of Thought (CoT) has been widely\nadopted in Vision Language Action (VLA) models, it demonstrates promising\ncapabilities in end to end autonomous driving. However, recent efforts to\nintegrate CoT reasoning often fall short in simple scenarios, introducing\nunnecessary computational overhead without improving decision quality. To\naddress this, we propose AdaThinkDrive, a novel VLA framework with a dual mode\nreasoning mechanism inspired by fast and slow thinking. First, our framework is\npretrained on large scale autonomous driving (AD) scenarios using both question\nanswering (QA) and trajectory datasets to acquire world knowledge and driving\ncommonsense. During supervised fine tuning (SFT), we introduce a two mode\ndataset, fast answering (w/o CoT) and slow thinking (with CoT), enabling the\nmodel to distinguish between scenarios that require reasoning. Furthermore, an\nAdaptive Think Reward strategy is proposed in conjunction with the Group\nRelative Policy Optimization (GRPO), which rewards the model for selectively\napplying CoT by comparing trajectory quality across different reasoning modes.\nExtensive experiments on the Navsim benchmark show that AdaThinkDrive achieves\na PDMS of 90.3, surpassing the best vision only baseline by 1.7 points.\nMoreover, ablations show that AdaThinkDrive surpasses both the never Think and\nalways Think baselines, improving PDMS by 2.0 and 1.4, respectively. It also\nreduces inference time by 14% compared to the always Think baseline,\ndemonstrating its ability to balance accuracy and efficiency through adaptive\nreasoning.", "AI": {"tldr": "AdaThinkDrive\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u6a21\u5f0f\u63a8\u7406\u673a\u5236\uff08\u5feb\u901f\u56de\u7b54\u548c\u6162\u901f\u601d\u8003\uff09\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9009\u62e9\u662f\u5426\u4f7f\u7528\u601d\u7ef4\u94fe\u63a8\u7406\u6765\u5e73\u8861\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u6280\u672f\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5f80\u5f80\u5728\u7b80\u5355\u573a\u666f\u4e0b\u5f15\u5165\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u800c\u65e0\u6cd5\u63d0\u5347\u51b3\u7b56\u8d28\u91cf\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u667a\u80fd\u9009\u62e9\u4f55\u65f6\u4f7f\u7528\u63a8\u7406\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u53cc\u6a21\u5f0f\u6570\u636e\u96c6\u8bad\u7ec3\uff08\u5feb\u901f\u56de\u7b54\u548c\u6162\u901f\u601d\u8003\uff09\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u601d\u7ef4\u5956\u52b1\u7b56\u7565\u548c\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff0c\u8ba9\u6a21\u578b\u5b66\u4f1a\u533a\u5206\u573a\u666f\u5e76\u9009\u62e9\u6027\u5e94\u7528\u601d\u7ef4\u94fe\u63a8\u7406\u3002", "result": "\u5728Navsim\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523090.3\u7684PDMS\u5206\u6570\uff0c\u6bd4\u6700\u4f73\u89c6\u89c9\u57fa\u7ebf\u63d0\u53471.7\u5206\uff0c\u6bd4\u59cb\u7ec8\u4f7f\u7528\u63a8\u7406\u7684\u57fa\u7ebf\u51cf\u5c1114%\u63a8\u7406\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "AdaThinkDrive\u901a\u8fc7\u81ea\u9002\u5e94\u63a8\u7406\u673a\u5236\u6210\u529f\u5e73\u8861\u4e86\u81ea\u52a8\u9a7e\u9a76\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u8bc1\u660e\u4e86\u9009\u62e9\u6027\u4f7f\u7528\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.13836", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13836", "abs": "https://arxiv.org/abs/2509.13836", "authors": ["Weihang Wang", "Xinhao Li", "Ziyue Wang", "Yan Pang", "Jielei Zhang", "Peiyi Li", "Qiang Zhang", "Longwen Gao"], "title": "Diving into Mitigating Hallucinations from a Vision Perspective for Large Vision-Language Models", "comment": "Accepted by EMNLP2025 Finding", "summary": "Object hallucination in Large Vision-Language Models (LVLMs) significantly\nimpedes their real-world applicability. As the primary component for accurately\ninterpreting visual information, the choice of visual encoder is pivotal. We\nhypothesize that the diverse training paradigms employed by different visual\nencoders instill them with distinct inductive biases, which leads to their\ndiverse hallucination performances. Existing benchmarks typically focus on\ncoarse-grained hallucination detection and fail to capture the diverse\nhallucinations elaborated in our hypothesis. To systematically analyze these\neffects, we introduce VHBench-10, a comprehensive benchmark with approximately\n10,000 samples for evaluating LVLMs across ten fine-grained hallucination\ncategories. Our evaluations confirm encoders exhibit unique hallucination\ncharacteristics. Building on these insights and the suboptimality of simple\nfeature fusion, we propose VisionWeaver, a novel Context-Aware Routing Network.\nIt employs global visual features to generate routing signals, dynamically\naggregating visual features from multiple specialized experts. Comprehensive\nexperiments confirm the effectiveness of VisionWeaver in significantly reducing\nhallucinations and improving overall model performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86VHBench-10\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u4e0d\u540c\u89c6\u89c9\u7f16\u7801\u5668\u5728LVLMs\u4e2d\u7684\u5e7b\u89c9\u8868\u73b0\uff0c\u5e76\u5f00\u53d1\u4e86VisionWeaver\u6846\u67b6\u6765\u52a8\u6001\u805a\u5408\u591a\u4e2a\u4e13\u5bb6\u7279\u5f81\u4ee5\u51cf\u5c11\u5e7b\u89c9", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\u4e25\u91cd\u963b\u788d\u4e86\u5b9e\u9645\u5e94\u7528\uff0c\u4e0d\u540c\u89c6\u89c9\u7f16\u7801\u5668\u7684\u8bad\u7ec3\u8303\u5f0f\u5dee\u5f02\u5bfc\u81f4\u5176\u5177\u6709\u4e0d\u540c\u7684\u5f52\u7eb3\u504f\u5dee\u548c\u5e7b\u89c9\u8868\u73b0\uff0c\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u6355\u6349\u8fd9\u79cd\u7ec6\u7c92\u5ea6\u7684\u5dee\u5f02", "method": "\u5f15\u5165VHBench-10\u57fa\u51c6\u6d4b\u8bd5\uff08\u7ea610,000\u4e2a\u6837\u672c\uff0c10\u4e2a\u7ec6\u7c92\u5ea6\u5e7b\u89c9\u7c7b\u522b\uff09\uff0c\u5e76\u63d0\u51faVisionWeaver\u6846\u67b6\u2014\u2014\u57fa\u4e8e\u5168\u5c40\u89c6\u89c9\u7279\u5f81\u751f\u6210\u8def\u7531\u4fe1\u53f7\uff0c\u52a8\u6001\u805a\u5408\u591a\u4e2a\u4e13\u95e8\u4e13\u5bb6\u7684\u89c6\u89c9\u7279\u5f81", "result": "\u8bc4\u4f30\u786e\u8ba4\u4e0d\u540c\u7f16\u7801\u5668\u5177\u6709\u72ec\u7279\u7684\u5e7b\u89c9\u7279\u5f81\uff0cVisionWeaver\u5728\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\u548c\u63d0\u9ad8\u6574\u4f53\u6a21\u578b\u6027\u80fd\u65b9\u9762\u8868\u73b0\u51fa\u6709\u6548\u6027", "conclusion": "\u89c6\u89c9\u7f16\u7801\u5668\u7684\u9009\u62e9\u5bf9LVLMs\u7684\u5e7b\u89c9\u8868\u73b0\u81f3\u5173\u91cd\u8981\uff0cVisionWeaver\u901a\u8fc7\u52a8\u6001\u7279\u5f81\u805a\u5408\u6709\u6548\u7f13\u89e3\u4e86\u7269\u4f53\u5e7b\u89c9\u95ee\u9898"}}
{"id": "2509.13776", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13776", "abs": "https://arxiv.org/abs/2509.13776", "authors": ["Chao Shuai", "Gaojian Wang", "Kun Pan", "Tong Wu", "Fanli Jin", "Haohan Tan", "Mengxiang Li", "Zhenguang Liu", "Feng Lin", "Kui Ren"], "title": "Morphology-optimized Multi-Scale Fusion: Combining Local Artifacts and Mesoscopic Semantics for Deepfake Detection and Localization", "comment": "The 3rd Place, IJCAI 2025 Workshop on Deepfake Detection,\n  Localization, and Interpretability", "summary": "While the pursuit of higher accuracy in deepfake detection remains a central\ngoal, there is an increasing demand for precise localization of manipulated\nregions. Despite the remarkable progress made in classification-based\ndetection, accurately localizing forged areas remains a significant challenge.\nA common strategy is to incorporate forged region annotations during model\ntraining alongside manipulated images. However, such approaches often neglect\nthe complementary nature of local detail and global semantic context, resulting\nin suboptimal localization performance. Moreover, an often-overlooked aspect is\nthe fusion strategy between local and global predictions. Naively combining the\noutputs from both branches can amplify noise and errors, thereby undermining\nthe effectiveness of the localization.\n  To address these issues, we propose a novel approach that independently\npredicts manipulated regions using both local and global perspectives. We\nemploy morphological operations to fuse the outputs, effectively suppressing\nnoise while enhancing spatial coherence. Extensive experiments reveal the\neffectiveness of each module in improving the accuracy and robustness of\nforgery localization.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u72ec\u7acb\u4f7f\u7528\u5c40\u90e8\u548c\u5168\u5c40\u89c6\u89d2\u9884\u6d4b\u7be1\u6539\u533a\u57df\uff0c\u5e76\u91c7\u7528\u5f62\u6001\u5b66\u64cd\u4f5c\u878d\u5408\u8f93\u51fa\uff0c\u6709\u6548\u6291\u5236\u566a\u58f0\u5e76\u589e\u5f3a\u7a7a\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u867d\u7136\u8ffd\u6c42\u66f4\u9ad8\u51c6\u786e\u7387\uff0c\u4f46\u7cbe\u786e\u5b9a\u4f4d\u7be1\u6539\u533a\u57df\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u5c40\u90e8\u7ec6\u8282\u548c\u5168\u5c40\u8bed\u4e49\u4e0a\u4e0b\u6587\u7684\u4e92\u8865\u6027\uff0c\u4e14\u878d\u5408\u7b56\u7565\u7b80\u5355\uff0c\u5bfc\u81f4\u5b9a\u4f4d\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u72ec\u7acb\u4f7f\u7528\u5c40\u90e8\u548c\u5168\u5c40\u89c6\u89d2\u9884\u6d4b\u7be1\u6539\u533a\u57df\uff0c\u91c7\u7528\u5f62\u6001\u5b66\u64cd\u4f5c\u878d\u5408\u4e24\u4e2a\u5206\u652f\u7684\u8f93\u51fa\uff0c\u6709\u6548\u6291\u5236\u566a\u58f0\u5e76\u589e\u5f3a\u7a7a\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u6bcf\u4e2a\u6a21\u5757\u5728\u63d0\u9ad8\u4f2a\u9020\u5b9a\u4f4d\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u521b\u65b0\u7684\u5c40\u90e8-\u5168\u5c40\u72ec\u7acb\u9884\u6d4b\u548c\u5f62\u6001\u5b66\u878d\u5408\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u4f2a\u9020\u533a\u57df\u5b9a\u4f4d\u7684\u6027\u80fd\u3002"}}
{"id": "2509.14199", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14199", "abs": "https://arxiv.org/abs/2509.14199", "authors": ["Haichao Zhang", "Wenhao Chai", "Shwai He", "Ang Li", "Yun Fu"], "title": "Dense Video Understanding with Gated Residual Tokenization", "comment": null, "summary": "High temporal resolution is essential for capturing fine-grained details in\nvideo understanding. However, current video large language models (VLLMs) and\nbenchmarks mostly rely on low-frame-rate sampling, such as uniform sampling or\nkeyframe selection, discarding dense temporal information. This compromise\navoids the high cost of tokenizing every frame, which otherwise leads to\nredundant computation and linear token growth as video length increases. While\nthis trade-off works for slowly changing content, it fails for tasks like\nlecture comprehension, where information appears in nearly every frame and\nrequires precise temporal alignment. To address this gap, we introduce Dense\nVideo Understanding (DVU), which enables high-FPS video comprehension by\nreducing both tokenization time and token overhead. Existing benchmarks are\nalso limited, as their QA pairs focus on coarse content changes. We therefore\npropose DIVE (Dense Information Video Evaluation), the first benchmark designed\nfor dense temporal reasoning. To make DVU practical, we present Gated Residual\nTokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-Gated\nTokenization uses pixel-level motion estimation to skip static regions during\ntokenization, achieving sub-linear growth in token count and compute. (2)\nSemantic-Scene Intra-Tokenization Merging fuses tokens across static regions\nwithin a scene, further reducing redundancy while preserving dynamic semantics.\nExperiments on DIVE show that GRT outperforms larger VLLM baselines and scales\npositively with FPS. These results highlight the importance of dense temporal\ninformation and demonstrate that GRT enables efficient, scalable high-FPS video\nunderstanding.", "AI": {"tldr": "\u63d0\u51fa\u4e86Dense Video Understanding (DVU)\u6846\u67b6\u548cGated Residual Tokenization (GRT)\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fd0\u52a8\u8865\u507f\u548c\u8bed\u4e49\u878d\u5408\u6280\u672f\u5b9e\u73b0\u9ad8\u5e27\u7387\u89c6\u9891\u7406\u89e3\uff0c\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u548ctoken\u6570\u91cf\u589e\u957f\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u4f4e\u5e27\u7387\u91c7\u6837\uff0c\u4e22\u5f03\u4e86\u5bc6\u96c6\u65f6\u95f4\u4fe1\u606f\uff0c\u65e0\u6cd5\u5904\u7406\u9700\u8981\u7cbe\u786e\u65f6\u95f4\u5bf9\u9f50\u7684\u4efb\u52a1\uff08\u5982\u8bb2\u5ea7\u7406\u89e3\uff09\u3002\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e5f\u5c40\u9650\u4e8e\u7c97\u7c92\u5ea6\u5185\u5bb9\u53d8\u5316\u3002", "method": "\u63d0\u51faGRT\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u8fd0\u52a8\u8865\u507f\u95e8\u63a7tokenization\u5229\u7528\u50cf\u7d20\u7ea7\u8fd0\u52a8\u4f30\u8ba1\u8df3\u8fc7\u9759\u6001\u533a\u57df\uff1b2) \u8bed\u4e49\u573a\u666f\u5185tokenization\u5408\u5e76\u878d\u5408\u9759\u6001\u533a\u57dftoken\uff0c\u5728\u4fdd\u7559\u52a8\u6001\u8bed\u4e49\u7684\u540c\u65f6\u51cf\u5c11\u5197\u4f59\u3002", "result": "\u5728DIVE\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cGRT\u4f18\u4e8e\u66f4\u5927\u7684VLLM\u57fa\u7ebf\u6a21\u578b\uff0c\u4e14\u6027\u80fd\u968f\u5e27\u7387\u589e\u52a0\u800c\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5bc6\u96c6\u65f6\u95f4\u4fe1\u606f\u7684\u91cd\u8981\u6027\u3002", "conclusion": "GRT\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u9ad8\u5e27\u7387\u89c6\u9891\u7406\u89e3\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u89c6\u9891\u7406\u89e3\u4e2d\u65f6\u95f4\u5206\u8fa8\u7387\u4e0d\u8db3\u7684\u95ee\u9898\u3002"}}
{"id": "2509.13784", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13784", "abs": "https://arxiv.org/abs/2509.13784", "authors": ["Hanfang Liang", "Bing Wang", "Shizhen Zhang", "Wen Jiang", "Yizhuo Yang", "Weixiang Guo", "Shenghai Yuan"], "title": "CETUS: Causal Event-Driven Temporal Modeling With Unified Variable-Rate Scheduling", "comment": "8 pages, 6 figures", "summary": "Event cameras capture asynchronous pixel-level brightness changes with\nmicrosecond temporal resolution, offering unique advantages for high-speed\nvision tasks. Existing methods often convert event streams into intermediate\nrepresentations such as frames, voxel grids, or point clouds, which inevitably\nrequire predefined time windows and thus introduce window latency. Meanwhile,\npointwise detection methods face computational challenges that prevent\nreal-time efficiency due to their high computational cost. To overcome these\nlimitations, we propose the Variable-Rate Spatial Event Mamba, a novel\narchitecture that directly processes raw event streams without intermediate\nrepresentations. Our method introduces a lightweight causal spatial\nneighborhood encoder to efficiently capture local geometric relations, followed\nby Mamba-based state space models for scalable temporal modeling with linear\ncomplexity. During inference, a controller adaptively adjusts the processing\nspeed according to the event rate, achieving an optimal balance between window\nlatency and inference latency.", "AI": {"tldr": "\u63d0\u51faVariable-Rate Spatial Event Mamba\u67b6\u6784\uff0c\u76f4\u63a5\u5904\u7406\u539f\u59cb\u4e8b\u4ef6\u6d41\uff0c\u65e0\u9700\u4e2d\u95f4\u8868\u793a\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u901f\u7387\u63a7\u5236\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u9ad8\u6548\u5904\u7406", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5c06\u4e8b\u4ef6\u6d41\u8f6c\u6362\u4e3a\u5e27\u3001\u4f53\u7d20\u7f51\u683c\u6216\u70b9\u4e91\u7b49\u4e2d\u95f4\u8868\u793a\uff0c\u8fd9\u5f15\u5165\u4e86\u7a97\u53e3\u5ef6\u8fdf\uff1b\u800c\u9010\u70b9\u68c0\u6d4b\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u6548\u7387", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u56e0\u679c\u7a7a\u95f4\u90bb\u57df\u7f16\u7801\u5668\u6355\u83b7\u5c40\u90e8\u51e0\u4f55\u5173\u7cfb\uff0c\u7136\u540e\u91c7\u7528\u57fa\u4e8eMamba\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u8fdb\u884c\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u53ef\u6269\u5c55\u65f6\u5e8f\u5efa\u6a21\uff0c\u63a8\u7406\u65f6\u63a7\u5236\u5668\u6839\u636e\u4e8b\u4ef6\u901f\u7387\u81ea\u9002\u5e94\u8c03\u6574\u5904\u7406\u901f\u5ea6", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u76f4\u63a5\u5904\u7406\u539f\u59cb\u4e8b\u4ef6\u6d41\uff0c\u907f\u514d\u4e86\u4e2d\u95f4\u8868\u793a\u5e26\u6765\u7684\u7a97\u53e3\u5ef6\u8fdf\uff0c\u540c\u65f6\u901a\u8fc7\u81ea\u9002\u5e94\u901f\u7387\u63a7\u5236\u5b9e\u73b0\u4e86\u7a97\u53e3\u5ef6\u8fdf\u548c\u63a8\u7406\u5ef6\u8fdf\u4e4b\u95f4\u7684\u6700\u4f18\u5e73\u8861", "conclusion": "\u63d0\u51fa\u7684Variable-Rate Spatial Event Mamba\u67b6\u6784\u4e3a\u9ad8\u901f\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u5ef6\u8fdf\u7684\u4e8b\u4ef6\u6d41\u5904\u7406\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027"}}
{"id": "2509.13789", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13789", "abs": "https://arxiv.org/abs/2509.13789", "authors": ["Hanshuai Cui", "Zhiqing Tang", "Zhifei Xu", "Zhi Yao", "Wenyi Zeng", "Weijia Jia"], "title": "BWCache: Accelerating Video Diffusion Transformers through Block-Wise Caching", "comment": null, "summary": "Recent advancements in Diffusion Transformers (DiTs) have established them as\nthe state-of-the-art method for video generation. However, their inherently\nsequential denoising process results in inevitable latency, limiting real-world\napplicability. Existing acceleration methods either compromise visual quality\ndue to architectural modifications or fail to reuse intermediate features at\nproper granularity. Our analysis reveals that DiT blocks are the primary\ncontributors to inference latency. Across diffusion timesteps, the feature\nvariations of DiT blocks exhibit a U-shaped pattern with high similarity during\nintermediate timesteps, which suggests substantial computational redundancy. In\nthis paper, we propose Block-Wise Caching (BWCache), a training-free method to\naccelerate DiT-based video generation. BWCache dynamically caches and reuses\nfeatures from DiT blocks across diffusion timesteps. Furthermore, we introduce\na similarity indicator that triggers feature reuse only when the differences\nbetween block features at adjacent timesteps fall below a threshold, thereby\nminimizing redundant computations while maintaining visual fidelity. Extensive\nexperiments on several video diffusion models demonstrate that BWCache achieves\nup to 2.24$\\times$ speedup with comparable visual quality.", "AI": {"tldr": "BWCache\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u52a0\u901f\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u7f13\u5b58\u548c\u91cd\u7528DiT\u5757\u7279\u5f81\u6765\u51cf\u5c11\u89c6\u9891\u751f\u6210\u4e2d\u7684\u5197\u4f59\u8ba1\u7b97\uff0c\u5b9e\u73b02.24\u500d\u52a0\u901f\u4e14\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf", "motivation": "\u73b0\u6709\u7684DiT\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u4e0d\u53ef\u907f\u514d\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u8981\u4e48\u727a\u7272\u89c6\u89c9\u8d28\u91cf\uff0c\u8981\u4e48\u65e0\u6cd5\u5728\u9002\u5f53\u7c92\u5ea6\u4e0a\u91cd\u7528\u4e2d\u95f4\u7279\u5f81", "method": "\u63d0\u51faBlock-Wise Caching (BWCache)\u65b9\u6cd5\uff0c\u52a8\u6001\u7f13\u5b58\u548c\u91cd\u7528DiT\u5757\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u76f8\u4f3c\u6027\u6307\u793a\u5668\u5728\u76f8\u90bb\u65f6\u95f4\u6b65\u7279\u5f81\u5dee\u5f02\u4f4e\u4e8e\u9608\u503c\u65f6\u89e6\u53d1\u7279\u5f81\u91cd\u7528", "result": "\u5728\u591a\u4e2a\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBWCache\u5b9e\u73b0\u4e86\u6700\u9ad82.24\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u7684\u89c6\u89c9\u8d28\u91cf", "conclusion": "BWCache\u901a\u8fc7\u6709\u6548\u5229\u7528DiT\u5757\u7279\u5f81\u7684\u65f6\u95f4\u76f8\u4f3c\u6027\uff0c\u6210\u529f\u89e3\u51b3\u4e86DiT\u89c6\u9891\u751f\u6210\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u4e3a\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.13792", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13792", "abs": "https://arxiv.org/abs/2509.13792", "authors": ["Inder Pal Singh", "Nidhal Eddine Chenni", "Abd El Rahman Shabayek", "Arunkumar Rathinam", "Djamila Aouada"], "title": "Bridging the Synthetic-Real Gap: Supervised Domain Adaptation for Robust Spacecraft 6-DoF Pose Estimation", "comment": null, "summary": "Spacecraft Pose Estimation (SPE) is a fundamental capability for autonomous\nspace operations such as rendezvous, docking, and in-orbit servicing. Hybrid\npipelines that combine object detection, keypoint regression, and\nPerspective-n-Point (PnP) solvers have recently achieved strong results on\nsynthetic datasets, yet their performance deteriorates sharply on real or\nlab-generated imagery due to the persistent synthetic-to-real domain gap.\nExisting unsupervised domain adaptation approaches aim to mitigate this issue\nbut often underperform when a modest number of labeled target samples are\navailable. In this work, we propose the first Supervised Domain Adaptation\n(SDA) framework tailored for SPE keypoint regression. Building on the Learning\nInvariant Representation and Risk (LIRR) paradigm, our method jointly optimizes\ndomain-invariant representations and task-specific risk using both labeled\nsynthetic and limited labeled real data, thereby reducing generalization error\nunder domain shift. Extensive experiments on the SPEED+ benchmark demonstrate\nthat our approach consistently outperforms source-only, fine-tuning, and oracle\nbaselines. Notably, with only 5% labeled target data, our method matches or\nsurpasses oracle performance trained on larger fractions of labeled data. The\nframework is lightweight, backbone-agnostic, and computationally efficient,\noffering a practical pathway toward robust and deployable spacecraft pose\nestimation in real-world space environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u822a\u5929\u5668\u59ff\u6001\u4f30\u8ba1\u5173\u952e\u70b9\u56de\u5f52\u7684\u76d1\u7763\u57df\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u57df\u4e0d\u53d8\u8868\u793a\u548c\u4efb\u52a1\u7279\u5b9a\u98ce\u9669\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5408\u6210\u6570\u636e\u5230\u771f\u5b9e\u6570\u636e\u7684\u57df\u9002\u5e94\u95ee\u9898\u3002", "motivation": "\u822a\u5929\u5668\u59ff\u6001\u4f30\u8ba1\u5728\u81ea\u4e3b\u7a7a\u95f4\u64cd\u4f5c\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684\u6df7\u5408\u65b9\u6cd5\u5728\u5408\u6210\u6570\u636e\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u5728\u771f\u5b9e\u56fe\u50cf\u4e0a\u6027\u80fd\u6025\u5267\u4e0b\u964d\u3002\u73b0\u6709\u7684\u65e0\u76d1\u7763\u57df\u9002\u5e94\u65b9\u6cd5\u5728\u6709\u5c11\u91cf\u6807\u8bb0\u76ee\u6807\u6837\u672c\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u57fa\u4e8e\u5b66\u4e60\u4e0d\u53d8\u8868\u793a\u548c\u98ce\u9669(LIRR)\u8303\u5f0f\uff0c\u8054\u5408\u4f7f\u7528\u6807\u8bb0\u7684\u5408\u6210\u6570\u636e\u548c\u6709\u9650\u7684\u6807\u8bb0\u771f\u5b9e\u6570\u636e\uff0c\u540c\u65f6\u4f18\u5316\u57df\u4e0d\u53d8\u8868\u793a\u548c\u4efb\u52a1\u7279\u5b9a\u98ce\u9669\u3002", "result": "\u5728SPEED+\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u4ec5\u6e90\u57df\u8bad\u7ec3\u3001\u5fae\u8c03\u548coracle\u57fa\u7ebf\u3002\u4ec5\u4f7f\u75285%\u6807\u8bb0\u76ee\u6807\u6570\u636e\u5c31\u80fd\u8fbe\u5230\u6216\u8d85\u8fc7\u4f7f\u7528\u66f4\u591a\u6807\u8bb0\u6570\u636e\u7684oracle\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u8f7b\u91cf\u7ea7\u3001\u4e3b\u5e72\u7f51\u7edc\u65e0\u5173\u4e14\u8ba1\u7b97\u9ad8\u6548\uff0c\u4e3a\u5728\u771f\u5b9e\u7a7a\u95f4\u73af\u5883\u4e2d\u5b9e\u73b0\u7a33\u5065\u53ef\u90e8\u7f72\u7684\u822a\u5929\u5668\u59ff\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u9014\u5f84\u3002"}}
{"id": "2509.13795", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13795", "abs": "https://arxiv.org/abs/2509.13795", "authors": ["Jiayu Yuan", "Ming Dai", "Enhui Zheng", "Chao Su", "Nanxing Chen", "Qiming Hu", "Shibo Zhu", "Yibin Cao"], "title": "SWA-PF: Semantic-Weighted Adaptive Particle Filter for Memory-Efficient 4-DoF UAV Localization in GNSS-Denied Environments", "comment": null, "summary": "Vision-based Unmanned Aerial Vehicle (UAV) localization systems have been\nextensively investigated for Global Navigation Satellite System (GNSS)-denied\nenvironments. However, existing retrieval-based approaches face limitations in\ndataset availability and persistent challenges including suboptimal real-time\nperformance, environmental sensitivity, and limited generalization capability,\nparticularly in dynamic or temporally varying environments. To overcome these\nlimitations, we present a large-scale Multi-Altitude Flight Segments dataset\n(MAFS) for variable altitude scenarios and propose a novel Semantic-Weighted\nAdaptive Particle Filter (SWA-PF) method. This approach integrates robust\nsemantic features from both UAV-captured images and satellite imagery through\ntwo key innovations: a semantic weighting mechanism and an optimized particle\nfiltering architecture. Evaluated using our dataset, the proposed method\nachieves 10x computational efficiency gain over feature extraction methods,\nmaintains global positioning errors below 10 meters, and enables rapid 4 degree\nof freedom (4-DoF) pose estimation within seconds using accessible\nlow-resolution satellite maps. Code and dataset will be available at\nhttps://github.com/YuanJiayuuu/SWA-PF.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u52a0\u6743\u81ea\u9002\u5e94\u7c92\u5b50\u6ee4\u6ce2\u7684\u65e0\u4eba\u673a\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u4f7f\u7528\u5927\u89c4\u6a21\u591a\u9ad8\u5ea6\u98de\u884c\u6570\u636e\u96c6\uff0c\u5728GNSS\u62d2\u6b62\u73af\u5883\u4e0b\u5b9e\u73b0\u5feb\u901f4\u81ea\u7531\u5ea6\u59ff\u6001\u4f30\u8ba1\uff0c\u5b9a\u4f4d\u8bef\u5dee\u4f4e\u4e8e10\u7c73\uff0c\u8ba1\u7b97\u6548\u7387\u63d0\u534710\u500d\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u68c0\u7d22\u7684\u65e0\u4eba\u673a\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u5728\u6570\u636e\u96c6\u53ef\u7528\u6027\u3001\u5b9e\u65f6\u6027\u80fd\u3001\u73af\u5883\u654f\u611f\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u6216\u65f6\u53d8\u73af\u5883\u4e2d\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u52a0\u6743\u81ea\u9002\u5e94\u7c92\u5b50\u6ee4\u6ce2(SWA-PF)\u65b9\u6cd5\uff0c\u96c6\u6210\u65e0\u4eba\u673a\u56fe\u50cf\u548c\u536b\u661f\u56fe\u50cf\u7684\u9c81\u68d2\u8bed\u4e49\u7279\u5f81\uff0c\u5305\u542b\u8bed\u4e49\u52a0\u6743\u673a\u5236\u548c\u4f18\u5316\u7684\u7c92\u5b50\u6ee4\u6ce2\u67b6\u6784\u3002", "result": "\u5728\u63d0\u51fa\u7684\u591a\u9ad8\u5ea6\u98de\u884c\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u65b9\u6cd5\u5b9e\u73b010\u500d\u8ba1\u7b97\u6548\u7387\u63d0\u5347\uff0c\u5168\u5c40\u5b9a\u4f4d\u8bef\u5dee\u4f4e\u4e8e10\u7c73\uff0c\u80fd\u591f\u5728\u51e0\u79d2\u5185\u4f7f\u7528\u4f4e\u5206\u8fa8\u7387\u536b\u661f\u5730\u56fe\u5b8c\u62104\u81ea\u7531\u5ea6\u59ff\u6001\u4f30\u8ba1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u5728GNSS\u62d2\u6b62\u73af\u5883\u4e0b\u7684\u5b9a\u4f4d\u6311\u6218\uff0c\u5177\u6709\u4f18\u5f02\u7684\u5b9e\u65f6\u6027\u80fd\u548c\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u5f00\u6e90\u63d0\u4f9b\u3002"}}
{"id": "2509.13801", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13801", "abs": "https://arxiv.org/abs/2509.13801", "authors": ["Wenlve Zhou", "Zhiheng Zhou", "Tiantao Xian", "Yikui Zhai", "Weibin Wu", "Biyun Ma"], "title": "Masked Feature Modeling Enhances Adaptive Segmentation", "comment": null, "summary": "Unsupervised domain adaptation (UDA) for semantic segmentation aims to\ntransfer models from a labeled source domain to an unlabeled target domain.\nWhile auxiliary self-supervised tasks-particularly contrastive learning-have\nimproved feature discriminability, masked modeling approaches remain\nunderexplored in this setting, largely due to architectural incompatibility and\nmisaligned optimization objectives. We propose Masked Feature Modeling (MFM), a\nnovel auxiliary task that performs feature masking and reconstruction directly\nin the feature space. Unlike existing masked modeling methods that reconstruct\nlow-level inputs or perceptual features (e.g., HOG or visual tokens), MFM\naligns its learning target with the main segmentation task, ensuring\ncompatibility with standard architectures like DeepLab and DAFormer without\nmodifying the inference pipeline. To facilitate effective reconstruction, we\nintroduce a lightweight auxiliary module, Rebuilder, which is trained jointly\nbut discarded during inference, adding zero computational overhead at test\ntime. Crucially, MFM leverages the segmentation decoder to classify the\nreconstructed features, tightly coupling the auxiliary objective with the\npixel-wise prediction task to avoid interference with the primary task.\nExtensive experiments across various architectures and UDA benchmarks\ndemonstrate that MFM consistently enhances segmentation performance, offering a\nsimple, efficient, and generalizable strategy for unsupervised domain-adaptive\nsemantic segmentation.", "AI": {"tldr": "\u63d0\u51faMasked Feature Modeling (MFM)\u4f5c\u4e3a\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u8bed\u4e49\u5206\u5272\u7684\u8f85\u52a9\u4efb\u52a1\uff0c\u901a\u8fc7\u5728\u7279\u5f81\u7a7a\u95f4\u8fdb\u884c\u63a9\u7801\u548c\u91cd\u5efa\uff0c\u4e0e\u5206\u5272\u4efb\u52a1\u76ee\u6807\u5bf9\u9f50\uff0c\u65e0\u9700\u4fee\u6539\u63a8\u7406\u67b6\u6784\u4e14\u96f6\u6d4b\u8bd5\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7684\u63a9\u7801\u5efa\u6a21\u65b9\u6cd5\u5728\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u8bed\u4e49\u5206\u5272\u4e2d\u5b58\u5728\u67b6\u6784\u4e0d\u517c\u5bb9\u548c\u4f18\u5316\u76ee\u6807\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u4e0e\u6807\u51c6\u5206\u5272\u67b6\u6784\u517c\u5bb9\u4e14\u76ee\u6807\u5bf9\u9f50\u7684\u8f85\u52a9\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMFM\u65b9\u6cd5\uff0c\u5728\u7279\u5f81\u7a7a\u95f4\u8fdb\u884c\u7279\u5f81\u63a9\u7801\u548c\u91cd\u5efa\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7Rebuilder\u6a21\u5757\u8fdb\u884c\u8054\u5408\u8bad\u7ec3\uff08\u63a8\u7406\u65f6\u4e22\u5f03\uff09\uff0c\u5229\u7528\u5206\u5272\u89e3\u7801\u5668\u5bf9\u91cd\u5efa\u7279\u5f81\u8fdb\u884c\u5206\u7c7b\uff0c\u786e\u4fdd\u8f85\u52a9\u76ee\u6807\u4e0e\u50cf\u7d20\u7ea7\u9884\u6d4b\u4efb\u52a1\u7d27\u5bc6\u8026\u5408\u3002", "result": "\u5728\u5404\u79cd\u67b6\u6784\u548cUDA\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMFM\u80fd\u6301\u7eed\u63d0\u5347\u5206\u5272\u6027\u80fd\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u9ad8\u6548\u4e14\u53ef\u6cdb\u5316\u7684\u7b56\u7565\u3002", "conclusion": "MFM\u662f\u4e00\u79cd\u6709\u6548\u7684\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u8bed\u4e49\u5206\u5272\u8f85\u52a9\u4efb\u52a1\uff0c\u901a\u8fc7\u7279\u5f81\u7a7a\u95f4\u63a9\u7801\u91cd\u5efa\u5b9e\u73b0\u4e86\u4e0e\u4e3b\u4efb\u52a1\u7684\u826f\u597d\u517c\u5bb9\u6027\u548c\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2509.13809", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13809", "abs": "https://arxiv.org/abs/2509.13809", "authors": ["Nick Theisen", "Kenny Schlegel", "Dietrich Paulus", "Peer Neubert"], "title": "Data-Efficient Spectral Classification of Hyperspectral Data Using MiniROCKET and HDC-MiniROCKET", "comment": "Accepted for publication at IEEE CASE 2025", "summary": "The classification of pixel spectra of hyperspectral images, i.e. spectral\nclassification, is used in many fields ranging from agricultural, over medical\nto remote sensing applications and is currently also expanding to areas such as\nautonomous driving. Even though for full hyperspectral images the\nbest-performing methods exploit spatial-spectral information, performing\nclassification solely on spectral information has its own advantages, e.g.\nsmaller model size and thus less data required for training. Moreover, spectral\ninformation is complementary to spatial information and improvements on either\npart can be used to improve spatial-spectral approaches in the future.\nRecently, 1D-Justo-LiuNet was proposed as a particularly efficient model with\nvery few parameters, which currently defines the state of the art in spectral\nclassification. However, we show that with limited training data the model\nperformance deteriorates. Therefore, we investigate MiniROCKET and\nHDC-MiniROCKET for spectral classification to mitigate that problem. The model\nextracts well-engineered features without trainable parameters in the feature\nextraction part and is therefore less vulnerable to limited training data. We\nshow that even though MiniROCKET has more parameters it outperforms\n1D-Justo-LiuNet in limited data scenarios and is mostly on par with it in the\ngeneral case", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u8bad\u7ec3\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528MiniROCKET\u548cHDC-MiniROCKET\u8fdb\u884c\u9ad8\u5149\u8c31\u56fe\u50cf\u5149\u8c31\u5206\u7c7b\uff0c\u4ee5\u89e3\u51b31D-Justo-LiuNet\u6a21\u578b\u5728\u6570\u636e\u4e0d\u8db3\u65f6\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "\u9ad8\u5149\u8c31\u56fe\u50cf\u7684\u5149\u8c31\u5206\u7c7b\u5728\u8bb8\u591a\u9886\u57df\u90fd\u6709\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u6700\u4f73\u6a21\u578b1D-Justo-LiuNet\u5728\u8bad\u7ec3\u6570\u636e\u6709\u9650\u65f6\u6027\u80fd\u4f1a\u663e\u8457\u4e0b\u964d\uff0c\u9700\u8981\u5bfb\u627e\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528MiniROCKET\u548cHDC-MiniROCKET\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u7279\u5f81\u63d0\u53d6\u90e8\u5206\u6ca1\u6709\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u4f7f\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\uff0c\u5bf9\u8bad\u7ec3\u6570\u636e\u91cf\u7684\u4f9d\u8d56\u8f83\u5c0f\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5c3d\u7ba1MiniROCKET\u6709\u66f4\u591a\u53c2\u6570\uff0c\u4f46\u5728\u6709\u9650\u6570\u636e\u573a\u666f\u4e0b\u4f18\u4e8e1D-Justo-LiuNet\uff0c\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "MiniROCKET\u7cfb\u5217\u6a21\u578b\u4e3a\u89e3\u51b3\u5149\u8c31\u5206\u7c7b\u4e2d\u8bad\u7ec3\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u5176\u65e0\u53c2\u6570\u7279\u5f81\u63d0\u53d6\u8bbe\u8ba1\u4f7f\u5176\u5bf9\u6570\u636e\u91cf\u7684\u654f\u611f\u6027\u964d\u4f4e\u3002"}}
{"id": "2509.13834", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13834", "abs": "https://arxiv.org/abs/2509.13834", "authors": ["Nguyen Lan Vi Vu", "Thanh-Huy Nguyen", "Thien Nguyen", "Daisuke Kihara", "Tianyang Wang", "Xingjian Li", "Min Xu"], "title": "Semi-MoE: Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation", "comment": "Accepted to BMVC 2025", "summary": "Semi-supervised learning has been employed to alleviate the need for\nextensive labeled data for histopathology image segmentation, but existing\nmethods struggle with noisy pseudo-labels due to ambiguous gland boundaries and\nmorphological misclassification. This paper introduces Semi-MOE, to the best of\nour knowledge, the first multi-task Mixture-of-Experts framework for\nsemi-supervised histopathology image segmentation. Our approach leverages three\nspecialized expert networks: A main segmentation expert, a signed distance\nfield regression expert, and a boundary prediction expert, each dedicated to\ncapturing distinct morphological features. Subsequently, the Multi-Gating\nPseudo-labeling module dynamically aggregates expert features, enabling a\nrobust fuse-and-refine pseudo-labeling mechanism. Furthermore, to eliminate\nmanual tuning while dynamically balancing multiple learning objectives, we\npropose an Adaptive Multi-Objective Loss. Extensive experiments on GlaS and\nCRAG benchmarks show that our method outperforms state-of-the-art approaches in\nlow-label settings, highlighting the potential of MoE-based architectures in\nadvancing semi-supervised segmentation. Our code is available at\nhttps://github.com/vnlvi2k3/Semi-MoE.", "AI": {"tldr": "Semi-MOE\u662f\u9996\u4e2a\u57fa\u4e8e\u591a\u4efb\u52a1\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\u7684\u534a\u76d1\u7763\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u4e2a\u4e13\u5bb6\u7f51\u7edc\u548c\u81ea\u9002\u5e94\u591a\u76ee\u6807\u635f\u5931\uff0c\u5728\u4f4e\u6807\u6ce8\u6570\u636e\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u56e0\u6a21\u7cca\u817a\u4f53\u8fb9\u754c\u548c\u5f62\u6001\u5b66\u8bef\u5206\u7c7b\u5bfc\u81f4\u7684\u566a\u58f0\u4f2a\u6807\u7b7e\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e09\u4e2a\u4e13\u5bb6\u7f51\u7edc\uff1a\u4e3b\u8981\u5206\u5272\u4e13\u5bb6\u3001\u7b26\u53f7\u8ddd\u79bb\u573a\u56de\u5f52\u4e13\u5bb6\u548c\u8fb9\u754c\u9884\u6d4b\u4e13\u5bb6\uff0c\u7ed3\u5408\u591a\u95e8\u63a7\u4f2a\u6807\u7b7e\u6a21\u5757\u548c\u81ea\u9002\u5e94\u591a\u76ee\u6807\u635f\u5931\u673a\u5236\u3002", "result": "\u5728GlaS\u548cCRAG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4e\u6807\u6ce8\u8bbe\u7f6e\u4e0b\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8e\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u7684\u65b9\u6cd5\u5728\u63a8\u8fdb\u534a\u76d1\u7763\u5206\u5272\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.13846", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13846", "abs": "https://arxiv.org/abs/2509.13846", "authors": ["Puru Vaish", "Felix Meister", "Tobias Heimann", "Christoph Brune", "Jelmer M. Wolterink"], "title": "Consistent View Alignment Improves Foundation Models for 3D Medical Image Segmentation", "comment": "MICCAI 2025: 1st Place in Transformer track and 2nd Place in\n  Convolution track of SSL3D-OpenMind challenge", "summary": "Many recent approaches in representation learning implicitly assume that\nuncorrelated views of a data point are sufficient to learn meaningful\nrepresentations for various downstream tasks. In this work, we challenge this\nassumption and demonstrate that meaningful structure in the latent space does\nnot emerge naturally. Instead, it must be explicitly induced. We propose a\nmethod that aligns representations from different views of the data to align\ncomplementary information without inducing false positives. Our experiments\nshow that our proposed self-supervised learning method, Consistent View\nAlignment, improves performance for downstream tasks, highlighting the critical\nrole of structured view alignment in learning effective representations. Our\nmethod achieved first and second place in the MICCAI 2025 SSL3D challenge when\nusing a Primus vision transformer and ResEnc convolutional neural network,\nrespectively. The code and pretrained model weights are released at\nhttps://github.com/Tenbatsu24/LatentCampus.", "AI": {"tldr": "\u672c\u6587\u6311\u6218\u4e86\u8868\u793a\u5b66\u4e60\u4e2d\u65e0\u76f8\u5173\u89c6\u56fe\u8db3\u4ee5\u5b66\u4e60\u6709\u610f\u4e49\u7684\u8868\u793a\u7684\u5047\u8bbe\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u663e\u5f0f\u5bf9\u9f50\u4e0d\u540c\u89c6\u56fe\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u5728\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9690\u542b\u5047\u8bbe\u6570\u636e\u70b9\u7684\u65e0\u76f8\u5173\u89c6\u56fe\u8db3\u4ee5\u5b66\u4e60\u6709\u610f\u4e49\u7684\u8868\u793a\uff0c\u4f46\u672c\u6587\u53d1\u73b0\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u6709\u610f\u4e49\u7ed3\u6784\u4e0d\u4f1a\u81ea\u7136\u51fa\u73b0\uff0c\u9700\u8981\u663e\u5f0f\u8bf1\u5bfc\u3002", "method": "\u63d0\u51faConsistent View Alignment\u65b9\u6cd5\uff0c\u5bf9\u9f50\u6570\u636e\u4e0d\u540c\u89c6\u56fe\u7684\u8868\u793a\uff0c\u4ee5\u5bf9\u9f50\u4e92\u8865\u4fe1\u606f\u800c\u4e0d\u5f15\u5165\u5047\u9633\u6027\u3002", "result": "\u5728MICCAI 2025 SSL3D\u6311\u6218\u8d5b\u4e2d\uff0c\u4f7f\u7528Primus vision transformer\u548cResEnc CNN\u5206\u522b\u83b7\u5f97\u7b2c\u4e00\u548c\u7b2c\u4e8c\u540d\uff0c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u7ed3\u6784\u5316\u89c6\u56fe\u5bf9\u9f50\u5728\u5b66\u4e60\u6709\u6548\u8868\u793a\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u663e\u5f0f\u5bf9\u9f50\u65b9\u6cd5\u4f18\u4e8e\u9690\u542b\u5047\u8bbe\u65e0\u76f8\u5173\u89c6\u56fe\u8db3\u591f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.13848", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13848", "abs": "https://arxiv.org/abs/2509.13848", "authors": ["Jiayi Pan", "Jiaming Xu", "Yongkang Zhou", "Guohao Dai"], "title": "SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation", "comment": null, "summary": "Feature caching has recently emerged as a promising method for diffusion\nmodel acceleration. It effectively alleviates the inefficiency problem caused\nby high computational requirements by caching similar features in the inference\nprocess of the diffusion model. In this paper, we analyze existing feature\ncaching methods from the perspective of information utilization, and point out\nthat relying solely on historical information will lead to constrained accuracy\nand speed performance. And we propose a novel paradigm that introduces future\ninformation via self-speculation based on the information similarity at the\nsame time step across different iteration times. Based on this paradigm, we\npresent \\textit{SpecDiff}, a training-free multi-level feature caching strategy\nincluding a cached feature selection algorithm and a multi-level feature\nclassification algorithm. (1) Feature selection algorithm based on\nself-speculative information. \\textit{SpecDiff} determines a dynamic importance\nscore for each token based on self-speculative information and historical\ninformation, and performs cached feature selection through the importance\nscore. (2) Multi-level feature classification algorithm based on feature\nimportance scores. \\textit{SpecDiff} classifies tokens by leveraging the\ndifferences in feature importance scores and introduces a multi-level feature\ncalculation strategy. Extensive experiments show that \\textit{SpecDiff}\nachieves average 2.80 \\times, 2.74 \\times , and 3.17\\times speedup with\nnegligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow\non NVIDIA A800-80GB GPU. By merging speculative and historical information,\n\\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing\nthe Pareto frontier of speedup and accuracy in the efficient diffusion model\ninference.", "AI": {"tldr": "SpecDiff\u662f\u4e00\u79cd\u57fa\u4e8e\u81ea\u63a8\u6d4b\u4fe1\u606f\u7684\u591a\u7ea7\u7279\u5f81\u7f13\u5b58\u7b56\u7565\uff0c\u901a\u8fc7\u5f15\u5165\u672a\u6765\u4fe1\u606f\u6765\u63d0\u5347\u6269\u6563\u6a21\u578b\u63a8\u7406\u6548\u7387\uff0c\u5728\u4fdd\u6301\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u663e\u8457\u52a0\u901f", "motivation": "\u73b0\u6709\u7279\u5f81\u7f13\u5b58\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u5386\u53f2\u4fe1\u606f\uff0c\u5bfc\u81f4\u7cbe\u5ea6\u548c\u901f\u5ea6\u6027\u80fd\u53d7\u9650\uff0c\u9700\u8981\u7a81\u7834\u901f\u5ea6-\u7cbe\u5ea6\u6743\u8861\u7684\u74f6\u9888", "method": "\u63d0\u51fa\u81ea\u63a8\u6d4b\u8303\u5f0f\u5f15\u5165\u672a\u6765\u4fe1\u606f\uff0c\u5305\u542b\u57fa\u4e8e\u81ea\u63a8\u6d4b\u4fe1\u606f\u7684\u7f13\u5b58\u7279\u5f81\u9009\u62e9\u7b97\u6cd5\u548c\u57fa\u4e8e\u7279\u5f81\u91cd\u8981\u6027\u5206\u6570\u7684\u591a\u7ea7\u7279\u5f81\u5206\u7c7b\u7b97\u6cd5", "result": "\u5728Stable Diffusion 3\u30013.5\u548cFLUX\u4e0a\u5206\u522b\u5b9e\u73b0\u5e73\u57472.80\u00d7\u30012.74\u00d7\u548c3.17\u00d7\u7684\u52a0\u901f\uff0c\u8d28\u91cf\u635f\u5931\u53ef\u5ffd\u7565", "conclusion": "\u901a\u8fc7\u878d\u5408\u63a8\u6d4b\u548c\u5386\u53f2\u4fe1\u606f\uff0cSpecDiff\u7a81\u7834\u4e86\u901f\u5ea6-\u7cbe\u5ea6\u6743\u8861\u7684\u74f6\u9888\uff0c\u63a8\u52a8\u4e86\u9ad8\u6548\u6269\u6563\u6a21\u578b\u63a8\u7406\u7684\u5e15\u7d2f\u6258\u524d\u6cbf"}}
{"id": "2509.13858", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13858", "abs": "https://arxiv.org/abs/2509.13858", "authors": ["Qianxin Xia", "Jiawei Du", "Guoming Lu", "Zhiyong Shu", "Jielei Wang"], "title": "EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics", "comment": null, "summary": "Dataset distillation aims to synthesize a compact dataset from the original\nlarge-scale one, enabling highly efficient learning while preserving\ncompetitive model performance. However, traditional techniques primarily\ncapture low-level visual features, neglecting the high-level semantic and\nstructural information inherent in images. In this paper, we propose EDITS, a\nnovel framework that exploits the implicit textual semantics within the image\ndata to achieve enhanced distillation. First, external texts generated by a\nVision Language Model (VLM) are fused with image features through a Global\nSemantic Query module, forming the prior clustered buffer. Local Semantic\nAwareness then selects representative samples from the buffer to construct\nimage and text prototypes, with the latter produced by guiding a Large Language\nModel (LLM) with meticulously crafted prompt. Ultimately, Dual Prototype\nGuidance strategy generates the final synthetic dataset through a diffusion\nmodel. Extensive experiments confirm the effectiveness of our method.Source\ncode is available in: https://github.com/einsteinxia/EDITS.", "AI": {"tldr": "EDITS\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6570\u636e\u96c6\u84b8\u998f\u6846\u67b6\uff0c\u5229\u7528\u56fe\u50cf\u4e2d\u7684\u9690\u5f0f\u6587\u672c\u8bed\u4e49\u4fe1\u606f\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u878d\u5408\u56fe\u50cf\u7279\u5f81\u4e0e\u5916\u90e8\u6587\u672c\uff0c\u6784\u5efa\u56fe\u50cf\u548c\u6587\u672c\u539f\u578b\uff0c\u6700\u7ec8\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\u3002", "motivation": "\u4f20\u7edf\u7684\u6570\u636e\u96c6\u84b8\u998f\u6280\u672f\u4e3b\u8981\u6355\u83b7\u4f4e\u7ea7\u89c6\u89c9\u7279\u5f81\uff0c\u5ffd\u89c6\u4e86\u56fe\u50cf\u4e2d\u56fa\u6709\u7684\u9ad8\u7ea7\u8bed\u4e49\u548c\u7ed3\u6784\u4fe1\u606f\uff0c\u5bfc\u81f4\u84b8\u998f\u6548\u679c\u6709\u9650\u3002", "method": "1. \u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5916\u90e8\u6587\u672c\u5e76\u4e0e\u56fe\u50cf\u7279\u5f81\u878d\u5408\uff0c\u5f62\u6210\u5148\u9a8c\u805a\u7c7b\u7f13\u51b2\u533a\uff1b2. \u901a\u8fc7\u5c40\u90e8\u8bed\u4e49\u611f\u77e5\u9009\u62e9\u4ee3\u8868\u6027\u6837\u672c\u6784\u5efa\u56fe\u50cf\u548c\u6587\u672c\u539f\u578b\uff1b3. \u5229\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u8bcd\u5f15\u5bfc\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u672c\u539f\u578b\uff1b4. \u91c7\u7528\u53cc\u539f\u578b\u5f15\u5bfc\u7b56\u7565\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u6700\u7ec8\u5408\u6210\u6570\u636e\u96c6\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728\u4fdd\u6301\u7ade\u4e89\u6027\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6548\u5b66\u4e60\u3002", "conclusion": "EDITS\u6846\u67b6\u901a\u8fc7\u5229\u7528\u56fe\u50cf\u4e2d\u7684\u6587\u672c\u8bed\u4e49\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u96c6\u84b8\u998f\u7684\u6548\u679c\uff0c\u4e3a\u9ad8\u6548\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13863", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13863", "abs": "https://arxiv.org/abs/2509.13863", "authors": ["Chu Chen", "Ander Biguri", "Jean-Michel Morel", "Raymond H. Chan", "Carola-Bibiane Sch\u00f6nlieb", "Jizhou Li"], "title": "LamiGauss: Pitching Radiative Gaussian for Sparse-View X-ray Laminography Reconstruction", "comment": null, "summary": "X-ray Computed Laminography (CL) is essential for non-destructive inspection\nof plate-like structures in applications such as microchips and composite\nbattery materials, where traditional computed tomography (CT) struggles due to\ngeometric constraints. However, reconstructing high-quality volumes from\nlaminographic projections remains challenging, particularly under highly\nsparse-view acquisition conditions. In this paper, we propose a reconstruction\nalgorithm, namely LamiGauss, that combines Gaussian Splatting radiative\nrasterization with a dedicated detector-to-world transformation model\nincorporating the laminographic tilt angle. LamiGauss leverages an\ninitialization strategy that explicitly filters out common laminographic\nartifacts from the preliminary reconstruction, preventing redundant Gaussians\nfrom being allocated to false structures and thereby concentrating model\ncapacity on representing the genuine object. Our approach effectively optimizes\ndirectly from sparse projections, enabling accurate and efficient\nreconstruction with limited data. Extensive experiments on both synthetic and\nreal datasets demonstrate the effectiveness and superiority of the proposed\nmethod over existing techniques. LamiGauss uses only 3$\\%$ of full views to\nachieve superior performance over the iterative method optimized on a full\ndataset.", "AI": {"tldr": "\u63d0\u51faLamiGauss\u7b97\u6cd5\uff0c\u7ed3\u5408\u9ad8\u65af\u6e85\u5c04\u8f90\u5c04\u5149\u6805\u5316\u548c\u4e13\u7528\u63a2\u6d4b\u5668-\u4e16\u754c\u53d8\u6362\u6a21\u578b\uff0c\u7528\u4e8e\u7a00\u758f\u89c6\u89d2X\u5c04\u7ebf\u5c42\u6790\u6210\u50cf\u91cd\u5efa\uff0c\u4ec5\u97003%\u5b8c\u6574\u89c6\u89d2\u5373\u53ef\u8fbe\u5230\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u6027\u80fd", "motivation": "\u4f20\u7edfCT\u5728\u5e73\u677f\u7ed3\u6784\u68c0\u6d4b\u4e2d\u5b58\u5728\u51e0\u4f55\u7ea6\u675f\u95ee\u9898\uff0c\u5c42\u6790\u6210\u50cf\u5728\u7a00\u758f\u89c6\u89d2\u6761\u4ef6\u4e0b\u91cd\u5efa\u9ad8\u8d28\u91cf\u4f53\u79ef\u4ecd\u5177\u6311\u6218\u6027", "method": "\u7ed3\u5408\u9ad8\u65af\u6e85\u5c04\u8f90\u5c04\u5149\u6805\u5316\u548c\u5305\u542b\u5c42\u6790\u503e\u659c\u89d2\u7684\u4e13\u7528\u63a2\u6d4b\u5668-\u4e16\u754c\u53d8\u6362\u6a21\u578b\uff0c\u91c7\u7528\u521d\u59cb\u5316\u7b56\u7565\u8fc7\u6ee4\u5e38\u89c1\u5c42\u6790\u4f2a\u5f71\uff0c\u9632\u6b62\u9ad8\u65af\u5206\u914d\u5230\u865a\u5047\u7ed3\u6784", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u4ec5\u4f7f\u75283%\u5b8c\u6574\u89c6\u89d2\u5373\u53ef\u8fbe\u5230\u4f18\u4e8e\u5728\u5168\u6570\u636e\u96c6\u4e0a\u4f18\u5316\u7684\u8fed\u4ee3\u65b9\u6cd5\u7684\u6027\u80fd", "conclusion": "LamiGauss\u80fd\u591f\u76f4\u63a5\u4ece\u7a00\u758f\u6295\u5f71\u4f18\u5316\uff0c\u5b9e\u73b0\u6709\u9650\u6570\u636e\u4e0b\u7684\u51c6\u786e\u9ad8\u6548\u91cd\u5efa\uff0c\u5728\u5c42\u6790\u6210\u50cf\u91cd\u5efa\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027"}}
{"id": "2509.13864", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13864", "abs": "https://arxiv.org/abs/2509.13864", "authors": ["Jovana Videnovic", "Matej Kristan", "Alan Lukezic"], "title": "Distractor-Aware Memory-Based Visual Object Tracking", "comment": "Code available on Github: https://github.com/jovanavidenovic/DAM4SAM", "summary": "Recent emergence of memory-based video segmentation methods such as SAM2 has\nled to models with excellent performance in segmentation tasks, achieving\nleading results on numerous benchmarks. However, these modes are not fully\nadjusted for visual object tracking, where distractors (i.e., objects visually\nsimilar to the target) pose a key challenge. In this paper we propose a\ndistractor-aware drop-in memory module and introspection-based management\nmethod for SAM2, leading to DAM4SAM. Our design effectively reduces the\ntracking drift toward distractors and improves redetection capability after\nobject occlusion. To facilitate the analysis of tracking in the presence of\ndistractors, we construct DiDi, a Distractor-Distilled dataset. DAM4SAM\noutperforms SAM2.1 on thirteen benchmarks and sets new state-of-the-art results\non ten. Furthermore, integrating the proposed distractor-aware memory into a\nreal-time tracker EfficientTAM leads to 11% improvement and matches tracking\nquality of the non-real-time SAM2.1-L on multiple tracking and segmentation\nbenchmarks, while integration with edge-based tracker EdgeTAM delivers 4%\nperformance boost, demonstrating a very good generalization across\narchitectures.", "AI": {"tldr": "\u63d0\u51fa\u4e86DAM4SAM\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5e72\u6270\u7269\u611f\u77e5\u7684\u5185\u5b58\u6a21\u5757\u548c\u81ea\u7701\u7ba1\u7406\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86SAM2\u5728\u89c6\u89c9\u76ee\u6807\u8ddf\u8e2a\u4e2d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5e72\u6270\u7269\u548c\u76ee\u6807\u906e\u6321\u65b9\u9762\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5185\u5b58\u7684\u89c6\u9891\u5206\u5272\u65b9\u6cd5\uff08\u5982SAM2\uff09\u5728\u5206\u5272\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u89c6\u89c9\u76ee\u6807\u8ddf\u8e2a\u4e2d\u9762\u4e34\u5e72\u6270\u7269\uff08\u5916\u89c2\u76f8\u4f3c\u7684\u975e\u76ee\u6807\u7269\u4f53\uff09\u5e26\u6765\u7684\u6311\u6218\uff0c\u5bb9\u6613\u5bfc\u81f4\u8ddf\u8e2a\u6f02\u79fb\u548c\u906e\u6321\u540e\u91cd\u68c0\u6d4b\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u5e72\u6270\u7269\u611f\u77e5\u7684\u5373\u63d2\u5373\u7528\u5185\u5b58\u6a21\u5757\u548c\u57fa\u4e8e\u81ea\u7701\u7684\u5185\u5b58\u7ba1\u7406\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86DiDi\u5e72\u6270\u7269\u84b8\u998f\u6570\u636e\u96c6\u7528\u4e8e\u5206\u6790\uff0c\u5e76\u5c06\u8be5\u6a21\u5757\u96c6\u6210\u5230\u4e0d\u540c\u7684\u8ddf\u8e2a\u5668\u67b6\u6784\u4e2d\u3002", "result": "DAM4SAM\u572813\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8aSAM2.1\uff0c\u572810\u4e2a\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0\u7684SOTA\uff1b\u96c6\u6210\u5230\u5b9e\u65f6\u8ddf\u8e2a\u5668EfficientTAM\u4e2d\u63d0\u534711%\u6027\u80fd\uff0c\u4e0eSAM2.1-L\u76f8\u5f53\uff1b\u96c6\u6210\u5230EdgeTAM\u4e2d\u63d0\u53474%\u6027\u80fd\uff0c\u663e\u793a\u51fa\u826f\u597d\u7684\u67b6\u6784\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5e72\u6270\u7269\u611f\u77e5\u5185\u5b58\u6a21\u5757\u80fd\u6709\u6548\u89e3\u51b3\u8ddf\u8e2a\u4e2d\u7684\u5e72\u6270\u7269\u6311\u6218\uff0c\u63d0\u5347\u8ddf\u8e2a\u7a33\u5b9a\u6027\u548c\u91cd\u68c0\u6d4b\u80fd\u529b\uff0c\u4e14\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u6027\u548c\u53ef\u79fb\u690d\u6027\u3002"}}
{"id": "2509.13873", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13873", "abs": "https://arxiv.org/abs/2509.13873", "authors": ["Siam Tahsin Bhuiyan", "Rashedur Rahman", "Sefatul Wasi", "Naomi Yagi", "Syoji Kobashi", "Ashraful Islam", "Saadia Binte Alam"], "title": "Invisible Yet Detected: PelFANet with Attention-Guided Anatomical Fusion for Pelvic Fracture Diagnosis", "comment": "Accepted at MICCAI EMERGE 2025", "summary": "Pelvic fractures pose significant diagnostic challenges, particularly in\ncases where fracture signs are subtle or invisible on standard radiographs. To\naddress this, we introduce PelFANet, a dual-stream attention network that fuses\nraw pelvic X-rays with segmented bone images to improve fracture\nclassification. The network em-ploys Fused Attention Blocks (FABlocks) to\niteratively exchange and refine fea-tures from both inputs, capturing global\ncontext and localized anatomical detail. Trained in a two-stage pipeline with a\nsegmentation-guided approach, PelFANet demonstrates superior performance over\nconventional methods. On the AMERI dataset, it achieves 88.68% accuracy and\n0.9334 AUC on visible fractures, while generalizing effectively to invisible\nfracture cases with 82.29% accuracy and 0.8688 AUC, despite not being trained\non them. These results highlight the clini-cal potential of anatomy-aware\ndual-input architectures for robust fracture detec-tion, especially in\nscenarios with subtle radiographic presentations.", "AI": {"tldr": "PelFANet\u662f\u4e00\u79cd\u53cc\u6d41\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u878d\u5408\u539f\u59cb\u9aa8\u76c6X\u5149\u7247\u548c\u5206\u5272\u9aa8\u56fe\u50cf\uff0c\u901a\u8fc7Fused Attention Blocks\u8fed\u4ee3\u4ea4\u6362\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u9aa8\u76c6\u9aa8\u6298\u5206\u7c7b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u7ec6\u5fae\u9aa8\u6298\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u9aa8\u76c6\u9aa8\u6298\u5728\u6807\u51c6X\u5149\u7247\u4e2d\u5e38\u5e38\u96be\u4ee5\u8bca\u65ad\uff0c\u7279\u522b\u662f\u5f53\u9aa8\u6298\u8ff9\u8c61\u7ec6\u5fae\u6216\u4e0d\u53ef\u89c1\u65f6\uff0c\u9700\u8981\u66f4\u5148\u8fdb\u7684\u68c0\u6d4b\u65b9\u6cd5\u6765\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faPelFANet\u53cc\u6d41\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u878d\u5408\u539f\u59cbX\u5149\u7247\u548c\u5206\u5272\u9aa8\u56fe\u50cf\uff0c\u4f7f\u7528FABlocks\u5757\u8fed\u4ee3\u4ea4\u6362\u548c\u7cbe\u70bc\u7279\u5f81\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u5206\u5272\u5f15\u5bfc\u7684\u8bad\u7ec3\u6d41\u7a0b\u3002", "result": "\u5728AMERI\u6570\u636e\u96c6\u4e0a\uff0c\u53ef\u89c1\u9aa8\u6298\u51c6\u786e\u738788.68%\uff0cAUC 0.9334\uff1b\u4e0d\u53ef\u89c1\u9aa8\u6298\u51c6\u786e\u738782.29%\uff0cAUC 0.8688\uff0c\u5373\u4f7f\u672a\u9488\u5bf9\u4e0d\u53ef\u89c1\u9aa8\u6298\u8fdb\u884c\u8bad\u7ec3\u3002", "conclusion": "\u89e3\u5256\u611f\u77e5\u7684\u53cc\u8f93\u5165\u67b6\u6784\u5728\u9aa8\u76c6\u9aa8\u6298\u68c0\u6d4b\u4e2d\u5177\u6709\u663e\u8457\u4e34\u5e8a\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u653e\u5c04\u5b66\u8868\u73b0\u7ec6\u5fae\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.13883", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13883", "abs": "https://arxiv.org/abs/2509.13883", "authors": ["Zhen Xu", "Guorui Lu", "Chang Gao", "Qinyu Chen"], "title": "EvHand-FPV: Efficient Event-Based 3D Hand Tracking from First-Person View", "comment": "8 pages", "summary": "Hand tracking holds great promise for intuitive interaction paradigms, but\nframe-based methods often struggle to meet the requirements of accuracy, low\nlatency, and energy efficiency, especially in resource-constrained settings\nsuch as Extended Reality (XR) devices. Event cameras provide $\\mu$s-level\ntemporal resolution at mW-level power by asynchronously sensing brightness\nchanges. In this work, we present EvHand-FPV, a lightweight framework for\negocentric First-Person-View 3D hand tracking from a single event camera. We\nconstruct an event-based FPV dataset that couples synthetic training data with\n3D labels and real event data with 2D labels for evaluation to address the\nscarcity of egocentric benchmarks. EvHand-FPV also introduces a wrist-based\nregion of interest (ROI) that localizes the hand region via geometric cues,\ncombined with an end-to-end mapping strategy that embeds ROI offsets into the\nnetwork to reduce computation without explicit reconstruction, and a multi-task\nlearning strategy with an auxiliary geometric feature head that improves\nrepresentations without test-time overhead. On our real FPV test set,\nEvHand-FPV improves 2D-AUCp from 0.77 to 0.85 while reducing parameters from\n11.2M to 1.2M by 89% and FLOPs per inference from 1.648G to 0.185G by 89%. It\nalso maintains a competitive 3D-AUCp of 0.84 on synthetic data. These results\ndemonstrate accurate and efficient egocentric event-based hand tracking\nsuitable for on-device XR applications. The dataset and code are available at\nhttps://github.com/zen5x5/EvHand-FPV.", "AI": {"tldr": "EvHand-FPV\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5355\u4e8b\u4ef6\u76f8\u673a\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d23D\u624b\u90e8\u8ffd\u8e2a\u6846\u67b6\uff0c\u901a\u8fc7\u624b\u8155ROI\u5b9a\u4f4d\u3001\u7aef\u5230\u7aef\u6620\u5c04\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u91cf\u548c\u53c2\u6570\u6570\u91cf\uff0c\u9002\u5408XR\u8bbe\u5907\u5e94\u7528\u3002", "motivation": "\u4f20\u7edf\u5e27\u5f0f\u65b9\u6cd5\u5728\u7cbe\u5ea6\u3001\u5ef6\u8fdf\u548c\u80fd\u6548\u65b9\u9762\u96be\u4ee5\u6ee1\u8db3XR\u8bbe\u5907\u7684\u9700\u6c42\uff0c\u800c\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u5fae\u79d2\u7ea7\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u6beb\u74e6\u7ea7\u529f\u8017\u7684\u4f18\u52bf\uff0c\u4f46\u7f3a\u4e4f\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "method": "\u6784\u5efa\u4e8b\u4ef6\u76f8\u673aFPV\u6570\u636e\u96c6\uff08\u5408\u62103D\u6807\u7b7e+\u771f\u5b9e2D\u6807\u7b7e\uff09\uff0c\u5f15\u5165\u624b\u8155ROI\u51e0\u4f55\u5b9a\u4f4d\u3001\u7aef\u5230\u7aef\u6620\u5c04\u5d4c\u5165ROI\u504f\u79fb\u51cf\u5c11\u8ba1\u7b97\uff0c\u4ee5\u53ca\u591a\u4efb\u52a1\u5b66\u4e60\u8f85\u52a9\u51e0\u4f55\u7279\u5f81\u5934\u63d0\u5347\u8868\u5f81\u80fd\u529b\u3002", "result": "\u5728\u771f\u5b9eFPV\u6d4b\u8bd5\u96c6\u4e0a\uff0c2D-AUCp\u4ece0.77\u63d0\u5347\u52300.85\uff0c\u53c2\u6570\u51cf\u5c1189%\uff0811.2M\u21921.2M\uff09\uff0c\u8ba1\u7b97\u91cf\u51cf\u5c1189%\uff081.648G\u21920.185G FLOPs\uff09\uff0c\u5408\u6210\u6570\u636e\u4e0a\u4fdd\u63010.84\u76843D-AUCp\u3002", "conclusion": "EvHand-FPV\u5b9e\u73b0\u4e86\u51c6\u786e\u9ad8\u6548\u7684\u81ea\u6211\u4e2d\u5fc3\u4e8b\u4ef6\u624b\u90e8\u8ffd\u8e2a\uff0c\u9002\u5408XR\u8bbe\u5907\u7684\u5b9e\u65f6\u5e94\u7528\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.13907", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13907", "abs": "https://arxiv.org/abs/2509.13907", "authors": ["Jiyun Im", "SuBeen Lee", "Miso Lee", "Jae-Pil Heo"], "title": "White Aggregation and Restoration for Few-shot 3D Point Cloud Semantic Segmentation", "comment": "9 pages, 5 figures", "summary": "Few-Shot 3D Point Cloud Segmentation (FS-PCS) aims to predict per-point\nlabels for an unlabeled point cloud, given only a few labeled examples. To\nextract discriminative representations from the limited support set, existing\nmethods have constructed prototypes using conventional algorithms such as\nfarthest point sampling. However, we point out that its initial randomness\nsignificantly affects FS-PCS performance and that the prototype generation\nprocess remains underexplored despite its prevalence. This motivates us to\ninvestigate an advanced prototype generation method based on attention\nmechanism. Despite its potential, we found that vanilla module suffers from the\ndistributional gap between learnable prototypical tokens and support features.\nTo overcome this, we propose White Aggregation and Restoration Module (WARM),\nwhich resolves the misalignment by sandwiching cross-attention between\nwhitening and coloring transformations. Specifically, whitening aligns the\nsupport features to prototypical tokens before attention process, and\nsubsequently coloring restores the original distribution to the attended\ntokens. This simple yet effective design enables robust attention, thereby\ngenerating representative prototypes by capturing the semantic relationships\namong support features. Our method achieves state-of-the-art performance with a\nsignificant margin on multiple FS-PCS benchmarks, demonstrating its\neffectiveness through extensive experiments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faWARM\u6a21\u5757\uff0c\u901a\u8fc7\u767d\u5316\u548c\u67d3\u8272\u53d8\u6362\u89e3\u51b3\u5c11\u6837\u672c3D\u70b9\u4e91\u5206\u5272\u4e2d\u53ef\u5b66\u4e60\u539f\u578b\u6807\u8bb0\u4e0e\u652f\u6301\u7279\u5f81\u4e4b\u95f4\u7684\u5206\u5e03\u5dee\u5f02\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u539f\u578b\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u5c11\u6837\u672c3D\u70b9\u4e91\u5206\u5272\u65b9\u6cd5\u4f7f\u7528\u4f20\u7edf\u7b97\u6cd5\uff08\u5982\u6700\u8fdc\u70b9\u91c7\u6837\uff09\u6784\u5efa\u539f\u578b\uff0c\u4f46\u521d\u59cb\u968f\u673a\u6027\u4e25\u91cd\u5f71\u54cd\u6027\u80fd\uff0c\u4e14\u539f\u578b\u751f\u6210\u8fc7\u7a0b\u7814\u7a76\u4e0d\u8db3\u3002\u6ce8\u610f\u529b\u673a\u5236\u867d\u5177\u6f5c\u529b\uff0c\u4f46\u5b58\u5728\u5206\u5e03\u5dee\u5f02\u95ee\u9898\u3002", "method": "\u63d0\u51faWhite Aggregation and Restoration Module (WARM)\uff0c\u5728\u767d\u5316\u548c\u67d3\u8272\u53d8\u6362\u4e4b\u95f4\u63d2\u5165\u4ea4\u53c9\u6ce8\u610f\u529b\uff1a\u767d\u5316\u5c06\u652f\u6301\u7279\u5f81\u4e0e\u539f\u578b\u6807\u8bb0\u5bf9\u9f50\uff0c\u6ce8\u610f\u529b\u5904\u7406\u540e\u67d3\u8272\u6062\u590d\u539f\u59cb\u5206\u5e03\uff0c\u4ece\u800c\u751f\u6210\u5177\u6709\u8bed\u4e49\u5173\u7cfb\u7684\u4ee3\u8868\u6027\u539f\u578b\u3002", "result": "\u5728\u591a\u4e2a\u5c11\u6837\u672c3D\u70b9\u4e91\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u9886\u5148\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "WARM\u6a21\u5757\u901a\u8fc7\u7b80\u5355\u7684\u767d\u5316-\u6ce8\u610f\u529b-\u67d3\u8272\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5e03\u5dee\u5f02\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4e3a\u5c11\u6837\u672c3D\u70b9\u4e91\u5206\u5272\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u539f\u578b\u751f\u6210\u65b9\u6848\u3002"}}
{"id": "2509.13919", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13919", "abs": "https://arxiv.org/abs/2509.13919", "authors": ["Yuanchen Wu", "Ke Yan", "Shouhong Ding", "Ziyin Zhou", "Xiaoqiang Li"], "title": "Towards Rationale-Answer Alignment of LVLMs via Self-Rationale Calibration", "comment": "Accepted by ICML 2025", "summary": "Large Vision-Language Models (LVLMs) have manifested strong visual question\nanswering capability. However, they still struggle with aligning the rationale\nand the generated answer, leading to inconsistent reasoning and incorrect\nresponses. To this end, this paper introduces the Self-Rationale Calibration\n(SRC) framework to iteratively calibrate the alignment between rationales and\nanswers. SRC begins by employing a lightweight \"rationale fine-tuning\"\napproach, which modifies the model's response format to require a rationale\nbefore deriving an answer without explicit prompts. Next, SRC searches for a\ndiverse set of candidate responses from the fine-tuned LVLMs for each sample,\nfollowed by a proposed pairwise scoring strategy using a tailored scoring\nmodel, R-Scorer, to evaluate both rationale quality and factual consistency of\ncandidates. Based on a confidence-weighted preference curation process, SRC\ndecouples the alignment calibration into a preference fine-tuning manner,\nleading to significant improvements of LVLMs in perception, reasoning, and\ngeneralization across multiple benchmarks. Our results emphasize the\nrationale-oriented alignment in exploring the potential of LVLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u81ea\u6821\u51c6\u63a8\u7406\u6846\u67b6(SRC)\uff0c\u901a\u8fc7\u8fed\u4ee3\u6821\u51c6\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u63a8\u7406\u8fc7\u7a0b\u4e0e\u7b54\u6848\u7684\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u611f\u77e5\u3001\u63a8\u7406\u548c\u6cdb\u5316\u65b9\u9762\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7ecf\u5e38\u51fa\u73b0\u63a8\u7406\u8fc7\u7a0b\u4e0e\u6700\u7ec8\u7b54\u6848\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u9519\u8bef\u56de\u7b54\u3002\u9700\u8981\u89e3\u51b3\u63a8\u7406\u4e0e\u7b54\u6848\u7684\u5bf9\u9f50\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\"\u63a8\u7406\u5fae\u8c03\"\u4fee\u6539\u6a21\u578b\u54cd\u5e94\u683c\u5f0f\uff0c\u8981\u6c42\u5148\u63d0\u4f9b\u63a8\u7406\u518d\u7ed9\u51fa\u7b54\u6848\uff1b\u4f7f\u7528R-Scorer\u8bc4\u5206\u6a21\u578b\u8bc4\u4f30\u5019\u9009\u54cd\u5e94\u7684\u63a8\u7406\u8d28\u91cf\u548c\u4e8b\u5b9e\u4e00\u81f4\u6027\uff1b\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u52a0\u6743\u7684\u504f\u597d\u5fae\u8c03\u65b9\u5f0f\u6821\u51c6\u5bf9\u9f50\u3002", "result": "SRC\u6846\u67b6\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u611f\u77e5\u3001\u63a8\u7406\u548c\u6cdb\u5316\u65b9\u9762\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u5f3a\u8c03\u63a8\u7406\u5bfc\u5411\u7684\u5bf9\u9f50\u5728\u6316\u6398\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6f5c\u529b\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u63d0\u5347\u6a21\u578b\u63a8\u7406\u4e00\u81f4\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13922", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13922", "abs": "https://arxiv.org/abs/2509.13922", "authors": ["Wenkui Yang", "Jie Cao", "Junxian Duan", "Ran He"], "title": "Towards Robust Defense against Customization via Protective Perturbation Resistant to Diffusion-based Purification", "comment": "Accepted to ICCV 2025", "summary": "Diffusion models like Stable Diffusion have become prominent in visual\nsynthesis tasks due to their powerful customization capabilities, which also\nintroduce significant security risks, including deepfakes and copyright\ninfringement. In response, a class of methods known as protective perturbation\nemerged, which mitigates image misuse by injecting imperceptible adversarial\nnoise. However, purification can remove protective perturbations, thereby\nexposing images again to the risk of malicious forgery. In this work, we\nformalize the anti-purification task, highlighting challenges that hinder\nexisting approaches, and propose a simple diagnostic protective perturbation\nnamed AntiPure. AntiPure exposes vulnerabilities of purification within the\n\"purification-customization\" workflow, owing to two guidance mechanisms: 1)\nPatch-wise Frequency Guidance, which reduces the model's influence over\nhigh-frequency components in the purified image, and 2) Erroneous Timestep\nGuidance, which disrupts the model's denoising strategy across different\ntimesteps. With additional guidance, AntiPure embeds imperceptible\nperturbations that persist under representative purification settings,\nachieving effective post-customization distortion. Experiments show that, as a\nstress test for purification, AntiPure achieves minimal perceptual discrepancy\nand maximal distortion, outperforming other protective perturbation methods\nwithin the purification-customization workflow.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AntiPure\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u56fe\u50cf\u4e2d\u6ce8\u5165\u96be\u4ee5\u5bdf\u89c9\u7684\u5bf9\u6297\u6027\u566a\u58f0\u6765\u62b5\u5fa1\u51c0\u5316\u653b\u51fb\uff0c\u4fdd\u62a4\u56fe\u50cf\u514d\u53d7\u6076\u610f\u4f2a\u9020\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5982Stable Diffusion\u7684\u5f3a\u5927\u5b9a\u5236\u80fd\u529b\u5e26\u6765\u4e86\u4e25\u91cd\u7684\u5b89\u5168\u98ce\u9669\uff0c\u5305\u62ec\u6df1\u5ea6\u4f2a\u9020\u548c\u7248\u6743\u4fb5\u6743\u3002\u73b0\u6709\u7684\u4fdd\u62a4\u6027\u6270\u52a8\u65b9\u6cd5\u5bb9\u6613\u88ab\u51c0\u5316\u6280\u672f\u79fb\u9664\uff0c\u5bfc\u81f4\u56fe\u50cf\u518d\u6b21\u9762\u4e34\u6076\u610f\u4f2a\u9020\u7684\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e86AntiPure\u65b9\u6cd5\uff0c\u91c7\u7528\u4e24\u79cd\u5f15\u5bfc\u673a\u5236\uff1a1) \u5757\u72b6\u9891\u7387\u5f15\u5bfc\uff0c\u51cf\u5c11\u6a21\u578b\u5bf9\u51c0\u5316\u56fe\u50cf\u9ad8\u9891\u5206\u91cf\u7684\u5f71\u54cd\uff1b2) \u9519\u8bef\u65f6\u95f4\u6b65\u5f15\u5bfc\uff0c\u6270\u4e71\u6a21\u578b\u5728\u4e0d\u540c\u65f6\u95f4\u6b65\u7684\u53bb\u566a\u7b56\u7565\u3002\u901a\u8fc7\u989d\u5916\u5f15\u5bfc\u5d4c\u5165\u96be\u4ee5\u5bdf\u89c9\u7684\u6270\u52a8\uff0c\u4f7f\u5176\u5728\u4ee3\u8868\u6027\u51c0\u5316\u8bbe\u7f6e\u4e0b\u6301\u7eed\u5b58\u5728\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAntiPure\u4f5c\u4e3a\u51c0\u5316\u7684\u538b\u529b\u6d4b\u8bd5\uff0c\u5b9e\u73b0\u4e86\u6700\u5c0f\u7684\u611f\u77e5\u5dee\u5f02\u548c\u6700\u5927\u7684\u5931\u771f\u6548\u679c\uff0c\u5728\u51c0\u5316-\u5b9a\u5236\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u4f18\u4e8e\u5176\u4ed6\u4fdd\u62a4\u6027\u6270\u52a8\u65b9\u6cd5\u3002", "conclusion": "AntiPure\u6709\u6548\u89e3\u51b3\u4e86\u4fdd\u62a4\u6027\u6270\u52a8\u6613\u88ab\u51c0\u5316\u79fb\u9664\u7684\u95ee\u9898\uff0c\u4e3a\u56fe\u50cf\u5b89\u5168\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u4fdd\u62a4\u65b9\u6848\uff0c\u5728\u5bf9\u6297\u51c0\u5316\u653b\u51fb\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.13936", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13936", "abs": "https://arxiv.org/abs/2509.13936", "authors": ["Harvey Mannering", "Zhiwu Huang", "Adam Prugel-Bennett"], "title": "Noise-Level Diffusion Guidance: Well Begun is Half Done", "comment": null, "summary": "Diffusion models have achieved state-of-the-art image generation. However,\nthe random Gaussian noise used to start the diffusion process influences the\nfinal output, causing variations in image quality and prompt adherence.\nExisting noise-level optimization approaches generally rely on extra dataset\nconstruction, additional networks, or backpropagation-based optimization,\nlimiting their practicality. In this paper, we propose Noise Level Guidance\n(NLG), a simple, efficient, and general noise-level optimization approach that\nrefines initial noise by increasing the likelihood of its alignment with\ngeneral guidance - requiring no additional training data, auxiliary networks,\nor backpropagation. The proposed NLG approach provides a unified framework\ngeneralizable to both conditional and unconditional diffusion models,\naccommodating various forms of diffusion-level guidance. Extensive experiments\non five standard benchmarks demonstrate that our approach enhances output\ngeneration quality and input condition adherence. By seamlessly integrating\nwith existing guidance methods while maintaining computational efficiency, our\nmethod establishes NLG as a practical and scalable enhancement to diffusion\nmodels. Code can be found at\nhttps://github.com/harveymannering/NoiseLevelGuidance.", "AI": {"tldr": "\u63d0\u51faNoise Level Guidance (NLG)\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u521d\u59cb\u566a\u58f0\u63d0\u9ad8\u6269\u6563\u6a21\u578b\u751f\u6210\u8d28\u91cf\u548c\u63d0\u793a\u9075\u5faa\u5ea6\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6570\u636e\u6216\u7f51\u7edc", "motivation": "\u6269\u6563\u6a21\u578b\u521d\u59cb\u968f\u673a\u9ad8\u65af\u566a\u58f0\u5f71\u54cd\u6700\u7ec8\u8f93\u51fa\u8d28\u91cf\u548c\u63d0\u793a\u9075\u5faa\u5ea6\uff0c\u73b0\u6709\u566a\u58f0\u4f18\u5316\u65b9\u6cd5\u9700\u8981\u989d\u5916\u6570\u636e\u96c6\u3001\u7f51\u7edc\u6216\u53cd\u5411\u4f20\u64ad\u4f18\u5316\uff0c\u5b9e\u7528\u6027\u53d7\u9650", "method": "NLG\u65b9\u6cd5\u901a\u8fc7\u589e\u52a0\u521d\u59cb\u566a\u58f0\u4e0e\u901a\u7528\u6307\u5bfc\u5bf9\u9f50\u7684\u53ef\u80fd\u6027\u6765\u4f18\u5316\u566a\u58f0\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6570\u636e\u3001\u8f85\u52a9\u7f51\u7edc\u6216\u53cd\u5411\u4f20\u64ad\uff0c\u63d0\u4f9b\u7edf\u4e00\u6846\u67b6\u9002\u7528\u4e8e\u6761\u4ef6\u548c\u65e0\u6761\u4ef6\u6269\u6563\u6a21\u578b", "result": "\u5728\u4e94\u4e2a\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u8f93\u51fa\u751f\u6210\u8d28\u91cf\u548c\u8f93\u5165\u6761\u4ef6\u9075\u5faa\u5ea6", "conclusion": "NLG\u4f5c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u589e\u5f3a\u65b9\u6cd5\uff0c\u80fd\u591f\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u6307\u5bfc\u65b9\u6cd5\u4e2d\u5e76\u4fdd\u6301\u8ba1\u7b97\u6548\u7387"}}
{"id": "2509.13939", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13939", "abs": "https://arxiv.org/abs/2509.13939", "authors": ["Gia Khanh Nguyen", "Yifeng Huang", "Minh Hoai"], "title": "Can Current AI Models Count What We Mean, Not What They See? A Benchmark and Systematic Evaluation", "comment": null, "summary": "Visual counting is a fundamental yet challenging task, especially when users\nneed to count objects of a specific type in complex scenes. While recent\nmodels, including class-agnostic counting models and large vision-language\nmodels (VLMs), show promise in counting tasks, their ability to perform\nfine-grained, intent-driven counting remains unclear. In this paper, we\nintroduce PairTally, a benchmark dataset specifically designed to evaluate\nfine-grained visual counting. Each of the 681 high-resolution images in\nPairTally contains two object categories, requiring models to distinguish and\ncount based on subtle differences in shape, size, color, or semantics. The\ndataset includes both inter-category (distinct categories) and intra-category\n(closely related subcategories) settings, making it suitable for rigorous\nevaluation of selective counting capabilities. We benchmark a variety of\nstate-of-the-art models, including exemplar-based methods, language-prompted\nmodels, and large VLMs. Our results show that despite recent advances, current\nmodels struggle to reliably count what users intend, especially in fine-grained\nand visually ambiguous cases. PairTally provides a new foundation for\ndiagnosing and improving fine-grained visual counting systems.", "AI": {"tldr": "PairTally\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8ba1\u6570\u80fd\u529b\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b681\u5f20\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u6bcf\u5f20\u56fe\u50cf\u5305\u542b\u4e24\u4e2a\u7269\u4f53\u7c7b\u522b\uff0c\u8981\u6c42\u6a21\u578b\u57fa\u4e8e\u5f62\u72b6\u3001\u5927\u5c0f\u3001\u989c\u8272\u6216\u8bed\u4e49\u7684\u7ec6\u5fae\u5dee\u5f02\u8fdb\u884c\u533a\u5206\u548c\u8ba1\u6570\u3002", "motivation": "\u5f53\u524d\u6a21\u578b\uff08\u5305\u62ec\u7c7b\u522b\u65e0\u5173\u8ba1\u6570\u6a21\u578b\u548c\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff09\u5728\u7ec6\u7c92\u5ea6\u3001\u610f\u56fe\u9a71\u52a8\u7684\u8ba1\u6570\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u4f30\u57fa\u51c6\u6765\u6d4b\u8bd5\u6a21\u578b\u662f\u5426\u80fd\u53ef\u9760\u5730\u8ba1\u6570\u7528\u6237\u60f3\u8981\u7684\u5185\u5bb9\u3002", "method": "\u6784\u5efaPairTally\u6570\u636e\u96c6\uff0c\u5305\u542b681\u5f20\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u6bcf\u5f20\u56fe\u6709\u4e24\u4e2a\u7269\u4f53\u7c7b\u522b\uff0c\u5206\u4e3a\u7c7b\u95f4\uff08\u4e0d\u540c\u7c7b\u522b\uff09\u548c\u7c7b\u5185\uff08\u5bc6\u5207\u76f8\u5173\u7684\u5b50\u7c7b\u522b\uff09\u4e24\u79cd\u8bbe\u7f6e\u3002\u5bf9\u591a\u79cd\u6700\u5148\u8fdb\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec\u57fa\u4e8e\u793a\u4f8b\u7684\u65b9\u6cd5\u3001\u8bed\u8a00\u63d0\u793a\u6a21\u578b\u548c\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5c3d\u7ba1\u6700\u8fd1\u6709\u6240\u8fdb\u5c55\uff0c\u4f46\u5f53\u524d\u6a21\u578b\u5728\u53ef\u9760\u5730\u8ba1\u6570\u7528\u6237\u610f\u56fe\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u7ec6\u7c92\u5ea6\u548c\u89c6\u89c9\u6a21\u7cca\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "PairTally\u4e3a\u8bca\u65ad\u548c\u6539\u8fdb\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8ba1\u6570\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u7cbe\u7ec6\u8ba1\u6570\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.14001", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14001", "abs": "https://arxiv.org/abs/2509.14001", "authors": ["Elena Camuffo", "Francesco Barbato", "Mete Ozay", "Simone Milani", "Umberto Michieli"], "title": "MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment", "comment": null, "summary": "We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment),\na knowledge distillation approach that transfers region-level multimodal\nsemantics from a large vision-language teacher (e.g., LLaVa) into a lightweight\nvision-only object detector student (e.g., YOLO). A translation module maps\nstudent features into a joint space, where the training of the student and\ntranslator is guided by a dual-objective loss that enforces both local\nalignment and global relational consistency. Unlike prior approaches focused on\ndense or global alignment, MOCHA operates at the object level, enabling\nefficient transfer of semantics without modifying the teacher or requiring\ntextual input at inference. We validate our method across four personalized\ndetection benchmarks under few-shot regimes. Results show consistent gains over\nbaselines, with a +10.1 average score improvement. Despite its compact\narchitecture, MOCHA reaches performance on par with larger multimodal models,\nproving its suitability for real-world deployment.", "AI": {"tldr": "MOCHA\u662f\u4e00\u79cd\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u5c06\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6559\u5e08\u6a21\u578b\u7684\u591a\u6a21\u6001\u8bed\u4e49\u77e5\u8bc6\u8f6c\u79fb\u5230\u8f7b\u91cf\u7ea7\u89c6\u89c9\u76ee\u6807\u68c0\u6d4b\u5b66\u751f\u6a21\u578b\u4e2d\uff0c\u901a\u8fc7\u5bf9\u8c61\u7ea7\u522b\u7684\u5bf9\u9f50\u5b9e\u73b0\u9ad8\u6548\u8bed\u4e49\u8fc1\u79fb\uff0c\u5728\u5c11\u6837\u672c\u4e2a\u6027\u5316\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5bc6\u96c6\u6216\u5168\u5c40\u5bf9\u9f50\uff0c\u7f3a\u4e4f\u5bf9\u8c61\u7ea7\u522b\u7684\u591a\u6a21\u6001\u8bed\u4e49\u8fc1\u79fb\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u4e0d\u9700\u8981\u4fee\u6539\u6559\u5e08\u6a21\u578b\u6216\u63a8\u7406\u65f6\u6587\u672c\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u533a\u57df\u7ea7\u591a\u6a21\u6001\u8bed\u4e49\u4ece\u5927\u578b\u591a\u6a21\u6001\u6559\u5e08\u6a21\u578b\u9ad8\u6548\u8f6c\u79fb\u5230\u8f7b\u91cf\u7ea7\u89c6\u89c9\u68c0\u6d4b\u5668\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMOCHA\u6846\u67b6\uff0c\u4f7f\u7528\u7ffb\u8bd1\u6a21\u5757\u5c06\u5b66\u751f\u7279\u5f81\u6620\u5c04\u5230\u8054\u5408\u7a7a\u95f4\uff0c\u901a\u8fc7\u53cc\u76ee\u6807\u635f\u5931\u51fd\u6570\uff08\u5c40\u90e8\u5bf9\u9f50\u548c\u5168\u5c40\u5173\u7cfb\u4e00\u81f4\u6027\uff09\u6307\u5bfc\u5b66\u751f\u548c\u7ffb\u8bd1\u6a21\u5757\u7684\u8bad\u7ec3\uff0c\u5728\u5bf9\u8c61\u7ea7\u522b\u8fdb\u884c\u8de8\u67b6\u6784\u5bf9\u9f50\u3002", "result": "\u5728\u56db\u4e2a\u4e2a\u6027\u5316\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u7684\u5c11\u6837\u672c\u573a\u666f\u4e2d\uff0c\u5e73\u5747\u5f97\u5206\u63d0\u5347+10.1\uff0c\u6027\u80fd\u8fbe\u5230\u4e0e\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u76f8\u5f53\u7684\u6c34\u5e73\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u9002\u7528\u6027\u3002", "conclusion": "MOCHA\u901a\u8fc7\u5bf9\u8c61\u7ea7\u522b\u7684\u77e5\u8bc6\u84b8\u998f\u6210\u529f\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u8bed\u4e49\u5411\u8f7b\u91cf\u7ea7\u68c0\u6d4b\u5668\u7684\u9ad8\u6548\u8fc1\u79fb\uff0c\u5728\u4fdd\u6301\u7d27\u51d1\u67b6\u6784\u7684\u540c\u65f6\u8fbe\u5230\u4e86\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u90e8\u7f72\u3002"}}
{"id": "2509.14012", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14012", "abs": "https://arxiv.org/abs/2509.14012", "authors": ["Tamara R. Lenhard", "Andreas Weinmann", "Tobias Koch"], "title": "Performance Optimization of YOLO-FEDER FusionNet for Robust Drone Detection in Visually Complex Environments", "comment": null, "summary": "Drone detection in visually complex environments remains challenging due to\nbackground clutter, small object scale, and camouflage effects. While generic\nobject detectors like YOLO exhibit strong performance in low-texture scenes,\ntheir effectiveness degrades in cluttered environments with low\nobject-background separability. To address these limitations, this work\npresents an enhanced iteration of YOLO-FEDER FusionNet -- a detection framework\nthat integrates generic object detection with camouflage object detection\ntechniques. Building upon the original architecture, the proposed iteration\nintroduces systematic advancements in training data composition, feature fusion\nstrategies, and backbone design. Specifically, the training process leverages\nlarge-scale, photo-realistic synthetic data, complemented by a small set of\nreal-world samples, to enhance robustness under visually complex conditions.\nThe contribution of intermediate multi-scale FEDER features is systematically\nevaluated, and detection performance is comprehensively benchmarked across\nmultiple YOLO-based backbone configurations. Empirical results indicate that\nintegrating intermediate FEDER features, in combination with backbone upgrades,\ncontributes to notable performance improvements. In the most promising\nconfiguration -- YOLO-FEDER FusionNet with a YOLOv8l backbone and FEDER\nfeatures derived from the DWD module -- these enhancements lead to a FNR\nreduction of up to 39.1 percentage points and a mAP increase of up to 62.8\npercentage points at an IoU threshold of 0.5, compared to the initial baseline.", "AI": {"tldr": "\u63d0\u51fa\u589e\u5f3a\u7248YOLO-FEDER FusionNet\u65e0\u4eba\u673a\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u901a\u7528\u76ee\u6807\u68c0\u6d4b\u548c\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u6280\u672f\uff0c\u5728\u590d\u6742\u89c6\u89c9\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u89c6\u89c9\u590d\u6742\u73af\u5883\u4e2d\u68c0\u6d4b\u56f0\u96be\u7684\u95ee\u9898\uff0c\u5305\u62ec\u80cc\u666f\u6742\u4e71\u3001\u76ee\u6807\u5c3a\u5ea6\u5c0f\u548c\u4f2a\u88c5\u6548\u5e94\u7b49\u6311\u6218\uff0c\u4f20\u7edf\u68c0\u6d4b\u5668\u5728\u4f4e\u7eb9\u7406\u573a\u666f\u8868\u73b0\u826f\u597d\u4f46\u5728\u6742\u4e71\u73af\u5883\u4e2d\u6548\u679c\u4e0b\u964d", "method": "\u5728\u539f\u59cb\u67b6\u6784\u57fa\u7840\u4e0a\u6539\u8fdb\u8bad\u7ec3\u6570\u636e\u7ec4\u6210\u3001\u7279\u5f81\u878d\u5408\u7b56\u7565\u548c\u9aa8\u5e72\u7f51\u7edc\u8bbe\u8ba1\uff0c\u4f7f\u7528\u5927\u89c4\u6a21\u7167\u7247\u7ea7\u5408\u6210\u6570\u636e\u914d\u5408\u5c11\u91cf\u771f\u5b9e\u6837\u672c\u8bad\u7ec3\uff0c\u7cfb\u7edf\u8bc4\u4f30\u591a\u5c3a\u5ea6FEDER\u7279\u5f81\u8d21\u732e\uff0c\u6d4b\u8bd5\u591a\u79cdYOLO\u9aa8\u5e72\u914d\u7f6e", "result": "\u6700\u4f73\u914d\u7f6e(YOLOv8l\u9aa8\u5e72+DWD\u6a21\u5757FEDER\u7279\u5f81)\u76f8\u6bd4\u57fa\u7ebf\u5b9e\u73b0FNR\u964d\u4f4e39.1\u4e2a\u767e\u5206\u70b9\uff0cmAP@0.5\u63d0\u534762.8\u4e2a\u767e\u5206\u70b9", "conclusion": "\u96c6\u6210\u4e2d\u95f4FEDER\u7279\u5f81\u7ed3\u5408\u9aa8\u5e72\u7f51\u7edc\u5347\u7ea7\u80fd\u663e\u8457\u63d0\u5347\u590d\u6742\u73af\u5883\u4e2d\u65e0\u4eba\u673a\u68c0\u6d4b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u878d\u5408\u901a\u7528\u68c0\u6d4b\u548c\u4f2a\u88c5\u68c0\u6d4b\u6280\u672f\u7684\u6709\u6548\u6027"}}
{"id": "2509.14033", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14033", "abs": "https://arxiv.org/abs/2509.14033", "authors": ["Weijie Yin", "Yongjie Ye", "Fangxun Shu", "Yue Liao", "Zijian Kang", "Hongyuan Dong", "Haiyang Yu", "Dingkang Yang", "Jiacong Wang", "Han Wang", "Wenzhuo Liu", "Xiao Liang", "Shuicheng Yan", "Chao Feng"], "title": "SAIL-VL2 Technical Report", "comment": "Technical Report", "summary": "We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)\nfor comprehensive multimodal understanding and reasoning. As the successor to\nSAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B\nparameter scales across diverse image and video benchmarks, demonstrating\nstrong capabilities from fine-grained perception to complex reasoning. Three\ncore innovations drive its effectiveness. First, a large-scale data curation\npipeline with scoring and filtering strategies enhances both quality and\ndistribution across captioning, OCR, QA, and video data, improving training\nefficiency. Second, a progressive training framework begins with a powerful\npre-trained vision encoder (SAIL-ViT), advances through multimodal\npre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that\nsystematically strengthens model capabilities. Third, architectural advances\nextend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.\nWith these contributions, SAIL-VL2 demonstrates competitive performance across\n106 datasets and achieves state-of-the-art results on challenging reasoning\nbenchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass\nleaderboard, SAIL-VL2-2B ranks first among officially released open-source\nmodels under the 4B parameter scale, while serving as an efficient and\nextensible foundation for the open-source multimodal community.", "AI": {"tldr": "SAIL-VL2\u662f\u4e00\u4e2a\u5f00\u6e90\u76842B\u548c8B\u53c2\u6570\u89c4\u6a21\u7684\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u5728\u591a\u6a21\u6001\u7406\u89e3\u548c\u63a8\u7406\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u7b5b\u9009\u3001\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u6846\u67b6\u548cMoE\u67b6\u6784\u521b\u65b0\u5b9e\u73b0\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u5168\u9762\u7684\u591a\u6a21\u6001\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u4f5c\u4e3aSAIL-VL\u7684\u7ee7\u4efb\u8005\uff0c\u65e8\u5728\u5728\u56fe\u50cf\u548c\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4ece\u7ec6\u7c92\u5ea6\u611f\u77e5\u5230\u590d\u6742\u63a8\u7406\u90fd\u5177\u5907\u5f3a\u5927\u80fd\u529b\u3002", "method": "1) \u5927\u89c4\u6a21\u6570\u636e\u7b5b\u9009\u7ba1\u9053\uff0c\u901a\u8fc7\u8bc4\u5206\u548c\u8fc7\u6ee4\u7b56\u7565\u63d0\u5347\u5b57\u5e55\u3001OCR\u3001\u95ee\u7b54\u548c\u89c6\u9891\u6570\u636e\u7684\u8d28\u91cf\u548c\u5206\u5e03\uff1b2) \u6e10\u8fdb\u5f0f\u8bad\u7ec3\u6846\u67b6\uff0c\u4ece\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u5f00\u59cb\uff0c\u7ecf\u8fc7\u591a\u6a21\u6001\u9884\u8bad\u7ec3\uff0c\u6700\u7ec8\u91c7\u7528\u601d\u7ef4\u878d\u5408SFT-RL\u6df7\u5408\u8303\u5f0f\uff1b3) \u67b6\u6784\u521b\u65b0\uff0c\u6269\u5c55\u5230\u9ad8\u6548\u7684\u7a00\u758f\u6df7\u5408\u4e13\u5bb6(MoE)\u8bbe\u8ba1\u3002", "result": "\u5728106\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u5728MMMU\u548cMathVista\u7b49\u5177\u6709\u6311\u6218\u6027\u7684\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7ed3\u679c\u3002SAIL-VL2-2B\u5728OpenCompass\u6392\u884c\u699c\u4e0a4B\u53c2\u6570\u89c4\u6a21\u4ee5\u4e0b\u7684\u5b98\u65b9\u5f00\u6e90\u6a21\u578b\u4e2d\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "SAIL-VL2\u4f5c\u4e3a\u4e00\u4e2a\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u7840\u6a21\u578b\uff0c\u4e3a\u5f00\u6e90\u591a\u6a21\u6001\u793e\u533a\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u89c6\u89c9-\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u4e86\u5353\u8d8a\u6027\u80fd\u3002"}}
{"id": "2509.14051", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14051", "abs": "https://arxiv.org/abs/2509.14051", "authors": ["Suhang You", "Carla Pitarch-Abaigar", "Sanket Kachole", "Sumedh Sonawane", "Juhyung Ha", "Anish Sudarshan Gada", "David Crandall", "Rakesh Shiradkar", "Spyridon Bakas"], "title": "PROFUSEme: PROstate Cancer Biochemical Recurrence Prediction via FUSEd Multi-modal Embeddings", "comment": "11 pages, 1 figure, method paper for CHIMERA 2025 Challenge", "summary": "Almost 30% of prostate cancer (PCa) patients undergoing radical prostatectomy\n(RP) experience biochemical recurrence (BCR), characterized by increased\nprostate specific antigen (PSA) and associated with increased mortality.\nAccurate early prediction of BCR, at the time of RP, would contribute to prompt\nadaptive clinical decision-making and improved patient outcomes. In this work,\nwe propose prostate cancer BCR prediction via fused multi-modal embeddings\n(PROFUSEme), which learns cross-modal interactions of clinical, radiology, and\npathology data, following an intermediate fusion configuration in combination\nwith Cox Proportional Hazard regressors. Quantitative evaluation of our\nproposed approach reveals superior performance, when compared with late fusion\nconfigurations, yielding a mean C-index of 0.861 ($\\sigma=0.112$) on the\ninternal 5-fold nested cross-validation framework, and a C-index of 0.7103 on\nthe hold out data of CHIMERA 2025 challenge validation leaderboard.", "AI": {"tldr": "\u63d0\u51faPROFUSEme\u65b9\u6cd5\uff0c\u901a\u8fc7\u878d\u5408\u4e34\u5e8a\u3001\u5f71\u50cf\u548c\u75c5\u7406\u591a\u6a21\u6001\u6570\u636e\u6765\u9884\u6d4b\u524d\u5217\u817a\u764c\u672f\u540e\u751f\u5316\u590d\u53d1\uff0c\u6027\u80fd\u4f18\u4e8e\u665a\u671f\u878d\u5408\u65b9\u6cd5\uff0c\u5728\u5185\u90e8\u9a8c\u8bc1\u548c\u5916\u90e8\u6311\u6218\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7ea630%\u524d\u5217\u817a\u764c\u60a3\u8005\u5728\u6839\u6cbb\u6027\u524d\u5217\u817a\u5207\u9664\u672f\u540e\u4f1a\u51fa\u73b0\u751f\u5316\u590d\u53d1\uff0c\u5bfc\u81f4\u6b7b\u4ea1\u7387\u589e\u52a0\u3002\u65e9\u671f\u51c6\u786e\u9884\u6d4b\u751f\u5316\u590d\u53d1\u6709\u52a9\u4e8e\u4e34\u5e8a\u51b3\u7b56\u548c\u6539\u5584\u60a3\u8005\u9884\u540e\u3002", "method": "\u91c7\u7528\u4e2d\u95f4\u878d\u5408\u914d\u7f6e\u7684\u591a\u6a21\u6001\u5d4c\u5165\u878d\u5408\u65b9\u6cd5(PROFUSEme)\uff0c\u7ed3\u5408Cox\u6bd4\u4f8b\u98ce\u9669\u56de\u5f52\u5668\uff0c\u5b66\u4e60\u4e34\u5e8a\u3001\u5f71\u50cf\u548c\u75c5\u7406\u6570\u636e\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\u5173\u7cfb\u3002", "result": "\u5728\u5185\u90e85\u6298\u5d4c\u5957\u4ea4\u53c9\u9a8c\u8bc1\u4e2d\u5e73\u5747C-index\u4e3a0.861(\u03c3=0.112)\uff0c\u5728CHIMERA 2025\u6311\u6218\u9a8c\u8bc1\u6392\u884c\u699c\u7684\u4fdd\u7559\u6570\u636e\u4e0aC-index\u4e3a0.7103\uff0c\u6027\u80fd\u4f18\u4e8e\u665a\u671f\u878d\u5408\u914d\u7f6e\u3002", "conclusion": "PROFUSEme\u65b9\u6cd5\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u6709\u6548\u9884\u6d4b\u524d\u5217\u817a\u764c\u672f\u540e\u751f\u5316\u590d\u53d1\uff0c\u4e3a\u4e34\u5e8a\u65e9\u671f\u5e72\u9884\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2509.14055", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14055", "abs": "https://arxiv.org/abs/2509.14055", "authors": ["Gang Cheng", "Xin Gao", "Li Hu", "Siqi Hu", "Mingyang Huang", "Chaonan Ji", "Ju Li", "Dechao Meng", "Jinwei Qi", "Penchong Qiao", "Zhen Shen", "Yafei Song", "Ke Sun", "Linrui Tian", "Feng Wang", "Guangyuan Wang", "Qi Wang", "Zhongjian Wang", "Jiayu Xiao", "Sheng Xu", "Bang Zhang", "Peng Zhang", "Xindi Zhang", "Zhe Zhang", "Jingren Zhou", "Lian Zhuo"], "title": "Wan-Animate: Unified Character Animation and Replacement with Holistic Replication", "comment": "Project Page: https://humanaigc.github.io/wan-animate/", "summary": "We introduce Wan-Animate, a unified framework for character animation and\nreplacement. Given a character image and a reference video, Wan-Animate can\nanimate the character by precisely replicating the expressions and movements of\nthe character in the video to generate high-fidelity character videos.\nAlternatively, it can integrate the animated character into the reference video\nto replace the original character, replicating the scene's lighting and color\ntone to achieve seamless environmental integration. Wan-Animate is built upon\nthe Wan model. To adapt it for character animation tasks, we employ a modified\ninput paradigm to differentiate between reference conditions and regions for\ngeneration. This design unifies multiple tasks into a common symbolic\nrepresentation. We use spatially-aligned skeleton signals to replicate body\nmotion and implicit facial features extracted from source images to reenact\nexpressions, enabling the generation of character videos with high\ncontrollability and expressiveness. Furthermore, to enhance environmental\nintegration during character replacement, we develop an auxiliary Relighting\nLoRA. This module preserves the character's appearance consistency while\napplying the appropriate environmental lighting and color tone. Experimental\nresults demonstrate that Wan-Animate achieves state-of-the-art performance. We\nare committed to open-sourcing the model weights and its source code.", "AI": {"tldr": "Wan-Animate\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u89d2\u8272\u52a8\u753b\u548c\u66ff\u6362\u6846\u67b6\uff0c\u80fd\u591f\u6839\u636e\u53c2\u8003\u89c6\u9891\u7cbe\u786e\u590d\u5236\u89d2\u8272\u7684\u8868\u60c5\u548c\u52a8\u4f5c\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u89d2\u8272\u89c6\u9891\uff0c\u6216\u5c06\u52a8\u753b\u89d2\u8272\u65e0\u7f1d\u96c6\u6210\u5230\u53c2\u8003\u89c6\u9891\u4e2d\u66ff\u6362\u539f\u59cb\u89d2\u8272\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u89d2\u8272\u52a8\u753b\u548c\u66ff\u6362\u4efb\u52a1\u4e2d\u7684\u9ad8\u4fdd\u771f\u5ea6\u751f\u6210\u548c\u73af\u5883\u65e0\u7f1d\u96c6\u6210\u95ee\u9898\uff0c\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u540c\u65f6\u5904\u7406\u89d2\u8272\u52a8\u753b\u751f\u6210\u548c\u573a\u666f\u66ff\u6362\uff0c\u5e76\u4fdd\u6301\u89d2\u8272\u5916\u89c2\u4e00\u81f4\u6027\u548c\u73af\u5883\u5149\u7167\u5339\u914d\u3002", "method": "\u57fa\u4e8eWan\u6a21\u578b\u6784\u5efa\uff0c\u91c7\u7528\u6539\u8fdb\u7684\u8f93\u5165\u8303\u5f0f\u533a\u5206\u53c2\u8003\u6761\u4ef6\u548c\u751f\u6210\u533a\u57df\uff0c\u4f7f\u7528\u7a7a\u95f4\u5bf9\u9f50\u7684\u9aa8\u9abc\u4fe1\u53f7\u590d\u5236\u8eab\u4f53\u8fd0\u52a8\uff0c\u4ece\u6e90\u56fe\u50cf\u63d0\u53d6\u9690\u5f0f\u9762\u90e8\u7279\u5f81\u91cd\u73b0\u8868\u60c5\uff0c\u5e76\u5f00\u53d1\u8f85\u52a9\u7684Relighting LoRA\u6a21\u5757\u6765\u589e\u5f3a\u73af\u5883\u5149\u7167\u96c6\u6210\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eWan-Animate\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\uff0c\u80fd\u591f\u751f\u6210\u5177\u6709\u9ad8\u53ef\u63a7\u6027\u548c\u8868\u73b0\u529b\u7684\u89d2\u8272\u89c6\u9891\uff0c\u5e76\u5b9e\u73b0\u65e0\u7f1d\u7684\u73af\u5883\u96c6\u6210\u6548\u679c\u3002", "conclusion": "Wan-Animate\u6210\u529f\u7edf\u4e00\u4e86\u89d2\u8272\u52a8\u753b\u548c\u66ff\u6362\u4efb\u52a1\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u7b26\u53f7\u8868\u793a\u548c\u5149\u7167\u5904\u7406\u6280\u672f\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u52a8\u753b\u751f\u6210\u548c\u73af\u5883\u96c6\u6210\uff0c\u4f5c\u8005\u627f\u8bfa\u5c06\u5f00\u6e90\u6a21\u578b\u6743\u91cd\u548c\u6e90\u4ee3\u7801\u3002"}}
{"id": "2509.14060", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14060", "abs": "https://arxiv.org/abs/2509.14060", "authors": ["Jun Du", "Weiwei Xing", "Ming Li", "Fei Richard Yu"], "title": "VSE-MOT: Multi-Object Tracking in Low-Quality Video Scenes Guided by Visual Semantic Enhancement", "comment": null, "summary": "Current multi-object tracking (MOT) algorithms typically overlook issues\ninherent in low-quality videos, leading to significant degradation in tracking\nperformance when confronted with real-world image deterioration. Therefore,\nadvancing the application of MOT algorithms in real-world low-quality video\nscenarios represents a critical and meaningful endeavor. To address the\nchallenges posed by low-quality scenarios, inspired by vision-language models,\nthis paper proposes a Visual Semantic Enhancement-guided Multi-Object Tracking\nframework (VSE-MOT). Specifically, we first design a tri-branch architecture\nthat leverages a vision-language model to extract global visual semantic\ninformation from images and fuse it with query vectors. Subsequently, to\nfurther enhance the utilization of visual semantic information, we introduce\nthe Multi-Object Tracking Adapter (MOT-Adapter) and the Visual Semantic Fusion\nModule (VSFM). The MOT-Adapter adapts the extracted global visual semantic\ninformation to suit multi-object tracking tasks, while the VSFM improves the\nefficacy of feature fusion. Through extensive experiments, we validate the\neffectiveness and superiority of the proposed method in real-world low-quality\nvideo scenarios. Its tracking performance metrics outperform those of existing\nmethods by approximately 8% to 20%, while maintaining robust performance in\nconventional scenarios.", "AI": {"tldr": "\u63d0\u51faVSE-MOT\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u4e49\u589e\u5f3a\u89e3\u51b3\u4f4e\u8d28\u91cf\u89c6\u9891\u4e2d\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u95ee\u9898\uff0c\u5728\u771f\u5b9e\u4f4e\u8d28\u91cf\u573a\u666f\u4e2d\u6027\u80fd\u63d0\u53478-20%", "motivation": "\u5f53\u524d\u591a\u76ee\u6807\u8ddf\u8e2a\u7b97\u6cd5\u5ffd\u89c6\u4f4e\u8d28\u91cf\u89c6\u9891\u95ee\u9898\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u9000\u5316\u60c5\u51b5\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u9700\u8981\u63d0\u5347MOT\u7b97\u6cd5\u5728\u4f4e\u8d28\u91cf\u89c6\u9891\u573a\u666f\u4e2d\u7684\u5e94\u7528\u80fd\u529b", "method": "\u8bbe\u8ba1\u4e09\u5206\u652f\u67b6\u6784\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u5168\u5c40\u89c6\u89c9\u8bed\u4e49\u4fe1\u606f\u5e76\u4e0e\u67e5\u8be2\u5411\u91cf\u878d\u5408\uff1b\u5f15\u5165MOT-Adapter\u9002\u914d\u591a\u76ee\u6807\u8ddf\u8e2a\u4efb\u52a1\uff0c\u4ee5\u53caVSFM\u6a21\u5757\u63d0\u5347\u7279\u5f81\u878d\u5408\u6548\u679c", "result": "\u5728\u771f\u5b9e\u4f4e\u8d28\u91cf\u89c6\u9891\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\uff0c\u8ddf\u8e2a\u6027\u80fd\u6307\u6807\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u7ea68%\u81f320%\uff0c\u540c\u65f6\u5728\u5e38\u89c4\u573a\u666f\u4e2d\u4fdd\u6301\u7a33\u5065\u6027\u80fd", "conclusion": "VSE-MOT\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4f4e\u8d28\u91cf\u89c6\u9891\u4e2d\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u6311\u6218\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u4e49\u589e\u5f3a\u663e\u8457\u63d0\u5347\u4e86\u8ddf\u8e2a\u6027\u80fd\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2509.14084", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14084", "abs": "https://arxiv.org/abs/2509.14084", "authors": ["Jingyi Yuan", "Jianxiong Ye", "Wenkang Chen", "Chenqiang Gao"], "title": "AD-DINOv3: Enhancing DINOv3 for Zero-Shot Anomaly Detection with Anomaly-Aware Calibration", "comment": null, "summary": "Zero-Shot Anomaly Detection (ZSAD) seeks to identify anomalies from arbitrary\nnovel categories, offering a scalable and annotation-efficient solution.\nTraditionally, most ZSAD works have been based on the CLIP model, which\nperforms anomaly detection by calculating the similarity between visual and\ntext embeddings. Recently, vision foundation models such as DINOv3 have\ndemonstrated strong transferable representation capabilities. In this work, we\nare the first to adapt DINOv3 for ZSAD. However, this adaptation presents two\nkey challenges: (i) the domain bias between large-scale pretraining data and\nanomaly detection tasks leads to feature misalignment; and (ii) the inherent\nbias toward global semantics in pretrained representations often leads to\nsubtle anomalies being misinterpreted as part of the normal foreground objects,\nrather than being distinguished as abnormal regions. To overcome these\nchallenges, we introduce AD-DINOv3, a novel vision-language multimodal\nframework designed for ZSAD. Specifically, we formulate anomaly detection as a\nmultimodal contrastive learning problem, where DINOv3 is employed as the visual\nbackbone to extract patch tokens and a CLS token, and the CLIP text encoder\nprovides embeddings for both normal and abnormal prompts. To bridge the domain\ngap, lightweight adapters are introduced in both modalities, enabling their\nrepresentations to be recalibrated for the anomaly detection task. Beyond this\nbaseline alignment, we further design an Anomaly-Aware Calibration Module\n(AACM), which explicitly guides the CLS token to attend to anomalous regions\nrather than generic foreground semantics, thereby enhancing discriminability.\nExtensive experiments on eight industrial and medical benchmarks demonstrate\nthat AD-DINOv3 consistently matches or surpasses state-of-the-art methods,\nverifying its superiority as a general zero-shot anomaly detection framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AD-DINOv3\uff0c\u4e00\u4e2a\u57fa\u4e8eDINOv3\u7684\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u548c\u5f02\u5e38\u611f\u77e5\u6821\u51c6\u6a21\u5757\u89e3\u51b3\u7279\u5f81\u5bf9\u9f50\u548c\u5c40\u90e8\u5f02\u5e38\u68c0\u6d4b\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6216\u8d85\u8d8aSOTA\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u4e3b\u8981\u57fa\u4e8eCLIP\u6a21\u578b\uff0c\u4f46DINOv3\u7b49\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u53ef\u8fc1\u79fb\u8868\u793a\u80fd\u529b\u3002\u7136\u800c\u76f4\u63a5\u5e94\u7528DINOv3\u9762\u4e34\u4e24\u4e2a\u6311\u6218\uff1a\u9884\u8bad\u7ec3\u6570\u636e\u4e0e\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u7684\u9886\u57df\u504f\u5dee\u5bfc\u81f4\u7279\u5f81\u9519\u4f4d\uff0c\u4ee5\u53ca\u9884\u8bad\u7ec3\u8868\u793a\u504f\u5411\u5168\u5c40\u8bed\u4e49\u800c\u5ffd\u7565\u5c40\u90e8\u5f02\u5e38\u3002", "method": "\u63d0\u51faAD-DINOv3\u591a\u6a21\u6001\u6846\u67b6\uff1a1) \u5c06\u5f02\u5e38\u68c0\u6d4b\u6784\u5efa\u4e3a\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u95ee\u9898\uff1b2) \u4f7f\u7528DINOv3\u63d0\u53d6\u89c6\u89c9\u7279\u5f81\uff0cCLIP\u6587\u672c\u7f16\u7801\u5668\u63d0\u4f9b\u6b63\u5e38/\u5f02\u5e38\u63d0\u793a\u5d4c\u5165\uff1b3) \u5f15\u5165\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u6865\u63a5\u9886\u57df\u5dee\u8ddd\uff1b4) \u8bbe\u8ba1\u5f02\u5e38\u611f\u77e5\u6821\u51c6\u6a21\u5757(AACM)\u5f15\u5bfcCLS token\u5173\u6ce8\u5f02\u5e38\u533a\u57df\u3002", "result": "\u57288\u4e2a\u5de5\u4e1a\u548c\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cAD-DINOv3\u59cb\u7ec8\u5339\u914d\u6216\u8d85\u8d8a\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f5c\u4e3a\u901a\u7528\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "AD-DINOv3\u6210\u529f\u5c06DINOv3\u9002\u914d\u5230\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u548c\u5f02\u5e38\u611f\u77e5\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u7279\u5f81\u5bf9\u9f50\u548c\u5c40\u90e8\u5f02\u5e38\u8bc6\u522b\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u65e0\u6807\u6ce8\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.14097", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.14097", "abs": "https://arxiv.org/abs/2509.14097", "authors": ["Yaru Chen", "Ruohao Guo", "Liting Gao", "Yang Xiang", "Qingyu Luo", "Zhenbo Li", "Wenwu Wang"], "title": "Teacher-Guided Pseudo Supervision and Cross-Modal Alignment for Audio-Visual Video Parsing", "comment": null, "summary": "Weakly-supervised audio-visual video parsing (AVVP) seeks to detect audible,\nvisible, and audio-visual events without temporal annotations. Previous work\nhas emphasized refining global predictions through contrastive or collaborative\nlearning, but neglected stable segment-level supervision and class-aware\ncross-modal alignment. To address this, we propose two strategies: (1) an\nexponential moving average (EMA)-guided pseudo supervision framework that\ngenerates reliable segment-level masks via adaptive thresholds or top-k\nselection, offering stable temporal guidance beyond video-level labels; and (2)\na class-aware cross-modal agreement (CMA) loss that aligns audio and visual\nembeddings at reliable segment-class pairs, ensuring consistency across\nmodalities while preserving temporal structure. Evaluations on LLP and UnAV-100\ndatasets shows that our method achieves state-of-the-art (SOTA) performance\nacross multiple metrics.", "AI": {"tldr": "\u63d0\u51faEMA\u5f15\u5bfc\u7684\u4f2a\u76d1\u7763\u6846\u67b6\u548c\u7c7b\u611f\u77e5\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u635f\u5931\uff0c\u7528\u4e8e\u5f31\u76d1\u7763\u97f3\u89c6\u9891\u89e3\u6790\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u7a33\u5b9a\u7684\u6bb5\u7ea7\u76d1\u7763\u548c\u7c7b\u611f\u77e5\u8de8\u6a21\u6001\u5bf9\u9f50\u7684\u95ee\u9898", "method": "\u4f7f\u7528EMA\u751f\u6210\u53ef\u9760\u6bb5\u7ea7\u63a9\u7801\u7684\u4f2a\u76d1\u7763\u6846\u67b6\uff0c\u4ee5\u53ca\u7c7b\u611f\u77e5\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u635f\u5931\u6765\u5bf9\u9f50\u97f3\u89c6\u9891\u5d4c\u5165", "result": "\u5728LLP\u548cUnAV-100\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0", "conclusion": "\u63d0\u51fa\u7684\u4e24\u79cd\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u5f31\u76d1\u7763\u97f3\u89c6\u9891\u89e3\u6790\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd"}}
{"id": "2509.14104", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14104", "abs": "https://arxiv.org/abs/2509.14104", "authors": ["Leonard Hackel", "Tom Burgert", "Beg\u00fcm Demir"], "title": "CSMoE: An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts", "comment": null, "summary": "Self-supervised learning through masked autoencoders has attracted great\nattention for remote sensing (RS) foundation model (FM) development, enabling\nimproved representation learning across diverse sensors and downstream tasks.\nHowever, existing RS FMs often either suffer from substantial computational\ncomplexity during both training and inference or exhibit limited\nrepresentational capacity. These issues restrict their practical applicability\nin RS. To address this limitation, we propose an adaptation for enhancing the\nefficiency of RS FMs by integrating the Soft mixture-of-experts (MoE) mechanism\ninto the FM. The integration of Soft MoEs into the FM allows modality-specific\nexpert specialization alongside shared cross-sensor representation learning. To\ndemonstrate the effectiveness of our adaptation, we apply it on the\nCross-Sensor Masked Autoencoder (CSMAE) model, resulting in the Cross-Sensor\nMixture-of-Experts (CSMoE) model. In addition, we introduce a thematic-climatic\ndescriptor-driven sampling strategy for the construction of a representative\nand diverse training set to train our CSMoE model. Extensive experiments on\nscene classification, semantic segmentation, and content-based image retrieval\ndemonstrate that our adaptation yields a reduction in computational\nrequirements while maintaining or improving representational performance.\nCompared to state-of-the-art RS FMs, CSMoE achieves a superior trade-off\nbetween representational capacity, accuracy, and computational efficiency. On\naverage, CSMoE achieves more than twice the computational efficiency of\nexisting RS FMs, while maintaining competitive performance across all\nexperiments. These results show the effectiveness of the proposed adaptation\nfor creating computationally efficient RS FMs. The code for the model, the\ntraining set creation, and the model weights will be available at\nhttps://git.tu-berlin.de/rsim/csmoe.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u96c6\u6210\u8f6f\u6df7\u5408\u4e13\u5bb6(Soft MoE)\u673a\u5236\u6765\u63d0\u5347\u9065\u611f\u57fa\u7840\u6a21\u578b\u6548\u7387\u7684\u65b9\u6cd5\uff0c\u521b\u5efa\u4e86Cross-Sensor Mixture-of-Experts (CSMoE)\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u9065\u611f\u57fa\u7840\u6a21\u578b\u8981\u4e48\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u8981\u4e48\u8868\u793a\u80fd\u529b\u6709\u9650\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u4fdd\u6301\u5f3a\u5927\u8868\u793a\u80fd\u529b\u53c8\u5177\u6709\u9ad8\u6548\u8ba1\u7b97\u6027\u80fd\u7684\u6a21\u578b\u3002", "method": "\u5c06\u8f6f\u6df7\u5408\u4e13\u5bb6(Soft MoE)\u673a\u5236\u96c6\u6210\u5230Cross-Sensor Masked Autoencoder (CSMAE)\u6a21\u578b\u4e2d\uff0c\u5f62\u6210CSMoE\u6a21\u578b\u3002\u540c\u65f6\u91c7\u7528\u4e3b\u9898-\u6c14\u5019\u63cf\u8ff0\u7b26\u9a71\u52a8\u7684\u91c7\u6837\u7b56\u7565\u6784\u5efa\u8bad\u7ec3\u96c6\u3002", "result": "CSMoE\u5728\u573a\u666f\u5206\u7c7b\u3001\u8bed\u4e49\u5206\u5272\u548c\u57fa\u4e8e\u5185\u5bb9\u7684\u56fe\u50cf\u68c0\u7d22\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u7684\u663e\u8457\u63d0\u5347\uff08\u5e73\u5747\u8d85\u8fc7\u73b0\u6709\u6a21\u578b\u7684\u4e24\u500d\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u4e86\u8868\u793a\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8f6f\u6df7\u5408\u4e13\u5bb6\u96c6\u6210\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u9065\u611f\u57fa\u7840\u6a21\u578b\u5728\u8ba1\u7b97\u6548\u7387\u548c\u8868\u793a\u80fd\u529b\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u521b\u5efa\u8ba1\u7b97\u9ad8\u6548\u7684\u9065\u611f\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2509.14119", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14119", "abs": "https://arxiv.org/abs/2509.14119", "authors": ["Jiabo MA", "Wenqiang Li", "Jinbang Li", "Ziyi Liu", "Linshan Wu", "Fengtao Zhou", "Li Liang", "Ronald Cheong Kin Chan", "Terence T. W. Wong", "Hao Chen"], "title": "Generative AI for Misalignment-Resistant Virtual Staining to Accelerate Histopathology Workflows", "comment": "the arxiv version of the under review journal paper", "summary": "Accurate histopathological diagnosis often requires multiple differently\nstained tissue sections, a process that is time-consuming, labor-intensive, and\nenvironmentally taxing due to the use of multiple chemical stains. Recently,\nvirtual staining has emerged as a promising alternative that is faster,\ntissue-conserving, and environmentally friendly. However, existing virtual\nstaining methods face significant challenges in clinical applications,\nprimarily due to their reliance on well-aligned paired data. Obtaining such\ndata is inherently difficult because chemical staining processes can distort\ntissue structures, and a single tissue section cannot undergo multiple staining\nprocedures without damage or loss of information. As a result, most available\nvirtual staining datasets are either unpaired or roughly paired, making it\ndifficult for existing methods to achieve accurate pixel-level supervision. To\naddress this challenge, we propose a robust virtual staining framework\nfeaturing cascaded registration mechanisms to resolve spatial mismatches\nbetween generated outputs and their corresponding ground truth. Experimental\nresults demonstrate that our method significantly outperforms state-of-the-art\nmodels across five datasets, achieving an average improvement of 3.2% on\ninternal datasets and 10.1% on external datasets. Moreover, in datasets with\nsubstantial misalignment, our approach achieves a remarkable 23.8% improvement\nin peak signal-to-noise ratio compared to baseline models. The exceptional\nrobustness of the proposed method across diverse datasets simplifies the data\nacquisition process for virtual staining and offers new insights for advancing\nits development.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u7ea7\u8054\u914d\u51c6\u673a\u5236\u7684\u9c81\u68d2\u865a\u62df\u67d3\u8272\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u751f\u6210\u8f93\u51fa\u4e0e\u771f\u5b9e\u6807\u7b7e\u4e4b\u95f4\u7684\u7a7a\u95f4\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u4f20\u7edf\u7ec4\u7ec7\u75c5\u7406\u5b66\u8bca\u65ad\u9700\u8981\u591a\u79cd\u5316\u5b66\u67d3\u8272\uff0c\u8fc7\u7a0b\u8017\u65f6\u3001\u52b3\u52a8\u5bc6\u96c6\u4e14\u5bf9\u73af\u5883\u4e0d\u53cb\u597d\u3002\u73b0\u6709\u865a\u62df\u67d3\u8272\u65b9\u6cd5\u4f9d\u8d56\u5bf9\u9f50\u826f\u597d\u7684\u914d\u5bf9\u6570\u636e\uff0c\u4f46\u83b7\u53d6\u8fd9\u79cd\u6570\u636e\u56f0\u96be\uff0c\u56e0\u4e3a\u5316\u5b66\u67d3\u8272\u8fc7\u7a0b\u4f1a\u5bfc\u81f4\u7ec4\u7ec7\u53d8\u5f62\uff0c\u4e14\u5355\u4e2a\u7ec4\u7ec7\u5207\u7247\u65e0\u6cd5\u8fdb\u884c\u591a\u6b21\u67d3\u8272", "method": "\u91c7\u7528\u7ea7\u8054\u914d\u51c6\u673a\u5236\u7684\u865a\u62df\u67d3\u8272\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u51b3\u751f\u6210\u8f93\u51fa\u4e0e\u771f\u5b9e\u6807\u7b7e\u4e4b\u95f4\u7684\u7a7a\u95f4\u4e0d\u5339\u914d\u95ee\u9898\u6765\u5b9e\u73b0\u51c6\u786e\u7684\u50cf\u7d20\u7ea7\u76d1\u7763", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u5185\u90e8\u6570\u636e\u96c6\u5e73\u5747\u63d0\u53473.2%\uff0c\u5916\u90e8\u6570\u636e\u96c6\u63d0\u534710.1%\u3002\u5728\u4e25\u91cd\u4e0d\u5bf9\u9f50\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u5cf0\u503c\u4fe1\u566a\u6bd4\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u534723.8%", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9c81\u68d2\u6027\uff0c\u7b80\u5316\u4e86\u865a\u62df\u67d3\u8272\u7684\u6570\u636e\u91c7\u96c6\u8fc7\u7a0b\uff0c\u5e76\u4e3a\u63a8\u8fdb\u5176\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2509.14120", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14120", "abs": "https://arxiv.org/abs/2509.14120", "authors": ["Sara Concas", "Simone Maurizio La Cava", "Andrea Panzino", "Ester Masala", "Giulia Orr\u00f9", "Gian Luca Marcialis"], "title": "Deceptive Beauty: Evaluating the Impact of Beauty Filters on Deepfake and Morphing Attack Detection", "comment": "Accepted at the 2025 IEEE INTERNATIONAL CONFERENCE ON Metrology for\n  eXtended Reality, Artificial Intelligence and Neural Engineering", "summary": "Digital beautification through social media filters has become increasingly\npopular, raising concerns about the reliability of facial images and videos and\nthe effectiveness of automated face analysis. This issue is particularly\ncritical for digital manipulation detectors, systems aiming at distinguishing\nbetween genuine and manipulated data, especially in cases involving deepfakes\nand morphing attacks designed to deceive humans and automated facial\nrecognition. This study examines whether beauty filters impact the performance\nof deepfake and morphing attack detectors. We perform a comprehensive analysis,\nevaluating multiple state-of-the-art detectors on benchmark datasets before and\nafter applying various smoothing filters. Our findings reveal performance\ndegradation, highlighting vulnerabilities introduced by facial enhancements and\nunderscoring the need for robust detection models resilient to such\nalterations.", "AI": {"tldr": "\u7f8e\u989c\u6ee4\u955c\u4f1a\u5f71\u54cd\u6df1\u5ea6\u4f2a\u9020\u548c\u53d8\u5f62\u653b\u51fb\u68c0\u6d4b\u5668\u7684\u6027\u80fd\uff0c\u5bfc\u81f4\u68c0\u6d4b\u51c6\u786e\u7387\u4e0b\u964d", "motivation": "\u793e\u4ea4\u5a92\u4f53\u7f8e\u989c\u6ee4\u955c\u7684\u666e\u53ca\u5f15\u53d1\u4e86\u5bf9\u9762\u90e8\u56fe\u50cf\u89c6\u9891\u53ef\u9760\u6027\u548c\u81ea\u52a8\u5316\u4eba\u8138\u5206\u6790\u6709\u6548\u6027\u7684\u62c5\u5fe7\uff0c\u7279\u522b\u662f\u5bf9\u65e8\u5728\u533a\u5206\u771f\u5b9e\u548c\u4f2a\u9020\u6570\u636e\u7684\u6570\u5b57\u64cd\u7eb5\u68c0\u6d4b\u5668\u7684\u5f71\u54cd", "method": "\u5bf9\u591a\u4e2a\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u5668\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5168\u9762\u5206\u6790\uff0c\u8bc4\u4f30\u5e94\u7528\u5404\u79cd\u5e73\u6ed1\u6ee4\u955c\u524d\u540e\u7684\u6027\u80fd\u8868\u73b0", "result": "\u7814\u7a76\u53d1\u73b0\u7f8e\u989c\u6ee4\u955c\u4f1a\u5bfc\u81f4\u68c0\u6d4b\u5668\u6027\u80fd\u4e0b\u964d\uff0c\u63ed\u793a\u4e86\u9762\u90e8\u7f8e\u5316\u5f15\u5165\u7684\u8106\u5f31\u6027", "conclusion": "\u9700\u8981\u5f00\u53d1\u80fd\u591f\u62b5\u6297\u6b64\u7c7b\u7f8e\u989c\u4fee\u9970\u7684\u9c81\u68d2\u68c0\u6d4b\u6a21\u578b"}}
{"id": "2509.14142", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14142", "abs": "https://arxiv.org/abs/2509.14142", "authors": ["Peng Xu", "Shengwu Xiong", "Jiajun Zhang", "Yaxiong Chen", "Bowen Zhou", "Chen Change Loy", "David A. Clifton", "Kyoung Mu Lee", "Luc Van Gool", "Ruiming He", "Ruilin Yao", "Xinwei Long", "Jirui Huang", "Kai Tian", "Sa Yang", "Yihua Shao", "Jin Feng", "Yue Zhong", "Jiakai Zhou", "Cheng Tang", "Tianyu Zou", "Yifang Zhang", "Junming Liang", "Guoyou Li", "Zhaoxiang Wang", "Qiang Zhou", "Yichen Zhao", "Shili Xiong", "Hyeongjin Nam", "Jaerin Lee", "Jaeyoung Chung", "JoonKyu Park", "Junghun Oh", "Kanggeon Lee", "Wooseok Lee", "Juneyoung Ro", "Turghun Osman", "Can Hu", "Chaoyang Liao", "Cheng Chen", "Chengcheng Han", "Chenhao Qiu", "Chong Peng", "Cong Xu", "Dailin Li", "Feiyu Wang", "Feng Gao", "Guibo Zhu", "Guopeng Tang", "Haibo Lu", "Han Fang", "Han Qi", "Hanxiao Wu", "Haobo Cheng", "Hongbo Sun", "Hongyao Chen", "Huayong Hu", "Hui Li", "Jiaheng Ma", "Jiang Yu", "Jianing Wang", "Jie Yang", "Jing He", "Jinglin Zhou", "Jingxuan Li", "Josef Kittler", "Lihao Zheng", "Linnan Zhao", "Mengxi Jia", "Muyang Yan", "Nguyen Thanh Thien", "Pu Luo", "Qi Li", "Shien Song", "Shijie Dong", "Shuai Shao", "Shutao Li", "Taofeng Xue", "Tianyang Xu", "Tianyi Gao", "Tingting Li", "Wei Zhang", "Weiyang Su", "Xiaodong Dong", "Xiao-Jun Wu", "Xiaopeng Zhou", "Xin Chen", "Xin Wei", "Xinyi You", "Xudong Kang", "Xujie Zhou", "Xusheng Liu", "Yanan Wang", "Yanbin Huang", "Yang Liu", "Yang Yang", "Yanglin Deng", "Yashu Kang", "Ye Yuan", "Yi Wen", "Yicen Tian", "Yilin Tao", "Yin Tang", "Yipeng Lin", "Yiqing Wang", "Yiting Xi", "Yongkang Yu", "Yumei Li", "Yuxin Qin", "Yuying Chen", "Yuzhe Cen", "Zhaofan Zou", "Zhaohong Liu", "Zhehao Shen", "Zhenglin Du", "Zhengyang Li", "Zhenni Huang", "Zhenwei Shao", "Zhilong Song", "Zhiyong Feng", "Zhiyu Wang", "Zhou Yu", "Ziang Li", "Zihan Zhai", "Zijian Zhang", "Ziyang Peng", "Ziyun Xiao", "Zongshu Li"], "title": "MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook", "comment": "ICCV 2025 MARS2 Workshop and Challenge \"Multimodal Reasoning and Slow\n  Thinking in the Large Model Era: Towards System 2 and Beyond''", "summary": "This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim\nto bring together different approaches in multimodal machine learning and LLMs\nvia a large benchmark. We hope it better allows researchers to follow the\nstate-of-the-art in this very dynamic area. Meanwhile, a growing number of\ntestbeds have boosted the evolution of general-purpose large language models.\nThus, this year's MARS2 focuses on real-world and specialized scenarios to\nbroaden the multimodal reasoning applications of MLLMs. Our organizing team\nreleased two tailored datasets Lens and AdsQA as test sets, which support\ngeneral reasoning in 12 daily scenarios and domain-specific reasoning in\nadvertisement videos, respectively. We evaluated 40+ baselines that include\nboth generalist MLLMs and task-specific models, and opened up three competition\ntracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question\nAnswering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative\nAdvertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and\nindustrial institutions have registered and 40+ valid submissions (out of\n1200+) have been included in our ranking lists. Our datasets, code sets (40+\nbaselines and 15+ participants' methods), and rankings are publicly available\non the MARS2 workshop website and our GitHub organization page\nhttps://github.com/mars2workshop/, where our updates and announcements of\nupcoming events will be continuously provided.", "AI": {"tldr": "MARS2 2025\u6311\u6218\u8d5b\u7efc\u8ff0\uff0c\u4e13\u6ce8\u4e8e\u591a\u6a21\u6001\u63a8\u7406\uff0c\u901a\u8fc7Lens\u548cAdsQA\u4e24\u4e2a\u6570\u636e\u96c6\u572812\u4e2a\u65e5\u5e38\u573a\u666f\u548c\u5e7f\u544a\u89c6\u9891\u9886\u57df\u8fdb\u884c\u8bc4\u4f30\uff0c\u5305\u542b40+\u57fa\u7ebf\u6a21\u578b\u548c76\u4e2a\u53c2\u8d5b\u56e2\u961f\u3002", "motivation": "\u6574\u5408\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0d\u540c\u65b9\u6cd5\uff0c\u901a\u8fc7\u5927\u578b\u57fa\u51c6\u6d4b\u8bd5\u8ddf\u8e2a\u8fd9\u4e00\u5feb\u901f\u53d1\u5c55\u9886\u57df\u7684\u6700\u65b0\u6280\u672f\uff0c\u5e76\u4e13\u6ce8\u4e8e\u73b0\u5b9e\u4e16\u754c\u548c\u4e13\u4e1a\u5316\u573a\u666f\u4ee5\u62d3\u5bbd\u591a\u6a21\u6001\u63a8\u7406\u5e94\u7528\u3002", "method": "\u53d1\u5e03\u4e24\u4e2a\u5b9a\u5236\u6570\u636e\u96c6Lens\u548cAdsQA\uff0c\u5206\u522b\u652f\u630112\u4e2a\u65e5\u5e38\u573a\u666f\u7684\u901a\u7528\u63a8\u7406\u548c\u5e7f\u544a\u89c6\u9891\u7684\u9886\u57df\u7279\u5b9a\u63a8\u7406\uff1b\u8bc4\u4f3040+\u57fa\u7ebf\u6a21\u578b\uff1b\u5f00\u8bbe\u4e09\u4e2a\u7ade\u8d5b\u8d5b\u9053\uff1aVG-RS\u3001VQA-SA\u548cVR-Ads\u3002", "result": "76\u4e2a\u6765\u81ea\u77e5\u540d\u5b66\u672f\u548c\u5de5\u4e1a\u673a\u6784\u7684\u56e2\u961f\u6ce8\u518c\uff0c40+\u6709\u6548\u63d0\u4ea4\uff08\u603b\u8ba11200+\u63d0\u4ea4\uff09\u88ab\u7eb3\u5165\u6392\u540d\u5217\u8868\uff1b\u6570\u636e\u96c6\u3001\u4ee3\u7801\u96c6\u548c\u6392\u540d\u516c\u5f00\u53ef\u7528\u3002", "conclusion": "MARS2\u6311\u6218\u8d5b\u6210\u529f\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u63a8\u7406\u7814\u7a76\uff0c\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u8d44\u6e90\u548c\u57fa\u51c6\uff0c\u672a\u6765\u5c06\u6301\u7eed\u66f4\u65b0\u5e76\u4e3e\u529e\u76f8\u5173\u6d3b\u52a8\u3002"}}
{"id": "2509.14149", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14149", "abs": "https://arxiv.org/abs/2509.14149", "authors": ["Haotian Li", "Jianbo Jiao"], "title": "An Exploratory Study on Abstract Images and Visual Representations Learned from Them", "comment": "Accepted to BMVC 2025", "summary": "Imagine living in a world composed solely of primitive shapes, could you\nstill recognise familiar objects? Recent studies have shown that abstract\nimages-constructed by primitive shapes-can indeed convey visual semantic\ninformation to deep learning models. However, representations obtained from\nsuch images often fall short compared to those derived from traditional raster\nimages. In this paper, we study the reasons behind this performance gap and\ninvestigate how much high-level semantic content can be captured at different\nabstraction levels. To this end, we introduce the Hierarchical Abstraction\nImage Dataset (HAID), a novel data collection that comprises abstract images\ngenerated from normal raster images at multiple levels of abstraction. We then\ntrain and evaluate conventional vision systems on HAID across various tasks\nincluding classification, segmentation, and object detection, providing a\ncomprehensive study between rasterised and abstract image representations. We\nalso discuss if the abstract image can be considered as a potentially effective\nformat for conveying visual semantic information and contributing to vision\ntasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u539f\u59cb\u5f62\u72b6\u6784\u6210\u7684\u62bd\u8c61\u56fe\u50cf\u662f\u5426\u80fd\u6709\u6548\u4f20\u9012\u89c6\u89c9\u8bed\u4e49\u4fe1\u606f\uff0c\u521b\u5efa\u4e86\u5206\u5c42\u62bd\u8c61\u56fe\u50cf\u6570\u636e\u96c6(HAID)\uff0c\u5728\u4e0d\u540c\u62bd\u8c61\u7ea7\u522b\u4e0a\u8bc4\u4f30\u4e86\u4f20\u7edf\u89c6\u89c9\u7cfb\u7edf\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u63a2\u7d22\u62bd\u8c61\u56fe\u50cf\uff08\u7531\u539f\u59cb\u5f62\u72b6\u6784\u6210\uff09\u4e0e\u4f20\u7edf\u5149\u6805\u56fe\u50cf\u5728\u4f20\u9012\u89c6\u89c9\u8bed\u4e49\u4fe1\u606f\u65b9\u9762\u7684\u6027\u80fd\u5dee\u8ddd\u539f\u56e0\uff0c\u7814\u7a76\u4e0d\u540c\u62bd\u8c61\u5c42\u6b21\u80fd\u6355\u6349\u591a\u5c11\u9ad8\u7ea7\u8bed\u4e49\u5185\u5bb9\u3002", "method": "\u5f15\u5165\u5206\u5c42\u62bd\u8c61\u56fe\u50cf\u6570\u636e\u96c6(HAID)\uff0c\u5305\u542b\u4ece\u6b63\u5e38\u5149\u6805\u56fe\u50cf\u751f\u6210\u7684\u591a\u4e2a\u62bd\u8c61\u7ea7\u522b\u7684\u62bd\u8c61\u56fe\u50cf\uff0c\u5728\u5206\u7c7b\u3001\u5206\u5272\u548c\u76ee\u6807\u68c0\u6d4b\u7b49\u4efb\u52a1\u4e0a\u8bad\u7ec3\u548c\u8bc4\u4f30\u4f20\u7edf\u89c6\u89c9\u7cfb\u7edf\u3002", "result": "\u63d0\u4f9b\u4e86\u5149\u6805\u5316\u56fe\u50cf\u4e0e\u62bd\u8c61\u56fe\u50cf\u8868\u793a\u4e4b\u95f4\u7684\u5168\u9762\u6bd4\u8f83\u7814\u7a76\uff0c\u5206\u6790\u4e86\u62bd\u8c61\u56fe\u50cf\u5728\u4e0d\u540c\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u62bd\u8c61\u56fe\u50cf\u662f\u5426\u53ef\u4ee5\u4f5c\u4e3a\u4f20\u9012\u89c6\u89c9\u8bed\u4e49\u4fe1\u606f\u7684\u6709\u6548\u683c\u5f0f\uff0c\u4ee5\u53ca\u5176\u5bf9\u89c6\u89c9\u4efb\u52a1\u7684\u6f5c\u5728\u8d21\u732e\u4ef7\u503c\u3002"}}
{"id": "2509.14151", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14151", "abs": "https://arxiv.org/abs/2509.14151", "authors": ["Rongyu Zhang", "Jiaming Liu", "Xiaoqi Li", "Xiaowei Chi", "Dan Wang", "Li Du", "Yuan Du", "Shanghang Zhang"], "title": "BEVUDA++: Geometric-aware Unsupervised Domain Adaptation for Multi-View 3D Object Detection", "comment": "Accepted by IEEE TCSVT", "summary": "Vision-centric Bird's Eye View (BEV) perception holds considerable promise\nfor autonomous driving. Recent studies have prioritized efficiency or accuracy\nenhancements, yet the issue of domain shift has been overlooked, leading to\nsubstantial performance degradation upon transfer. We identify major domain\ngaps in real-world cross-domain scenarios and initiate the first effort to\naddress the Domain Adaptation (DA) challenge in multi-view 3D object detection\nfor BEV perception. Given the complexity of BEV perception approaches with\ntheir multiple components, domain shift accumulation across multi-geometric\nspaces (e.g., 2D, 3D Voxel, BEV) poses a significant challenge for BEV domain\nadaptation. In this paper, we introduce an innovative geometric-aware\nteacher-student framework, BEVUDA++, to diminish this issue, comprising a\nReliable Depth Teacher (RDT) and a Geometric Consistent Student (GCS) model.\nSpecifically, RDT effectively blends target LiDAR with dependable depth\npredictions to generate depth-aware information based on uncertainty\nestimation, enhancing the extraction of Voxel and BEV features that are\nessential for understanding the target domain. To collaboratively reduce the\ndomain shift, GCS maps features from multiple spaces into a unified geometric\nembedding space, thereby narrowing the gap in data distribution between the two\ndomains. Additionally, we introduce a novel Uncertainty-guided Exponential\nMoving Average (UEMA) to further reduce error accumulation due to domain shifts\ninformed by previously obtained uncertainty guidance. To demonstrate the\nsuperiority of our proposed method, we execute comprehensive experiments in\nfour cross-domain scenarios, securing state-of-the-art performance in BEV 3D\nobject detection tasks, e.g., 12.9\\% NDS and 9.5\\% mAP enhancement on Day-Night\nadaptation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86BEVUDA++\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u9760\u7684\u6df1\u5ea6\u6559\u5e08\u6a21\u578b\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u5b66\u751f\u6a21\u578b\u6765\u89e3\u51b3BEV\u611f\u77e5\u4e2d\u7684\u57df\u9002\u5e94\u95ee\u9898\uff0c\u5728\u56db\u4e2a\u8de8\u57df\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684BEV\u611f\u77e5\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6548\u7387\u6216\u7cbe\u5ea6\u63d0\u5347\uff0c\u4f46\u5ffd\u7565\u4e86\u57df\u504f\u79fb\u95ee\u9898\uff0c\u5bfc\u81f4\u5728\u5b9e\u9645\u8de8\u57df\u573a\u666f\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u672c\u6587\u9996\u6b21\u5c1d\u8bd5\u89e3\u51b3\u591a\u89c6\u56fe3D\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u57df\u9002\u5e94\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u51e0\u4f55\u611f\u77e5\u7684\u5e08\u751f\u6846\u67b6BEVUDA++\uff0c\u5305\u542b\u53ef\u9760\u6df1\u5ea6\u6559\u5e08(RDT)\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u5b66\u751f(GCS)\u6a21\u578b\u3002RDT\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u751f\u6210\u6df1\u5ea6\u611f\u77e5\u4fe1\u606f\uff0cGCS\u5c06\u591a\u7a7a\u95f4\u7279\u5f81\u6620\u5c04\u5230\u7edf\u4e00\u7684\u51e0\u4f55\u5d4c\u5165\u7a7a\u95f4\u3002\u8fd8\u5f15\u5165\u4e86\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u6307\u6570\u79fb\u52a8\u5e73\u5747(UEMA)\u6765\u51cf\u5c11\u8bef\u5dee\u7d2f\u79ef\u3002", "result": "\u5728\u56db\u4e2a\u8de8\u57df\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u5168\u9762\u5b9e\u9a8c\uff0c\u5728BEV 3D\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u663c\u591c\u9002\u5e94\u573a\u666f\u4e2d\u5b9e\u73b0\u4e8612.9% NDS\u548c9.5% mAP\u7684\u63d0\u5347\u3002", "conclusion": "BEVUDA++\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86BEV\u611f\u77e5\u4e2d\u7684\u57df\u9002\u5e94\u95ee\u9898\uff0c\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7684\u65b9\u6cd5\u51cf\u5c11\u4e86\u591a\u51e0\u4f55\u7a7a\u95f4\u4e2d\u7684\u57df\u504f\u79fb\u7d2f\u79ef\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u57df\u573a\u666f\u4e0b\u7684\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2509.14165", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14165", "abs": "https://arxiv.org/abs/2509.14165", "authors": ["Michal Szczepanski", "Martyna Poreba", "Karim Haroun"], "title": "Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High Resolutions", "comment": null, "summary": "Vision Transformers (ViTs) achieve state-of-the-art performance in semantic\nsegmentation but are hindered by high computational and memory costs. To\naddress this, we propose STEP (SuperToken and Early-Pruning), a hybrid\ntoken-reduction framework that combines dynamic patch merging and token pruning\nto enhance efficiency without significantly compromising accuracy. At the core\nof STEP is dCTS, a lightweight CNN-based policy network that enables flexible\nmerging into superpatches. Encoder blocks integrate also early-exits to remove\nhigh-confident supertokens, lowering computational load. We evaluate our method\non high-resolution semantic segmentation benchmarks, including images up to\n1024 x 1024, and show that when dCTS is applied alone, the token count can be\nreduced by a factor of 2.5 compared to the standard 16 x 16 pixel patching\nscheme. This yields a 2.6x reduction in computational cost and a 3.4x increase\nin throughput when using ViT-Large as the backbone. Applying the full STEP\nframework further improves efficiency, reaching up to a 4x reduction in\ncomputational complexity and a 1.7x gain in inference speed, with a maximum\naccuracy drop of no more than 2.0%. With the proposed STEP configurations, up\nto 40% of tokens can be confidently predicted and halted before reaching the\nfinal encoder layer.", "AI": {"tldr": "STEP\u662f\u4e00\u4e2a\u6df7\u5408\u4ee4\u724c\u51cf\u5c11\u6846\u67b6\uff0c\u7ed3\u5408\u52a8\u6001\u8865\u4e01\u5408\u5e76\u548c\u4ee4\u724c\u526a\u679d\uff0c\u663e\u8457\u964d\u4f4eVision Transformers\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "Vision Transformers\u5728\u8bed\u4e49\u5206\u5272\u4e2d\u8868\u73b0\u4f18\u5f02\u4f46\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u51cf\u5c11\u4ee4\u724c\u6570\u91cf\u800c\u4e0d\u663e\u8457\u5f71\u54cd\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faSTEP\u6846\u67b6\uff0c\u5305\u542bdCTS\uff08\u8f7b\u91cf\u7ea7CNN\u7b56\u7565\u7f51\u7edc\uff09\u5b9e\u73b0\u52a8\u6001\u8865\u4e01\u5408\u5e76\u6210\u8d85\u8865\u4e01\uff0c\u5e76\u5728\u7f16\u7801\u5668\u5757\u4e2d\u96c6\u6210\u65e9\u671f\u9000\u51fa\u673a\u5236\u6765\u79fb\u9664\u9ad8\u7f6e\u4fe1\u5ea6\u8d85\u4ee4\u724c\u3002", "result": "\u57281024x1024\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\uff0cdCTS\u5355\u72ec\u5e94\u7528\u53ef\u5c06\u4ee4\u724c\u6570\u91cf\u51cf\u5c112.5\u500d\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e2.6\u500d\uff0c\u541e\u5410\u91cf\u63d0\u9ad83.4\u500d\u3002\u5b8c\u6574STEP\u6846\u67b6\u5b9e\u73b04\u500d\u8ba1\u7b97\u590d\u6742\u5ea6\u964d\u4f4e\u548c1.7\u500d\u63a8\u7406\u901f\u5ea6\u63d0\u5347\uff0c\u7cbe\u5ea6\u4e0b\u964d\u4e0d\u8d85\u8fc72.0%\u3002", "conclusion": "STEP\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86ViTs\u5728\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u4ee4\u724c\u51cf\u5c11\u673a\u5236\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u7cbe\u5ea6\u7684\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2509.14227", "categories": ["cs.CV", "I.2.10; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.14227", "abs": "https://arxiv.org/abs/2509.14227", "authors": ["Nisarg A. Shah", "Amir Ziai", "Chaitanya Ekanadham", "Vishal M. Patel"], "title": "Cin\u00e9aste: A Fine-grained Contextual Movie Question Answering Benchmark", "comment": "11 pages, 5 figures, 5 tables", "summary": "While recent advancements in vision-language models have improved video\nunderstanding, diagnosing their capacity for deep, narrative comprehension\nremains a challenge. Existing benchmarks often test short-clip recognition or\nuse template-based questions, leaving a critical gap in evaluating fine-grained\nreasoning over long-form narrative content. To address these gaps, we introduce\n$\\mathsf{Cin\\acute{e}aste}$, a comprehensive benchmark for long-form movie\nunderstanding. Our dataset comprises 3,119 multiple-choice question-answer\npairs derived from 1,805 scenes across 200 diverse movies, spanning five novel\nfine-grained contextual reasoning categories. We use GPT-4o to generate\ndiverse, context-rich questions by integrating visual descriptions, captions,\nscene titles, and summaries, which require deep narrative understanding. To\nensure high-quality evaluation, our pipeline incorporates a two-stage filtering\nprocess: Context-Independence filtering ensures questions require video\ncontext, while Contextual Veracity filtering validates factual consistency\nagainst the movie content, mitigating hallucinations. Experiments show that\nexisting MLLMs struggle on $\\mathsf{Cin\\acute{e}aste}$; our analysis reveals\nthat long-range temporal reasoning is a primary bottleneck, with the top\nopen-source model achieving only 63.15\\% accuracy. This underscores significant\nchallenges in fine-grained contextual understanding and the need for\nadvancements in long-form movie comprehension.", "AI": {"tldr": "Cineaste\u662f\u4e00\u4e2a\u7528\u4e8e\u957f\u89c6\u9891\u7535\u5f71\u7406\u89e3\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b3119\u4e2a\u591a\u9009\u9898\uff0c\u6db5\u76d6200\u90e8\u7535\u5f71\u76841805\u4e2a\u573a\u666f\uff0c\u6d4b\u8bd55\u79cd\u7ec6\u7c92\u5ea6\u63a8\u7406\u80fd\u529b\u3002\u73b0\u6709\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u6700\u9ad8\u4ec563.15%\u51c6\u786e\u7387\uff0c\u663e\u793a\u957f\u65f6\u5e8f\u63a8\u7406\u662f\u4e3b\u8981\u74f6\u9888\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u4e3b\u8981\u6d4b\u8bd5\u77ed\u89c6\u9891\u8bc6\u522b\u6216\u6a21\u677f\u5316\u95ee\u9898\uff0c\u7f3a\u4e4f\u5bf9\u957f\u53d9\u4e8b\u5185\u5bb9\u7684\u7ec6\u7c92\u5ea6\u63a8\u7406\u8bc4\u4f30\u3002\u9700\u8981\u6784\u5efa\u80fd\u591f\u6df1\u5ea6\u8bc4\u4f30\u6a21\u578b\u5bf9\u7535\u5f71\u53d9\u4e8b\u7406\u89e3\u80fd\u529b\u7684\u57fa\u51c6\u3002", "method": "\u4f7f\u7528GPT-4o\u751f\u6210\u591a\u6837\u5316\u3001\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u95ee\u9898\uff0c\u6574\u5408\u89c6\u89c9\u63cf\u8ff0\u3001\u5b57\u5e55\u3001\u573a\u666f\u6807\u9898\u548c\u6458\u8981\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8fc7\u6ee4\uff1a\u4e0a\u4e0b\u6587\u72ec\u7acb\u6027\u8fc7\u6ee4\u786e\u4fdd\u95ee\u9898\u9700\u8981\u89c6\u9891\u4e0a\u4e0b\u6587\uff0c\u4e0a\u4e0b\u6587\u771f\u5b9e\u6027\u8fc7\u6ee4\u9a8c\u8bc1\u4e0e\u7535\u5f71\u5185\u5bb9\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002", "result": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728Cineaste\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u6700\u4f73\u5f00\u6e90\u6a21\u578b\u51c6\u786e\u7387\u4ec5\u4e3a63.15%\u3002\u5206\u6790\u8868\u660e\u957f\u65f6\u5e8f\u63a8\u7406\u662f\u4e3b\u8981\u74f6\u9888\u3002", "conclusion": "\u8be5\u57fa\u51c6\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u957f\u89c6\u9891\u7535\u5f71\u7406\u89e3\u65b9\u9762\u7684\u663e\u8457\u6311\u6218\uff0c\u5f3a\u8c03\u4e86\u8be5\u9886\u57df\u9700\u8981\u8fdb\u4e00\u6b65\u6280\u672f\u7a81\u7834\u3002"}}
{"id": "2509.14232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14232", "abs": "https://arxiv.org/abs/2509.14232", "authors": ["Zhaokai Wang", "Penghao Yin", "Xiangyu Zhao", "Changyao Tian", "Yu Qiao", "Wenhai Wang", "Jifeng Dai", "Gen Luo"], "title": "GenExam: A Multidisciplinary Text-to-Image Exam", "comment": null, "summary": "Exams are a fundamental test of expert-level intelligence and require\nintegrated understanding, reasoning, and generation. Existing exam-style\nbenchmarks mainly focus on understanding and reasoning tasks, and current\ngeneration benchmarks emphasize the illustration of world knowledge and visual\nconcepts, neglecting the evaluation of rigorous drawing exams. We introduce\nGenExam, the first benchmark for multidisciplinary text-to-image exams,\nfeaturing 1,000 samples across 10 subjects with exam-style prompts organized\nunder a four-level taxonomy. Each problem is equipped with ground-truth images\nand fine-grained scoring points to enable a precise evaluation of semantic\ncorrectness and visual plausibility. Experiments show that even\nstate-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve\nless than 15% strict scores, and most models yield almost 0%, suggesting the\ngreat challenge of our benchmark. By framing image generation as an exam,\nGenExam offers a rigorous assessment of models' ability to integrate knowledge,\nreasoning, and generation, providing insights on the path to general AGI.", "AI": {"tldr": "GenExam\u662f\u9996\u4e2a\u591a\u5b66\u79d1\u6587\u672c\u5230\u56fe\u50cf\u8003\u8bd5\u57fa\u51c6\uff0c\u5305\u542b10\u4e2a\u5b66\u79d1\u76841000\u4e2a\u6837\u672c\uff0c\u91c7\u7528\u56db\u7ea7\u5206\u7c7b\u6cd5\u7ec4\u7ec7\u8003\u8bd5\u5f0f\u63d0\u793a\uff0c\u7528\u4e8e\u7cbe\u786e\u8bc4\u4f30\u8bed\u4e49\u6b63\u786e\u6027\u548c\u89c6\u89c9\u5408\u7406\u6027\u3002", "motivation": "\u73b0\u6709\u8003\u8bd5\u5f0f\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\uff0c\u800c\u5f53\u524d\u751f\u6210\u57fa\u51c6\u5f3a\u8c03\u4e16\u754c\u77e5\u8bc6\u548c\u89c6\u89c9\u6982\u5ff5\u7684\u5c55\u793a\uff0c\u5ffd\u89c6\u4e86\u4e25\u683c\u7ed8\u56fe\u8003\u8bd5\u7684\u8bc4\u4f30\u3002", "method": "\u6784\u5efa\u5305\u542b10\u4e2a\u5b66\u79d11000\u4e2a\u6837\u672c\u7684\u591a\u5b66\u79d1\u6587\u672c\u5230\u56fe\u50cf\u8003\u8bd5\u57fa\u51c6\uff0c\u6bcf\u4e2a\u95ee\u9898\u914d\u5907\u771f\u5b9e\u56fe\u50cf\u548c\u7ec6\u7c92\u5ea6\u8bc4\u5206\u70b9\uff0c\u91c7\u7528\u56db\u7ea7\u5206\u7c7b\u6cd5\u7ec4\u7ec7\u8003\u8bd5\u5f0f\u63d0\u793a\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5982GPT-Image-1\u548cGemini-2.5-Flash-Image\u7684\u4e25\u683c\u5f97\u5206\u4e5f\u4f4e\u4e8e15%\uff0c\u5927\u591a\u6570\u6a21\u578b\u5f97\u5206\u63a5\u8fd10%\uff0c\u8868\u660e\u8be5\u57fa\u51c6\u5177\u6709\u5de8\u5927\u6311\u6218\u6027\u3002", "conclusion": "\u901a\u8fc7\u5c06\u56fe\u50cf\u751f\u6210\u6846\u67b6\u5316\u4e3a\u8003\u8bd5\uff0cGenExam\u63d0\u4f9b\u4e86\u5bf9\u6a21\u578b\u6574\u5408\u77e5\u8bc6\u3001\u63a8\u7406\u548c\u751f\u6210\u80fd\u529b\u7684\u4e25\u683c\u8bc4\u4f30\uff0c\u4e3a\u901a\u5f80\u901a\u7528AGI\u7684\u9053\u8def\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
